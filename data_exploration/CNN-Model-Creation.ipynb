{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "43598701",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.keras.layers import Dense, Embedding,GlobalMaxPooling1D\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Embedding\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Dropout, Conv1D, MaxPool1D, GlobalMaxPool1D, Embedding, Activation\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import PorterStemmer\n",
    "from sklearn import preprocessing\n",
    "import pickle\n",
    "\n",
    "# Preprocessing and model is used from \n",
    "# https://www.kaggle.com/code/jagannathrk/word2vec-cnn-text-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "291f2af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "def tokenization(text):\n",
    "    \"\"\"Use this function to tokenize text.\n",
    "\n",
    "    :param text: Text as list\n",
    "    :type text: list[spacy.tokens.token.Token]\n",
    "    :return: Tokenized text as list\n",
    "    :rtype: list[spacy.tokens.token.Token]\n",
    "    \"\"\"\n",
    "\n",
    "    token_list = []\n",
    "    for doc in text: \n",
    "        # iterate over tokens in docs\n",
    "        for token in doc:\n",
    "            token_list.append(token)\n",
    "\n",
    "    return token_list\n",
    "\n",
    "\n",
    "def stop_word_removal(text): \n",
    "    \"\"\"Use this function to remove stop words. \n",
    "\n",
    "    :param text: Tokens to remove stop words from \n",
    "    :type text: list[spacy.tokens.token.Token]\n",
    "    :return: Tokens without stop words\n",
    "    :rtype: list[spacy.tokens.token.Token]\n",
    "    \"\"\"\n",
    "\n",
    "    token_list_without_stop = []\n",
    "    # Don't add token to list if stop word\n",
    "    for token in text:\n",
    "        if token.is_stop == False: \n",
    "            token_list_without_stop.append(token)\n",
    "\n",
    "    return token_list_without_stop\n",
    "\n",
    "\n",
    "def punctutation_removal(text): \n",
    "    \"\"\"Use this function to remove punctuation.\n",
    "\n",
    "    :param text: Tokens to remove punctuation from\n",
    "    :type text: list[spacy.tokens.token.Token]\n",
    "    :return: Tokens without punctuation\n",
    "    :rtype: list[spacy.tokens.token.Token]\n",
    "    \"\"\"\n",
    "\n",
    "    token_list_no_stop_no_punct = []\n",
    "    # Don't add token to list if punctuation\n",
    "    for token in text:\n",
    "        if token.is_punct == False:\n",
    "            token_list_no_stop_no_punct.append(token)\n",
    "\n",
    "    return token_list_no_stop_no_punct\n",
    "\n",
    "\n",
    "def lemmatization(text): \n",
    "    \"\"\"Use this function to lemmatize a given text.\n",
    "\n",
    "    :param text: Tokens to lemmatize\n",
    "    :type text: list[spacy.tokens.token.Token]\n",
    "    :return: lemmatized tokens\n",
    "    :rtype: list[str]\n",
    "    \"\"\"\n",
    "\n",
    "    token_list_no_stop_no_punct_lemmatized = []\n",
    "    for token in text: \n",
    "        if \"\\n\" not in token.lemma_:\n",
    "            token_list_no_stop_no_punct_lemmatized.append(token.lemma_)\n",
    "    return token_list_no_stop_no_punct_lemmatized\n",
    "\n",
    "\n",
    "def processing_pipeline(song_data):\n",
    "    \"\"\"Use this function to execute the entire processing pipeline on given song data.\n",
    "    Preprocessing steps:\n",
    "    - Tokenization\n",
    "    - Stop word removal\n",
    "    - Punctuation removal\n",
    "    - Lemmatization\n",
    "    - ...\n",
    "\n",
    "    :param song_data: song data saved in a json file containing song name, artist name and lyrics\n",
    "    :type song_data: dict\n",
    "    :return: preprocessed song data\n",
    "    :rtype: dict\n",
    "    \"\"\"\n",
    "\n",
    "    nlp = spacy.load(\"en_core_web_sm\", disable = ['ner'])\n",
    "    \n",
    "    for row in range(len(song_data)):\n",
    "        text_nlp_pipe = list(nlp.pipe([song_data.iloc[row][\"Lyric\"]]))\n",
    "    \n",
    "        # Tokenization\n",
    "        song_data.at[row,\"Lyric\"] = tokenization(text_nlp_pipe)\n",
    "        # Stop word removal\n",
    "        song_data.at[row,\"Lyric\"] = stop_word_removal(song_data.iloc[row][\"Lyric\"])\n",
    "        # Punctuation removal\n",
    "        song_data.at[row,\"Lyric\"] = punctutation_removal(song_data.iloc[row][\"Lyric\"])\n",
    "        # Lemmatization\n",
    "        song_data.at[row,\"Lyric\"] = lemmatization(song_data.iloc[row][\"Lyric\"])\n",
    "        song_data.at[row,\"Lyric\"] = \" \".join(song_data.iloc[row][\"Lyric\"])\n",
    "        song_data.at[row,\"Lyric\"] = song_data.iloc[row][\"Lyric\"].lower()\n",
    "        if row%500 == 0 and row >= 500:\n",
    "            print(f\"processsed: {row} rows out of {len(song_data)}\")\n",
    "    return song_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "af1a15a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/song-data-labels-cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "521cac23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processsed: 500 rows out of 27705\n",
      "processsed: 1000 rows out of 27705\n",
      "processsed: 1500 rows out of 27705\n",
      "processsed: 2000 rows out of 27705\n",
      "processsed: 2500 rows out of 27705\n",
      "processsed: 3000 rows out of 27705\n",
      "processsed: 3500 rows out of 27705\n",
      "processsed: 4000 rows out of 27705\n",
      "processsed: 4500 rows out of 27705\n",
      "processsed: 5000 rows out of 27705\n",
      "processsed: 5500 rows out of 27705\n",
      "processsed: 6000 rows out of 27705\n",
      "processsed: 6500 rows out of 27705\n",
      "processsed: 7000 rows out of 27705\n",
      "processsed: 7500 rows out of 27705\n",
      "processsed: 8000 rows out of 27705\n",
      "processsed: 8500 rows out of 27705\n",
      "processsed: 9000 rows out of 27705\n",
      "processsed: 9500 rows out of 27705\n",
      "processsed: 10000 rows out of 27705\n",
      "processsed: 10500 rows out of 27705\n",
      "processsed: 11000 rows out of 27705\n",
      "processsed: 11500 rows out of 27705\n",
      "processsed: 12000 rows out of 27705\n",
      "processsed: 12500 rows out of 27705\n",
      "processsed: 13000 rows out of 27705\n",
      "processsed: 13500 rows out of 27705\n",
      "processsed: 14000 rows out of 27705\n",
      "processsed: 14500 rows out of 27705\n",
      "processsed: 15000 rows out of 27705\n",
      "processsed: 15500 rows out of 27705\n",
      "processsed: 16000 rows out of 27705\n",
      "processsed: 16500 rows out of 27705\n",
      "processsed: 17000 rows out of 27705\n",
      "processsed: 17500 rows out of 27705\n",
      "processsed: 18000 rows out of 27705\n",
      "processsed: 18500 rows out of 27705\n",
      "processsed: 19000 rows out of 27705\n",
      "processsed: 19500 rows out of 27705\n",
      "processsed: 20000 rows out of 27705\n",
      "processsed: 20500 rows out of 27705\n",
      "processsed: 21000 rows out of 27705\n",
      "processsed: 21500 rows out of 27705\n",
      "processsed: 22000 rows out of 27705\n",
      "processsed: 22500 rows out of 27705\n",
      "processsed: 23000 rows out of 27705\n",
      "processsed: 23500 rows out of 27705\n",
      "processsed: 24000 rows out of 27705\n",
      "processsed: 24500 rows out of 27705\n",
      "processsed: 25000 rows out of 27705\n",
      "processsed: 25500 rows out of 27705\n",
      "processsed: 26000 rows out of 27705\n",
      "processsed: 26500 rows out of 27705\n",
      "processsed: 27000 rows out of 27705\n",
      "processsed: 27500 rows out of 27705\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['lie floor feather find hand wanna believe not think s god heaven hell lose lookin sign maybe fool swear spine get shiver like kiss time tho tear stream like river know ask sign scare tho body start shakin feel dry eye tho heart break know love sign sign findin street starin reach turn lookin stranger eye s god heaven hell tt feel like walk cos touch like swear spine get shiver like kiss time tho tear stream like river know ask sign scare tho body start shakin feel dry eye tho heart break know love sign sign lie floor feather find hand wanna believe not think swear spine get shiver like kiss time tho tear stream like river know ask sign scare tho body start shakin feel dry eye tho heart break know love sign sign',\n",
       " 'oh lord like know think oh oh lord wanna love life heart wonder right try forget near darle miss darling oh bring darle sweet kiss darle think happy think mistake think darling right wrong near darle miss darling oh bring darle sweet kiss darle oh jah oh jah like near oh jah oh jah miss love']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)\n",
    "df = processing_pipeline(df)\n",
    "list(df[\"Lyric\"].head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2ea2bcaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['lie', 'floor', 'feather', 'find', 'hand', 'wanna', 'believe', 'not', 'think', 's', 'god', 'heaven', 'hell', 'lose', 'lookin', 'sign', 'maybe', 'fool', 'swear', 'spine', 'get', 'shiver', 'like', 'kiss', 'time', 'tho', 'tear', 'stream', 'like', 'river', 'know', 'ask', 'sign', 'scare', 'tho', 'body', 'start', 'shakin', 'feel', 'dry', 'eye', 'tho', 'heart', 'break', 'know', 'love', 'sign', 'sign', 'findin', 'street', 'starin', 'reach', 'turn', 'lookin', 'stranger', 'eye', 's', 'god', 'heaven', 'hell', 'tt', 'feel', 'like', 'walk', 'cos', 'touch', 'like', 'swear', 'spine', 'get', 'shiver', 'like', 'kiss', 'time', 'tho', 'tear', 'stream', 'like', 'river', 'know', 'ask', 'sign', 'scare', 'tho', 'body', 'start', 'shakin', 'feel', 'dry', 'eye', 'tho', 'heart', 'break', 'know', 'love', 'sign', 'sign', 'lie', 'floor', 'feather', 'find', 'hand', 'wanna', 'believe', 'not', 'think', 'swear', 'spine', 'get', 'shiver', 'like', 'kiss', 'time', 'tho', 'tear', 'stream', 'like', 'river', 'know', 'ask', 'sign', 'scare', 'tho', 'body', 'start', 'shakin', 'feel', 'dry', 'eye', 'tho', 'heart', 'break', 'know', 'love', 'sign', 'sign'], ['oh', 'lord', 'like', 'know', 'think', 'oh', 'oh', 'lord', 'wanna', 'love', 'life', 'heart', 'wonder', 'right', 'try', 'forget', 'near', 'darle', 'miss', 'darling', 'oh', 'bring', 'darle', 'sweet', 'kiss', 'darle', 'think', 'happy', 'think', 'mistake', 'think', 'darling', 'right', 'wrong', 'near', 'darle', 'miss', 'darling', 'oh', 'bring', 'darle', 'sweet', 'kiss', 'darle', 'oh', 'jah', 'oh', 'jah', 'like', 'near', 'oh', 'jah', 'oh', 'jah', 'miss', 'love']]\n"
     ]
    }
   ],
   "source": [
    "# merge lyrics together\n",
    "lyrics = []\n",
    "for i in df['Lyric']:\n",
    "    lyrics.append(i.split())\n",
    "print(lyrics[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "a9ef229f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec<vocab=27165, vector_size=150, alpha=0.025>\n"
     ]
    }
   ],
   "source": [
    "# train the word2vec model\n",
    "# vector size according to https://moj-analytical-services.github.io/NLP-guidance/NNmodels.html#:~:text=The%20standard%20Word2Vec%20pre%2Dtrained,fewer%20dimensions%20to%20represent%20them.\n",
    "# micount = 2 to prevent misspellings\n",
    "word2vec_model = Word2Vec(lyrics, vector_size=150, window=5, min_count=2, workers=16)\n",
    "print(word2vec_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "58b953eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0    76    19   223   739   251   176   236     9   269  1216   406\n",
      "    544    34   415     9  3789    52   351    76    16   162    10     1\n",
      "     34   393    68    36   177    52     2   136    44     9    52    67\n",
      "   1407   148   778    76    46   422    20    53    16   104   487    52\n",
      "      2  2007    47     5    55     5    55    76    16   162    10     1\n",
      "     34   393    68    36   177    52     2   136    44     9    52    67\n",
      "   1407   148   778    43   206     5   452    16   438     3    16    47\n",
      "     16    99    76   162    18   162    10     1    34   439   107    68\n",
      "     36   177    96   136    44     9    52    67  1407   148   778    76]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0   108\n",
      "    160   282   282   120    12   763   322  2709    10    81   376   711\n",
      "    132   802    15   131   140   124   587   587     2     4    13   130\n",
      "    150   345   420   138    87   432    14     1    22     5    14   109\n",
      "     22    71   432     1 15006     5    14     1     1 15006     5    14\n",
      "      1     1    14    14    45   188  1330   220    65    86    26    15\n",
      "     76   334   333     3    26  1995   112   131   140   124   587   587\n",
      "      1   136     2     4    13   130   150   345   420   138    87   432\n",
      "   4244    14     1    22     5    14   109    22    71   432     1 15006\n",
      "     14     1     1 15006     5    14     1     1    14     1    22     5\n",
      "     14   109    22    71   432     1    14     1    22     5    14   109\n",
      "     22    71   432     1    14     1    22     5    14   109    22    71\n",
      "    432     1    14     1    22    14    14    14   109 26598 17244 26599]]\n"
     ]
    }
   ],
   "source": [
    "# use the keras tokenizer and apply it to the lyrics\n",
    "# number in first row is vocab size\n",
    "token = Tokenizer(27165)\n",
    "token.fit_on_texts(df['Lyric'])\n",
    "text = token.texts_to_sequences(df['Lyric'])\n",
    "text = pad_sequences(text, 180)\n",
    "print(text[3:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "30bd205d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the tokenizer\n",
    "\n",
    "# saving\n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(token, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "2b322a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode the labels\n",
    "le = preprocessing.LabelEncoder()\n",
    "y = le.fit_transform(df['Mood'])\n",
    "y = to_categorical(y)\n",
    "# save the label encoder\n",
    "numpy.save('label_encoder.npy', le.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "71d3a4c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(y[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "152ea206",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(np.array(text), y, test_size=0.2, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4d1db6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to use the word2vec as an layer in keras \n",
    "# taken from the gensim wikipage: https://github.com/RaRe-Technologies/gensim/wiki/Using-Gensim-Embeddings-with-Keras-and-Tensorflow\n",
    "\n",
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "def gensim_to_keras_embedding(model, train_embeddings=False):\n",
    "    \"\"\"Get a Keras 'Embedding' layer with weights set from Word2Vec model's learned word embeddings.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_embeddings : bool\n",
    "        If False, the returned weights are frozen and stopped from being updated.\n",
    "        If True, the weights can / will be further updated in Keras.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    `keras.layers.Embedding`\n",
    "        Embedding layer, to be used as input to deeper network layers.\n",
    "\n",
    "    \"\"\"\n",
    "    keyed_vectors = model.wv  # structure holding the result of training\n",
    "    weights = keyed_vectors.vectors  # vectors themselves, a 2D numpy array    \n",
    "    index_to_key = keyed_vectors.index_to_key  # which row in `weights` corresponds to which word?\n",
    "\n",
    "    layer = Embedding(\n",
    "        input_dim=weights.shape[0],\n",
    "        output_dim=weights.shape[1],\n",
    "        weights=[weights],\n",
    "        trainable=train_embeddings,\n",
    "    )\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a660e0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_model = Sequential()\n",
    "keras_model.add(gensim_to_keras_embedding(word2vec_model, True))\n",
    "keras_model.add(Dropout(0.2))\n",
    "keras_model.add(Conv1D(50, 5, activation='relu', padding='same', strides=1))\n",
    "keras_model.add(Conv1D(50, 5, activation='relu', padding='same', strides=1))\n",
    "keras_model.add(MaxPool1D())\n",
    "keras_model.add(Dropout(0.2))\n",
    "keras_model.add(Conv1D(100, 5, activation='relu', padding='same', strides=1))\n",
    "keras_model.add(Conv1D(100, 5, activation='relu', padding='same', strides=1))\n",
    "keras_model.add(MaxPool1D())\n",
    "keras_model.add(Dropout(0.2))\n",
    "keras_model.add(Conv1D(150, 5, activation='relu', padding='same', strides=1))\n",
    "keras_model.add(Conv1D(150, 5, activation='relu', padding='same', strides=1))\n",
    "keras_model.add(GlobalMaxPool1D())\n",
    "keras_model.add(Dropout(0.2))\n",
    "keras_model.add(Dense(200))\n",
    "keras_model.add(Activation('relu'))\n",
    "keras_model.add(Dropout(0.2))\n",
    "# Number of moods to be classified to\n",
    "keras_model.add(Dense(17))\n",
    "keras_model.add(Activation('softmax'))\n",
    "keras_model.compile(loss='binary_crossentropy', metrics=['acc'], optimizer='adam')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1170775b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "693/693 [==============================] - 72s 100ms/step - loss: 0.1866 - acc: 0.2750 - val_loss: 0.1916 - val_acc: 0.3043\n",
      "Epoch 2/3\n",
      "693/693 [==============================] - 76s 110ms/step - loss: 0.1767 - acc: 0.3139 - val_loss: 0.1757 - val_acc: 0.3337\n",
      "Epoch 3/3\n",
      "693/693 [==============================] - 71s 103ms/step - loss: 0.1727 - acc: 0.3429 - val_loss: 0.1745 - val_acc: 0.3270\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f27439da7d0>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras_model.fit(x_train, y_train, batch_size=32, epochs=3, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d3da8cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 7). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cnn_model_v1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cnn_model_v1/assets\n"
     ]
    }
   ],
   "source": [
    "keras_model.save('./cnn_model_v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13efec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "\n",
    "# let's create a function that creates the model (required for KerasClassifier) \n",
    "# while accepting the hyperparameters we want to tune \n",
    "# we also pass some default values such as optimizer='rmsprop'\n",
    "def create_model(filters,multiplicator, multiplicator2, kernel, adapt_embedding):\n",
    "    # define model\n",
    "    \n",
    "    keras_model = Sequential()\n",
    "    keras_model.add(gensim_to_keras_embedding(word2vec_model, True))\n",
    "    keras_model.add(Dropout(0.2))\n",
    "    keras_model.add(Conv1D(filters, kernel, activation='relu', padding='same', strides=1))\n",
    "    keras_model.add(Conv1D(filters, kernel, activation='relu', padding='same', strides=1))\n",
    "    keras_model.add(MaxPool1D())\n",
    "    keras_model.add(Dropout(0.2))\n",
    "    keras_model.add(Conv1D(filters*multiplicator, kernel, activation='relu', padding='same', strides=1))\n",
    "    keras_model.add(Conv1D(filters*multiplicator, kernel, activation='relu', padding='same', strides=1))\n",
    "    keras_model.add(MaxPool1D())\n",
    "    keras_model.add(Dropout(0.2))\n",
    "    keras_model.add(Conv1D(filters*multiplicator2, kernel, activation='relu', padding='same', strides=1))\n",
    "    keras_model.add(Conv1D(filters*multiplicator2, kernel, activation='relu', padding='same', strides=1))\n",
    "    keras_model.add(GlobalMaxPool1D())\n",
    "    keras_model.add(Dropout(0.2))\n",
    "    keras_model.add(Dense(200))\n",
    "    keras_model.add(Activation('relu'))\n",
    "    keras_model.add(Dropout(0.2))\n",
    "    # Number of moods to be classified to\n",
    "    keras_model.add(Dense(17))\n",
    "    keras_model.add(Activation('softmax'))\n",
    "    \n",
    "    keras_model.compile(loss='binary_crossentropy', metrics=['acc'], optimizer='adam')\n",
    "    return keras_model\n",
    "\n",
    "\n",
    "\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "batch_size = 32\n",
    "epochs = 3\n",
    "\n",
    "model_CV = KerasClassifier(build_fn=create_model, epochs=epochs, \n",
    "                           batch_size=batch_size, verbose=1)\n",
    "# define the grid search parameters\n",
    "filters = [50]\n",
    "multiplicator = [2]\n",
    "multiplicator2 = [4]\n",
    "kernel = [5]\n",
    "adapt_embedding = [True, False]\n",
    "\n",
    "param_grid = dict(filters=filters, multiplicator=multiplicator, multiplicator2=multiplicator2, kernel=kernel, adapt_embedding=adapt_embedding)\n",
    "grid = GridSearchCV(estimator=model_CV, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "grid_result = grid.fit(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1b1a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print results\n",
    "print(f'Best Accuracy for {grid_result.best_score_} using {grid_result.best_params_}')\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f' mean={mean:.4}, std={stdev:.4} using {param}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f391ee6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "def tokenization(text: list[spacy.tokens.token.Token]) -> list[spacy.tokens.token.Token]:\n",
    "    \"\"\"Use this function to tokenize text.\n",
    "\n",
    "    :param text: Text as list\n",
    "    :type text: list[spacy.tokens.token.Token]\n",
    "    :return: Tokenized text as list\n",
    "    :rtype: list[spacy.tokens.token.Token]\n",
    "    \"\"\"\n",
    "\n",
    "    token_list = []\n",
    "    for doc in text: \n",
    "        # iterate over tokens in docs\n",
    "        for token in doc:\n",
    "            token_list.append(token)\n",
    "\n",
    "    return token_list\n",
    "\n",
    "\n",
    "def stop_word_removal(text: list[spacy.tokens.token.Token]) -> list[spacy.tokens.token.Token]: \n",
    "    \"\"\"Use this function to remove stop words. \n",
    "\n",
    "    :param text: Tokens to remove stop words from \n",
    "    :type text: list[spacy.tokens.token.Token]\n",
    "    :return: Tokens without stop words\n",
    "    :rtype: list[spacy.tokens.token.Token]\n",
    "    \"\"\"\n",
    "\n",
    "    token_list_without_stop = []\n",
    "    # Don't add token to list if stop word\n",
    "    for token in text:\n",
    "        if token.is_stop == False: \n",
    "            token_list_without_stop.append(token)\n",
    "\n",
    "    return token_list_without_stop\n",
    "\n",
    "\n",
    "def punctutation_removal(text: list[spacy.tokens.token.Token]) -> list[spacy.tokens.token.Token]: \n",
    "    \"\"\"Use this function to remove punctuation.\n",
    "\n",
    "    :param text: Tokens to remove punctuation from\n",
    "    :type text: list[spacy.tokens.token.Token]\n",
    "    :return: Tokens without punctuation\n",
    "    :rtype: list[spacy.tokens.token.Token]\n",
    "    \"\"\"\n",
    "\n",
    "    token_list_no_stop_no_punct = []\n",
    "    # Don't add token to list if punctuation\n",
    "    for token in text:\n",
    "        if token.is_punct == False:\n",
    "            token_list_no_stop_no_punct.append(token)\n",
    "\n",
    "    return token_list_no_stop_no_punct\n",
    "\n",
    "\n",
    "def lemmatization(text: list[spacy.tokens.token.Token]) -> list[str]: \n",
    "    \"\"\"Use this function to lemmatize a given text.\n",
    "\n",
    "    :param text: Tokens to lemmatize\n",
    "    :type text: list[spacy.tokens.token.Token]\n",
    "    :return: lemmatized tokens\n",
    "    :rtype: list[str]\n",
    "    \"\"\"\n",
    "\n",
    "    token_list_no_stop_no_punct_lemmatized = []\n",
    "    for token in text: \n",
    "        if \"\\n\" not in token.lemma_:\n",
    "            token_list_no_stop_no_punct_lemmatized.append(token.lemma_)\n",
    "    return token_list_no_stop_no_punct_lemmatized\n",
    "\n",
    "\n",
    "def processing_pipeline(song_data: dict) -> dict:\n",
    "    \"\"\"Use this function to execute the entire processing pipeline on given song data.\n",
    "    Preprocessing steps:\n",
    "    - Tokenization\n",
    "    - Stop word removal\n",
    "    - Punctuation removal\n",
    "    - Lemmatization\n",
    "    - ...\n",
    "\n",
    "    :param song_data: song data saved in a json file containing song name, artist name and lyrics\n",
    "    :type song_data: dict\n",
    "    :return: preprocessed song data\n",
    "    :rtype: dict\n",
    "    \"\"\"\n",
    "\n",
    "    nlp = spacy.load(\"en_core_web_sm\", disable = ['ner'])\n",
    "    text_nlp_pipe = list(nlp.pipe([song_data[\"Lyrics\"]]))\n",
    "    \n",
    "    # Tokenization\n",
    "    song_data[\"Lyrics\"] = tokenization(text_nlp_pipe)\n",
    "    # Stop word removal\n",
    "    song_data[\"Lyrics\"] = stop_word_removal(song_data[\"Lyrics\"])\n",
    "    # Punctuation removal\n",
    "    song_data[\"Lyrics\"] = punctutation_removal(song_data[\"Lyrics\"])\n",
    "    # Lemmatization\n",
    "    song_data[\"Lyrics\"] = lemmatization(song_data[\"Lyrics\"])\n",
    "\n",
    "    return song_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "578cbdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test model:\n",
    "test_song = {\n",
    "        \"Song\": \"Mockingbird\",\n",
    "        \"Artist\": \"Eminem\",\n",
    "        \"Lyrics\": (\n",
    "            \"\"\"Yeah\n",
    "            I know sometimes things may not always make sense to you right now\n",
    "            But hey, what daddy always tell you?\n",
    "            Straighten up little soldier\n",
    "            Stiffen up that upper lip\n",
    "            What you crying about?\n",
    "            You got me\n",
    "            Hailie, I know you miss your mom, and I know you miss your dad\n",
    "            When I'm gone, but I'm trying to give you the life that I never had\n",
    "            I can see you're sad, even when you smile, even when you laugh\n",
    "            I can see it in your eyes, deep inside you want to cry\n",
    "            'Cause you're scared, I ain't there, daddy's with you in your prayers\n",
    "            No more crying, wipe them tears, daddy's here, no more nightmares\n",
    "            We gon' pull together through it, we gon' do it\n",
    "            Laney uncle's crazy, ain't he? Yeah, but he loves you girl and you better know it\n",
    "            We're all we got in this world, when it spins, when it swirls\n",
    "            When it whirls, when it twirls, two little beautiful girls\n",
    "            Lookin' puzzled, in a daze, I know it's confusing you\n",
    "            Daddy's always on the move, mamma's always on the news\n",
    "            I try to keep you sheltered from it, but somehow it seems\n",
    "            The harder that I try to do that, the more it backfires on me\n",
    "            All the things growing up, his daddy, daddy had to see\n",
    "            Daddy don't want you to see, but you see just as much as he did\n",
    "            We did not plan it to be this way, your mother and me\n",
    "            But things have gotten so bad between us, I don't see us ever being together\n",
    "            Ever again like we used to be when we was teenagers\n",
    "            But then of course everything always happens for a reason\n",
    "            I guess it was never meant to be\n",
    "            But it's just something we have no control, over and that's what destiny is\n",
    "            But no more worries, rest your head and go to sleep\n",
    "            Maybe one day we'll wake up, and this will all just be a dream\n",
    "            Now hush little baby, don't you cry\n",
    "            Everything's gonna be alright\n",
    "            Stiffen that upper-lip up, little lady, I told ya\n",
    "            Daddy's here to hold ya through the night\n",
    "            I know mommy's not here right now, and we don't know why\n",
    "            We fear how we feel inside\n",
    "            It may seem a little crazy, pretty baby\n",
    "            But I promise momma's gon' be alright\n",
    "            Huh, it's funny\n",
    "            I remember back one year when daddy had no money\n",
    "            Mommy wrapped the Christmas presents up and stuck 'em under the tree\n",
    "            And said, \"Some of 'em were from me, 'cause Daddy couldn't buy 'em\"\n",
    "            I'll never forget that Christmas, I sat up the whole night crying\n",
    "            'Cause daddy felt like a bum\n",
    "            See daddy had a job\n",
    "            But his job was to keep the food on the table for you and mom\n",
    "            And at the time every house that we lived in\n",
    "            Either kept getting broke into and robbed\n",
    "            Or shot up on the block\n",
    "            And your Mom was saving money for you in a jar\n",
    "            Tryna start a piggy bank for you, so you could go to college\n",
    "            Almost had a thousand dollars 'til someone broke in and stole it\n",
    "            And I know it hurt so bad, it broke your momma's heart\n",
    "            And it seemed like everything was just startin' to fall apart\n",
    "            Mom and dad was arguin' a lot, so momma moved back\n",
    "            On the Chalmers in the flat one-bedroom apartment\n",
    "            And dad moved back to the other side of 8 Mile on Novara\n",
    "            And that's when daddy went to California with his C.D\n",
    "            And met Dr. Dre, and flew you and momma out to see me\n",
    "            But daddy had to work, you and momma had to leave me\n",
    "            Then you started seeing daddy on the T.V\n",
    "            And momma didn't like it, and you and Laney were to young to understand it\n",
    "            Papa was a rollin' stone, momma developed a habit\n",
    "            And it all happened too fast for either one of us to grab it\n",
    "            I'm just sorry you were there and had to witness it first hand\n",
    "            'Cause all I ever wanted to do was just make you proud\n",
    "            Now I'm sittin' in this empty house\n",
    "            Just reminiscing, lookin' at your baby pictures\n",
    "            It just trips me out\n",
    "            To see how much you both have grown\n",
    "            It's almost like you're sisters now\n",
    "            Wow, guess you pretty much are, and daddy's still here\n",
    "            Laney, I'm talkin' to you too, daddy's still here\n",
    "            I like the sound of that, yeah, It's got a ring to it, don't it?\n",
    "            Shh, momma's only gone for the moment\n",
    "            Now hush little baby, don't you cry\n",
    "            Everything's gonna be alright\n",
    "            Stiffen that upper-lip up, little lady, I told ya\n",
    "            Daddy's here to hold ya through the night\n",
    "            I know mommy's not here right now, and we don't know why\n",
    "            We fear how we feel inside\n",
    "            It may seem a little crazy, pretty baby\n",
    "            But I promise, momma's gon' be alright\n",
    "            And if you ask me too\n",
    "            Daddy's gonna buy you a Mockingbird\n",
    "            I'ma give you the world\n",
    "            I'ma buy a diamond ring for you, I'ma sing for you\n",
    "            I'll do anything for you to see you smile\n",
    "            And if that Mockingbird don't sing, and that ring don't shine\n",
    "            I'ma break that birdies neck\n",
    "            I'd go back to the jeweler who sold it to ya\n",
    "            And make him eat every karat, don't fuck with dad (haha)\"\"\"\n",
    "        ),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "8781897a",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_song = processing_pipeline(test_song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "b0c51077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['yeah', 'know', 'thing', 'sense', 'right', 'hey', 'daddy', 'tell', 'straighten', 'little', 'soldier', 'Stiffen', 'upper', 'lip', 'cry', 'get', 'Hailie', 'know', 'miss', 'mom', 'know', 'miss', 'dad', 'go', 'try', 'life', 'sad', 'smile', 'laugh', 'eye', 'deep', 'inside', 'want', 'cry', \"'cause\", 'scared', 'be', 'daddy', 'prayer', 'crying', 'wipe', 'tear', 'daddy', 'nightmare', 'gon', 'pull', 'gon', 'Laney', 'uncle', 'crazy', 'be', 'yeah', 'love', 'girl', 'well', 'know', 'get', 'world', 'spin', 'swirl', 'whirl', 'twirl', 'little', 'beautiful', 'girl', 'Lookin', 'puzzle', 'daze', 'know', 'confuse', 'Daddy', 'mamma', 'news', 'try', 'shelter', 'hard', 'try', 'backfire', 'thing', 'grow', 'daddy', 'daddy', 'daddy', 'want', 'plan', 'way', 'mother', 'thing', 'get', 'bad', 'like', 'teenager', 'course', 'happen', 'reason', 'guess', 'mean', 'control', 'destiny', 'worry', 'rest', 'head', 'sleep', 'maybe', 'day', 'wake', 'dream', 'hush', 'little', 'baby', 'cry', 'go', 'to', 'alright', 'Stiffen', 'upper', 'lip', 'little', 'lady', 'tell', 'ya', 'Daddy', 'hold', 'ya', 'night', 'know', 'mommy', 'right', 'know', 'fear', 'feel', 'inside', 'little', 'crazy', 'pretty', 'baby', 'promise', 'momma', 'gon', 'alright', 'Huh', 'funny', 'remember', 'year', 'daddy', 'money', 'Mommy', 'wrap', 'Christmas', 'present', 'stick', 'them', 'tree', 'say', 'them', \"'cause\", 'daddy', 'buy', 'them', 'forget', 'Christmas', 'sit', 'night', 'cry', \"'cause\", 'daddy', 'feel', 'like', 'bum', 'daddy', 'job', 'job', 'food', 'table', 'mom', 'time', 'house', 'live', 'keep', 'getting', 'break', 'rob', 'shoot', 'block', 'Mom', 'save', 'money', 'jar', 'Tryna', 'start', 'piggy', 'bank', 'college', 'thousand', 'dollar', 'til', 'break', 'steal', 'know', 'hurt', 'bad', 'break', 'momma', 'heart', 'like', 'startin', 'fall', 'apart', 'Mom', 'dad', 'arguin', 'lot', 'momma', 'move', 'Chalmers', 'flat', 'bedroom', 'apartment', 'dad', 'move', '8', 'Mile', 'Novara', 'daddy', 'go', 'California', 'C.D', 'meet', 'Dr.', 'Dre', 'fly', 'momma', 'daddy', 'work', 'momma', 'leave', 'start', 'see', 'daddy', 'T.V', 'momma', 'like', 'Laney', 'young', 'understand', 'Papa', 'rollin', 'stone', 'momma', 'develop', 'habit', 'happen', 'fast', 'grab', 'sorry', 'witness', 'hand', \"'cause\", 'want', 'proud', 'sittin', 'house', 'reminiscing', 'lookin', 'baby', 'picture', 'trip', 'grow', 'like', 'sister', 'wow', 'guess', 'pretty', 'daddy', 'Laney', 'talkin', 'daddy', 'like', 'sound', 'yeah', 'get', 'ring', 'Shh', 'momma', 'go', 'moment', 'hush', 'little', 'baby', 'cry', 'go', 'to', 'alright', 'Stiffen', 'upper', 'lip', 'little', 'lady', 'tell', 'ya', 'Daddy', 'hold', 'ya', 'night', 'know', 'mommy', 'right', 'know', 'fear', 'feel', 'inside', 'little', 'crazy', 'pretty', 'baby', 'promise', 'momma', 'gon', 'alright', 'ask', 'Daddy', 'go', 'to', 'buy', 'Mockingbird', 'world', 'buy', 'diamond', 'ring', 'sing', 'smile', 'Mockingbird', 'sing', 'ring', 'shine', 'break', 'birdie', 'neck', 'jeweler', 'sell', 'ya', 'eat', 'karat', 'fuck', 'dad', 'haha']\n"
     ]
    }
   ],
   "source": [
    "print(processed_song[\"Lyrics\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "e72b5d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the tokenizer\n",
    "with open('tokenizer.pickle', 'rb') as handle:\n",
    "    token = pickle.load(handle)\n",
    "# tokenize\n",
    "text = token.texts_to_sequences([processed_song[\"Lyrics\"]])\n",
    "text = pad_sequences(text, 180)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "1f801eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  179    26    60    62   447    10     3  1320   447   775   775  1071\n",
      "    855  1017     8   265    41   218  1169    53  1722   413   869  1017\n",
      "    156   211  2485  1636    82  6451  1412  3248   459   900   285    53\n",
      "    371     2   136    98    53  1139    21     3  2863    37   258  1017\n",
      "    906 17626   304  1139   255  1561  1258  2518   906   255  1689   392\n",
      "    447     4   929   147  2364   126  1139   447   150  1139    24    82\n",
      "     73   447  1139     3 19532   189   162  1033  1047   314  1139  6769\n",
      "   1766   232   242   702   291  1315    49    62     9   757  1193   265\n",
      "  14576   528    14   365   664   180     3   601  1212   208   267   447\n",
      "  19532   603   447     3   167    18     6   317  6076  1139     4   249\n",
      "   1099    43    14    60     4    13   166  9315  3856   334    43   313\n",
      "     20   110   447    40   110    26     2  2008    27     2   159    10\n",
      "     64    43   182   267    14   262  1139   571   166   140   447     4\n",
      "     13   349 12588    33   349   638   317   121   124 12588   121   317\n",
      "    144    53  7780   797  8104   445   110   424 12472   153   906  3373]]\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "70e71d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 21ms/step\n"
     ]
    }
   ],
   "source": [
    "pred = keras_model.predict(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "1a8601a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.03035141 0.07305858 0.01072862 0.00733498 0.10687384 0.01872347\n",
      "  0.00504669 0.09882194 0.00760916 0.00933376 0.00872805 0.02308791\n",
      "  0.19617064 0.00064153 0.08658496 0.19915532 0.11774925]]\n"
     ]
    }
   ],
   "source": [
    "print(pred)\n",
    "# propability distr."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "de5d65b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_mood=np.argmax(pred,axis=1)\n",
    "# get the mood that is predicted the most "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "0a0dde91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15]\n"
     ]
    }
   ],
   "source": [
    "print(pred_mood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "1dc196a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sad']\n"
     ]
    }
   ],
   "source": [
    "# load the label encoder\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "encoder.classes_ = numpy.load('label_encoder.npy', allow_pickle=True)\n",
    "\n",
    "print(encoder.inverse_transform(pred_mood))\n",
    "# reverse transform the mood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ac45eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ta_3.10",
   "language": "python",
   "name": "ta_3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
