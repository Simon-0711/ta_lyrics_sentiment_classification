{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7027cc2",
   "metadata": {},
   "source": [
    "## This notebook was for the first cut solution of a CNN + for evaluating the different dataset we generated from our data in accordance with the CNN predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "43598701",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.keras.layers import Dense, Embedding,GlobalMaxPooling1D\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Embedding\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Dropout, Conv1D, MaxPool1D, GlobalMaxPool1D, Embedding, Activation\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import PorterStemmer\n",
    "from sklearn import preprocessing\n",
    "import pickle\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "# Preprocessing and model is used from \n",
    "# https://www.kaggle.com/code/jagannathrk/word2vec-cnn-text-classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2446eea8",
   "metadata": {},
   "source": [
    "## Adapt the preprocessing pipeline to the songs so thath the input is cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1e0cdeee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "def tokenization(text):\n",
    "    \"\"\"Use this function to tokenize text.\n",
    "\n",
    "    :param text: Text as list\n",
    "    :type text: list[spacy.tokens.token.Token]\n",
    "    :return: Tokenized text as list\n",
    "    :rtype: list[spacy.tokens.token.Token]\n",
    "    \"\"\"\n",
    "\n",
    "    token_list = []\n",
    "    for doc in text: \n",
    "        # iterate over tokens in docs\n",
    "        for token in doc:\n",
    "            token_list.append(token)\n",
    "\n",
    "    return token_list\n",
    "\n",
    "\n",
    "def stop_word_removal(text): \n",
    "    \"\"\"Use this function to remove stop words. \n",
    "\n",
    "    :param text: Tokens to remove stop words from \n",
    "    :type text: list[spacy.tokens.token.Token]\n",
    "    :return: Tokens without stop words\n",
    "    :rtype: list[spacy.tokens.token.Token]\n",
    "    \"\"\"\n",
    "\n",
    "    token_list_without_stop = []\n",
    "    # Don't add token to list if stop word\n",
    "    for token in text:\n",
    "        if token.is_stop == False: \n",
    "            token_list_without_stop.append(token)\n",
    "\n",
    "    return token_list_without_stop\n",
    "\n",
    "\n",
    "def punctutation_removal(text): \n",
    "    \"\"\"Use this function to remove punctuation.\n",
    "\n",
    "    :param text: Tokens to remove punctuation from\n",
    "    :type text: list[spacy.tokens.token.Token]\n",
    "    :return: Tokens without punctuation\n",
    "    :rtype: list[spacy.tokens.token.Token]\n",
    "    \"\"\"\n",
    "\n",
    "    token_list_no_stop_no_punct = []\n",
    "    # Don't add token to list if punctuation\n",
    "    for token in text:\n",
    "        if token.is_punct == False:\n",
    "            token_list_no_stop_no_punct.append(token)\n",
    "\n",
    "    return token_list_no_stop_no_punct\n",
    "\n",
    "\n",
    "def lemmatization(text): \n",
    "    \"\"\"Use this function to lemmatize a given text.\n",
    "\n",
    "    :param text: Tokens to lemmatize\n",
    "    :type text: list[spacy.tokens.token.Token]\n",
    "    :return: lemmatized tokens\n",
    "    :rtype: list[str]\n",
    "    \"\"\"\n",
    "\n",
    "    token_list_no_stop_no_punct_lemmatized = []\n",
    "    for token in text: \n",
    "        if \"\\n\" not in token.lemma_:\n",
    "            token_list_no_stop_no_punct_lemmatized.append(token.lemma_)\n",
    "    return token_list_no_stop_no_punct_lemmatized\n",
    "\n",
    "\n",
    "def processing_pipeline(song_data):\n",
    "    \"\"\"Use this function to execute the entire processing pipeline on given song data.\n",
    "    Preprocessing steps:\n",
    "    - Tokenization\n",
    "    - Stop word removal\n",
    "    - Punctuation removal\n",
    "    - Lemmatization\n",
    "    - ...\n",
    "\n",
    "    :param song_data: song data saved in a json file containing song name, artist name and lyrics\n",
    "    :type song_data: dict\n",
    "    :return: preprocessed song data\n",
    "    :rtype: dict\n",
    "    \"\"\"\n",
    "\n",
    "    nlp = spacy.load(\"en_core_web_sm\", disable = ['ner'])\n",
    "    \n",
    "    for row in range(len(song_data)):\n",
    "        text_nlp_pipe = list(nlp.pipe([song_data.iloc[row][\"Lyric\"]]))\n",
    "    \n",
    "        # Tokenization\n",
    "        song_data.at[row,\"Lyric\"] = tokenization(text_nlp_pipe)\n",
    "        # Stop word removal\n",
    "        song_data.at[row,\"Lyric\"] = stop_word_removal(song_data.iloc[row][\"Lyric\"])\n",
    "        # Punctuation removal\n",
    "        song_data.at[row,\"Lyric\"] = punctutation_removal(song_data.iloc[row][\"Lyric\"])\n",
    "        # Lemmatization\n",
    "        song_data.at[row,\"Lyric\"] = lemmatization(song_data.iloc[row][\"Lyric\"])\n",
    "        song_data.at[row,\"Lyric\"] = \" \".join(song_data.iloc[row][\"Lyric\"])\n",
    "        song_data.at[row,\"Lyric\"] = song_data.iloc[row][\"Lyric\"].lower()\n",
    "        if row%500 == 0 and row >= 500:\n",
    "            print(f\"processsed: {row} rows out of {len(song_data)}\")\n",
    "    return song_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f400ebb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to use the word2vec as an layer in keras \n",
    "# taken from the gensim wikipage: https://github.com/RaRe-Technologies/gensim/wiki/Using-Gensim-Embeddings-with-Keras-and-Tensorflow\n",
    "\n",
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "def gensim_to_keras_embedding(model, train_embeddings=False):\n",
    "    \"\"\"Get a Keras 'Embedding' layer with weights set from Word2Vec model's learned word embeddings.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_embeddings : bool\n",
    "        If False, the returned weights are frozen and stopped from being updated.\n",
    "        If True, the weights can / will be further updated in Keras.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    `keras.layers.Embedding`\n",
    "        Embedding layer, to be used as input to deeper network layers.\n",
    "\n",
    "    \"\"\"\n",
    "    keyed_vectors = model.wv  # structure holding the result of training\n",
    "    weights = keyed_vectors.vectors  # vectors themselves, a 2D numpy array    \n",
    "    index_to_key = keyed_vectors.index_to_key  # which row in `weights` corresponds to which word?\n",
    "\n",
    "    layer = Embedding(\n",
    "        input_dim=weights.shape[0],\n",
    "        output_dim=weights.shape[1],\n",
    "        weights=[weights],\n",
    "        trainable=train_embeddings,\n",
    "    )\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ebb8a83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(mood_count,filters,multiplicator, multiplicator2, kernel, adapt_embedding, outputfunction):\n",
    "    \n",
    "    keras_model = Sequential()\n",
    "    keras_model.add(gensim_to_keras_embedding(word2vec_model, True))\n",
    "    keras_model.add(Dropout(0.2))\n",
    "    keras_model.add(Conv1D(filters, kernel, activation='relu', padding='same', strides=1))\n",
    "    keras_model.add(Conv1D(filters, kernel, activation='relu', padding='same', strides=1))\n",
    "    keras_model.add(MaxPool1D())\n",
    "    keras_model.add(Dropout(0.2))\n",
    "    keras_model.add(Conv1D(filters*multiplicator, kernel, activation='relu', padding='same', strides=1))\n",
    "    keras_model.add(Conv1D(filters*multiplicator, kernel, activation='relu', padding='same', strides=1))\n",
    "    keras_model.add(MaxPool1D())\n",
    "    keras_model.add(Dropout(0.2))\n",
    "    keras_model.add(Conv1D(filters*multiplicator2, kernel, activation='relu', padding='same', strides=1))\n",
    "    keras_model.add(Conv1D(filters*multiplicator2, kernel, activation='relu', padding='same', strides=1))\n",
    "    keras_model.add(GlobalMaxPool1D())\n",
    "    keras_model.add(Dropout(0.2))\n",
    "    keras_model.add(Dense(200))\n",
    "    keras_model.add(Activation('relu'))\n",
    "    keras_model.add(Dropout(0.2))\n",
    "    # Number of moods to be classified to\n",
    "    keras_model.add(Dense(mood_count))\n",
    "    keras_model.add(Activation(outputfunction))\n",
    "    \n",
    "    keras_model.compile(loss='binary_crossentropy', metrics=['acc'], optimizer='adam')\n",
    "    return keras_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2ad98c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_dataset(dataset_path, mood_count):\n",
    "    print(f\"Testing now dataset {dataset_path} with {mood_count} moods\")\n",
    "    df = pd.read_csv(dataset_path)\n",
    "    print((df[\"Mood\"].unique()))\n",
    "    df = processing_pipeline(df)\n",
    "    list(df[\"Lyric\"].head(2))\n",
    "    # merge lyrics together\n",
    "    lyrics = []\n",
    "    for i in df['Lyric']:\n",
    "        lyrics.append(i.split())\n",
    "    print(lyrics[:2])\n",
    "    \n",
    "    # train the word2vec model\n",
    "    # vector size according to https://moj-analytical-services.github.io/NLP-guidance/NNmodels.html#:~:text=The%20standard%20Word2Vec%20pre%2Dtrained,fewer%20dimensions%20to%20represent%20them.\n",
    "    # mincount = 2 to prevent misspellings\n",
    "    word2vec_model = Word2Vec(lyrics, vector_size=150, window=5, min_count=2, workers=16)\n",
    "    \n",
    "    # use the keras tokenizer and apply it to the lyrics\n",
    "    # number in first row is vocab size from word2vec model\n",
    "    token = Tokenizer(len(word2vec_model.wv))\n",
    "    token.fit_on_texts(df['Lyric'])\n",
    "    text = token.texts_to_sequences(df['Lyric'])\n",
    "    text = pad_sequences(text, 180)\n",
    "    print(text[3:5])\n",
    "    \n",
    "    # encode the labels\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    y = le.fit_transform(df['Mood'])\n",
    "    y = to_categorical(y)\n",
    "    # save the label encoder\n",
    "    np.save('label_encoder.npy', le.classes_)\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(np.array(text), y, test_size=0.2, stratify=y)\n",
    "    \n",
    "    seed = 7\n",
    "    np.random.seed(seed)\n",
    "    batch_size = 32\n",
    "    epochs = 3\n",
    "\n",
    "    model_CV = KerasClassifier(build_fn=create_model, epochs=epochs, \n",
    "                               batch_size=batch_size, verbose=1)\n",
    "    # grid search parameters\n",
    "    filters = [50, 100]\n",
    "    multiplicator = [2]\n",
    "    multiplicator2 = [3,4]\n",
    "    kernel = [5]\n",
    "    adapt_embedding = [True, False]\n",
    "    outputfunction = [\"relu\",\"softmax\"]\n",
    "\n",
    "    param_grid = dict(mood_count=[mood_count], filters=filters, multiplicator=multiplicator, multiplicator2=multiplicator2, kernel=kernel, adapt_embedding=adapt_embedding, outputfunction=outputfunction)\n",
    "    grid = GridSearchCV(estimator=model_CV, param_grid=param_grid, n_jobs=-1, cv=2)\n",
    "    grid_result = grid.fit(x_train, y_train)\n",
    "    \n",
    "    \n",
    "    # print results\n",
    "    print(f'Best Accuracy for dataset {dataset_path} is: {grid_result.best_score_} using {grid_result.best_params_}')\n",
    "    means = grid_result.cv_results_['mean_test_score']\n",
    "    stds = grid_result.cv_results_['std_test_score']\n",
    "    params = grid_result.cv_results_['params']\n",
    "    for mean, stdev, param in zip(means, stds, params):\n",
    "        print(f' mean={mean:.4}, std={stdev:.4} using {param}')\n",
    "    print(\"##\"*50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2ea2bcaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing now dataset ./data/song-data-labels-cleaned.csv with 17 moods\n",
      "['romantic' 'happy' 'sad' 'grief' 'upbeat' 'earnest' 'cheerful'\n",
      " 'depressed' 'anger' 'calm' 'excitement' 'aggression' 'confident' 'angst'\n",
      " 'brooding' 'desire' 'pessimism']\n",
      "processsed: 500 rows out of 27705\n",
      "processsed: 1000 rows out of 27705\n",
      "processsed: 1500 rows out of 27705\n",
      "processsed: 2000 rows out of 27705\n",
      "processsed: 2500 rows out of 27705\n",
      "processsed: 3000 rows out of 27705\n",
      "processsed: 3500 rows out of 27705\n",
      "processsed: 4000 rows out of 27705\n",
      "processsed: 4500 rows out of 27705\n",
      "processsed: 5000 rows out of 27705\n",
      "processsed: 5500 rows out of 27705\n",
      "processsed: 6000 rows out of 27705\n",
      "processsed: 6500 rows out of 27705\n",
      "processsed: 7000 rows out of 27705\n",
      "processsed: 7500 rows out of 27705\n",
      "processsed: 8000 rows out of 27705\n",
      "processsed: 8500 rows out of 27705\n",
      "processsed: 9000 rows out of 27705\n",
      "processsed: 9500 rows out of 27705\n",
      "processsed: 10000 rows out of 27705\n",
      "processsed: 10500 rows out of 27705\n",
      "processsed: 11000 rows out of 27705\n",
      "processsed: 11500 rows out of 27705\n",
      "processsed: 12000 rows out of 27705\n",
      "processsed: 12500 rows out of 27705\n",
      "processsed: 13000 rows out of 27705\n",
      "processsed: 13500 rows out of 27705\n",
      "processsed: 14000 rows out of 27705\n",
      "processsed: 14500 rows out of 27705\n",
      "processsed: 15000 rows out of 27705\n",
      "processsed: 15500 rows out of 27705\n",
      "processsed: 16000 rows out of 27705\n",
      "processsed: 16500 rows out of 27705\n",
      "processsed: 17000 rows out of 27705\n",
      "processsed: 17500 rows out of 27705\n",
      "processsed: 18000 rows out of 27705\n",
      "processsed: 18500 rows out of 27705\n",
      "processsed: 19000 rows out of 27705\n",
      "processsed: 19500 rows out of 27705\n",
      "processsed: 20000 rows out of 27705\n",
      "processsed: 20500 rows out of 27705\n",
      "processsed: 21000 rows out of 27705\n",
      "processsed: 21500 rows out of 27705\n",
      "processsed: 22000 rows out of 27705\n",
      "processsed: 22500 rows out of 27705\n",
      "processsed: 23000 rows out of 27705\n",
      "processsed: 23500 rows out of 27705\n",
      "processsed: 24000 rows out of 27705\n",
      "processsed: 24500 rows out of 27705\n",
      "processsed: 25000 rows out of 27705\n",
      "processsed: 25500 rows out of 27705\n",
      "processsed: 26000 rows out of 27705\n",
      "processsed: 26500 rows out of 27705\n",
      "processsed: 27000 rows out of 27705\n",
      "processsed: 27500 rows out of 27705\n",
      "[['lie', 'floor', 'feather', 'find', 'hand', 'wanna', 'believe', 'not', 'think', 's', 'god', 'heaven', 'hell', 'lose', 'lookin', 'sign', 'maybe', 'fool', 'swear', 'spine', 'get', 'shiver', 'like', 'kiss', 'time', 'tho', 'tear', 'stream', 'like', 'river', 'know', 'ask', 'sign', 'scare', 'tho', 'body', 'start', 'shakin', 'feel', 'dry', 'eye', 'tho', 'heart', 'break', 'know', 'love', 'sign', 'sign', 'findin', 'street', 'starin', 'reach', 'turn', 'lookin', 'stranger', 'eye', 's', 'god', 'heaven', 'hell', 'tt', 'feel', 'like', 'walk', 'cos', 'touch', 'like', 'swear', 'spine', 'get', 'shiver', 'like', 'kiss', 'time', 'tho', 'tear', 'stream', 'like', 'river', 'know', 'ask', 'sign', 'scare', 'tho', 'body', 'start', 'shakin', 'feel', 'dry', 'eye', 'tho', 'heart', 'break', 'know', 'love', 'sign', 'sign', 'lie', 'floor', 'feather', 'find', 'hand', 'wanna', 'believe', 'not', 'think', 'swear', 'spine', 'get', 'shiver', 'like', 'kiss', 'time', 'tho', 'tear', 'stream', 'like', 'river', 'know', 'ask', 'sign', 'scare', 'tho', 'body', 'start', 'shakin', 'feel', 'dry', 'eye', 'tho', 'heart', 'break', 'know', 'love', 'sign', 'sign'], ['oh', 'lord', 'like', 'know', 'think', 'oh', 'oh', 'lord', 'wanna', 'love', 'life', 'heart', 'wonder', 'right', 'try', 'forget', 'near', 'darle', 'miss', 'darling', 'oh', 'bring', 'darle', 'sweet', 'kiss', 'darle', 'think', 'happy', 'think', 'mistake', 'think', 'darling', 'right', 'wrong', 'near', 'darle', 'miss', 'darling', 'oh', 'bring', 'darle', 'sweet', 'kiss', 'darle', 'oh', 'jah', 'oh', 'jah', 'like', 'near', 'oh', 'jah', 'oh', 'jah', 'miss', 'love']]\n",
      "[[    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0    78    18   224   735   249   175   235     9   267  1204   412\n",
      "    541    35   413     9  3797    52   347    78    16   161    10     1\n",
      "     35   394    68    36   177    52     2   136    44     9    52    67\n",
      "   1409   148   787    78    46   418    19    53    16   104   490    52\n",
      "      2  1993    47     5    55     5    55    78    16   161    10     1\n",
      "     35   394    68    36   177    52     2   136    44     9    52    67\n",
      "   1409   148   787    43   207     5   450    16   435     3    16    47\n",
      "     16   100    78   161    20   161    10     1    35   438   107    68\n",
      "     36   177    96   136    44     9    52    67  1409   148   787    78]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0   106   160   281   281   120    12   761   321  2678\n",
      "     10    81   376   706   132   801    15   130   140   123   580   580\n",
      "      2   342   416   139    87   430    14     1    22     5    14   108\n",
      "     22    70   430     1 14839     5    14     1     1 14839     5    14\n",
      "      1     1    14    14    45   187  1320   218    65    86    26    15\n",
      "     78   331   332     3    26  1978   110   130   140   123   580   580\n",
      "      1   136     2     4    13   129   152   342   416   139    87   430\n",
      "   4265    14     1    22     5    14   108    22    70   430     1 14839\n",
      "     14     1     1 14839     5    14     1     1    14     1    22     5\n",
      "     14   108    22    70   430     1    14     1    22     5    14   108\n",
      "     22    70   430     1    14     1    22     5    14   108    22    70\n",
      "    430     1    14     1    22    14    14    14   108 26280 17050 26281]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1834/658696831.py:44: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  model_CV = KerasClassifier(build_fn=create_model, epochs=epochs,\n",
      "2023-02-18 09:47:19.426113: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-18 09:47:19.455471: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-18 09:47:19.480374: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-18 09:47:19.547145: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-18 09:47:19.605113: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-18 09:47:19.660453: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-02-18 09:47:19.660886: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-02-18 09:47:19.675881: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-02-18 09:47:19.676035: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-02-18 09:47:19.702067: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-02-18 09:47:19.702181: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-02-18 09:47:19.703443: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-18 09:47:19.717937: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-18 09:47:19.768552: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-02-18 09:47:19.768674: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-02-18 09:47:19.771947: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-18 09:47:19.828500: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-02-18 09:47:19.828634: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-02-18 09:47:19.947337: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-02-18 09:47:19.947796: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-02-18 09:47:19.986414: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-02-18 09:47:19.986592: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-02-18 09:47:20.051300: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-02-18 09:47:20.051501: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-02-18 09:47:20.954093: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-18 09:47:20.954667: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-18 09:47:20.954765: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-02-18 09:47:20.970882: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-18 09:47:20.971094: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-18 09:47:20.971145: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-02-18 09:47:21.026343: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-18 09:47:21.027606: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-18 09:47:21.027724: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-18 09:47:21.183100: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-18 09:47:21.183772: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-18 09:47:21.183863: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-02-18 09:47:21.191589: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-18 09:47:21.191809: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-18 09:47:21.191877: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-02-18 09:47:21.261149: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-18 09:47:21.261567: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-18 09:47:21.261645: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-02-18 09:47:21.327011: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-18 09:47:21.327290: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-18 09:47:21.327369: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-02-18 09:47:21.374949: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-18 09:47:21.376394: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-18 09:47:21.376569: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-02-18 09:47:22.623589: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-02-18 09:47:22.623816: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-02-18 09:47:22.623849: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (LAPTOP-D6IC8GJA): /proc/driver/nvidia/version does not exist\n",
      "2023-02-18 09:47:22.624567: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-18 09:47:22.667672: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-02-18 09:47:22.667793: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-02-18 09:47:22.667973: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (LAPTOP-D6IC8GJA): /proc/driver/nvidia/version does not exist\n",
      "2023-02-18 09:47:22.668651: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-18 09:47:22.692627: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-02-18 09:47:22.692704: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-02-18 09:47:22.692741: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (LAPTOP-D6IC8GJA): /proc/driver/nvidia/version does not exist\n",
      "2023-02-18 09:47:22.693137: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-18 09:47:22.883672: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-02-18 09:47:22.883860: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-02-18 09:47:22.884144: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (LAPTOP-D6IC8GJA): /proc/driver/nvidia/version does not exist\n",
      "2023-02-18 09:47:22.884645: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-18 09:47:22.891480: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-02-18 09:47:22.891624: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-02-18 09:47:22.891717: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (LAPTOP-D6IC8GJA): /proc/driver/nvidia/version does not exist\n",
      "2023-02-18 09:47:22.892109: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-18 09:47:23.019605: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-02-18 09:47:23.019708: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-02-18 09:47:23.019756: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (LAPTOP-D6IC8GJA): /proc/driver/nvidia/version does not exist\n",
      "2023-02-18 09:47:23.020110: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-18 09:47:23.063520: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-02-18 09:47:23.063651: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-02-18 09:47:23.063710: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (LAPTOP-D6IC8GJA): /proc/driver/nvidia/version does not exist\n",
      "2023-02-18 09:47:23.064092: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-18 09:47:23.205979: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-02-18 09:47:23.206022: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-02-18 09:47:23.206057: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (LAPTOP-D6IC8GJA): /proc/driver/nvidia/version does not exist\n",
      "2023-02-18 09:47:23.206326: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "Epoch 1/3\n",
      "Epoch 1/3\n",
      "Epoch 1/3\n",
      "Epoch 1/3\n",
      "Epoch 1/3\n",
      "Epoch 1/3\n",
      "Epoch 1/3\n",
      "347/347 [==============================] - 246s 697ms/step - loss: 0.1893 - acc: 0.2676\n",
      "317/347 [==========================>...] - ETA: 22s - loss: 0.3351 - acc: 0.2797Epoch 2/3\n",
      "347/347 [==============================] - 247s 700ms/step - loss: 0.4585 - acc: 0.2495\n",
      "Epoch 2/3\n",
      "347/347 [==============================] - 248s 704ms/step - loss: 0.3434 - acc: 0.2577\n",
      "319/347 [==========================>...] - ETA: 21s - loss: 0.3349 - acc: 0.2800Epoch 2/3\n",
      "347/347 [==============================] - 248s 705ms/step - loss: 0.1941 - acc: 0.2601\n",
      "Epoch 2/3\n",
      "347/347 [==============================] - 265s 753ms/step - loss: 0.1883 - acc: 0.2632\n",
      "Epoch 2/3\n",
      "347/347 [==============================] - 268s 763ms/step - loss: 0.3340 - acc: 0.2812\n",
      "  4/347 [..............................] - ETA: 4:46 - loss: 0.1775 - acc: 0.3203Epoch 2/3\n",
      "347/347 [==============================] - 272s 772ms/step - loss: 0.2887 - acc: 0.2716\n",
      "Epoch 2/3\n",
      "347/347 [==============================] - 276s 778ms/step - loss: 0.1889 - acc: 0.2660\n",
      " 43/347 [==>...........................] - ETA: 3:34 - loss: 0.4540 - acc: 0.1504Epoch 2/3\n",
      "347/347 [==============================] - 241s 696ms/step - loss: 0.3425 - acc: 0.2619\n",
      "274/347 [======================>.......] - ETA: 56s - loss: 0.1793 - acc: 0.2933Epoch 3/3\n",
      "347/347 [==============================] - 243s 701ms/step - loss: 0.1788 - acc: 0.2920\n",
      "Epoch 3/3\n",
      "347/347 [==============================] - 243s 700ms/step - loss: 0.2208 - acc: 0.2949\n",
      "Epoch 3/3\n",
      "347/347 [==============================] - 243s 701ms/step - loss: 0.1796 - acc: 0.2803\n",
      "Epoch 3/3\n",
      "347/347 [==============================] - 268s 772ms/step - loss: 0.1784 - acc: 0.2936\n",
      "Epoch 3/3\n",
      "347/347 [==============================] - 266s 766ms/step - loss: 0.3147 - acc: 0.2894\n",
      "Epoch 3/3\n",
      "347/347 [==============================] - 265s 763ms/step - loss: 0.2513 - acc: 0.2946\n",
      "Epoch 3/3\n",
      "347/347 [==============================] - 269s 775ms/step - loss: 0.1796 - acc: 0.2893\n",
      "Epoch 3/3\n",
      "347/347 [==============================] - 237s 683ms/step - loss: 0.3185 - acc: 0.2890\n",
      "347/347 [==============================] - 236s 681ms/step - loss: 0.1770 - acc: 0.3016\n",
      "347/347 [==============================] - 237s 683ms/step - loss: 0.1783 - acc: 0.2899\n",
      "347/347 [==============================] - 240s 691ms/step - loss: 0.2200 - acc: 0.2950\n",
      "347/347 [==============================] - 38s 102ms/step - loss: 0.1795 - acc: 0.2963\n",
      "347/347 [==============================] - 39s 103ms/step - loss: 0.2230 - acc: 0.2897\n",
      "347/347 [==============================] - 238s 686ms/step - loss: 0.1753 - acc: 0.3132\n",
      "232/347 [===================>..........] - ETA: 11s - loss: 0.3139 - acc: 0.3004Epoch 1/3\n",
      "347/347 [==============================] - 240s 691ms/step - loss: 0.3145 - acc: 0.2907\n",
      "347/347 [==============================] - 238s 685ms/step - loss: 0.2455 - acc: 0.2955\n",
      "278/347 [=======================>......] - ETA: 6s - loss: 0.3151 - acc: 0.2980Epoch 1/3\n",
      "347/347 [==============================] - 34s 86ms/step - loss: 0.3153 - acc: 0.2953\n",
      "347/347 [==============================] - 33s 84ms/step - loss: 0.1782 - acc: 0.3011\n",
      "347/347 [==============================] - 236s 679ms/step - loss: 0.1775 - acc: 0.2953\n",
      "100/347 [=======>......................] - ETA: 11s - loss: 0.3143 - acc: 0.2966Epoch 1/3\n",
      "347/347 [==============================] - 19s 48ms/step - loss: 0.3140 - acc: 0.2953\n",
      "  6/347 [..............................] - ETA: 3:24 - loss: 0.3805 - acc: 0.0833Epoch 1/3\n",
      " 29/347 [=>............................] - ETA: 2:54 - loss: 0.5740 - acc: 0.1897Epoch 1/3\n",
      "347/347 [==============================] - 44s 121ms/step - loss: 0.1756 - acc: 0.3249\n",
      "347/347 [==============================] - 45s 124ms/step - loss: 0.2329 - acc: 0.2897\n",
      "347/347 [==============================] - 44s 122ms/step - loss: 0.1764 - acc: 0.3094\n",
      "116/347 [=========>....................] - ETA: 3:27 - loss: 0.2022 - acc: 0.2379Epoch 1/3\n",
      "118/347 [=========>....................] - ETA: 3:25 - loss: 0.2018 - acc: 0.2378Epoch 1/3\n",
      "131/347 [==========>...................] - ETA: 3:05 - loss: 0.3970 - acc: 0.2583Epoch 1/3\n",
      "347/347 [==============================] - 393s 1s/step - loss: 0.9072 - acc: 0.0225\n",
      "148/347 [===========>..................] - ETA: 5:22 - loss: 0.1947 - acc: 0.2540Epoch 2/3\n",
      "347/347 [==============================] - 397s 1s/step - loss: 0.3103 - acc: 0.2813\n",
      "Epoch 2/3\n",
      "347/347 [==============================] - 412s 1s/step - loss: 0.1893 - acc: 0.2614\n",
      "Epoch 2/3\n",
      "347/347 [==============================] - 442s 1s/step - loss: 0.1914 - acc: 0.2633\n",
      "Epoch 2/3\n",
      "347/347 [==============================] - 533s 1s/step - loss: 0.5940 - acc: 0.1771\n",
      "Epoch 2/3\n",
      "347/347 [==============================] - 573s 2s/step - loss: 0.1894 - acc: 0.2574\n",
      "Epoch 2/3\n",
      "347/347 [==============================] - 576s 2s/step - loss: 0.3370 - acc: 0.2870\n",
      "Epoch 2/3\n",
      "347/347 [==============================] - 587s 2s/step - loss: 0.1879 - acc: 0.2677\n",
      "Epoch 2/3\n",
      "347/347 [==============================] - 468s 1s/step - loss: 0.9074 - acc: 0.0220\n",
      "Epoch 3/3\n",
      "347/347 [==============================] - 468s 1s/step - loss: 0.2466 - acc: 0.2952\n",
      "194/347 [===============>..............] - ETA: 4:02 - loss: 0.4098 - acc: 0.2247Epoch 3/3\n",
      "347/347 [==============================] - 468s 1s/step - loss: 0.1793 - acc: 0.2819\n",
      "Epoch 3/3\n",
      "347/347 [==============================] - 465s 1s/step - loss: 0.1793 - acc: 0.2923\n",
      "Epoch 3/3\n",
      "347/347 [==============================] - 544s 2s/step - loss: 0.3393 - acc: 0.2525\n",
      "Epoch 3/3\n",
      "347/347 [==============================] - 540s 2s/step - loss: 0.1794 - acc: 0.2839\n",
      "Epoch 3/3\n",
      "347/347 [==============================] - 539s 2s/step - loss: 0.2780 - acc: 0.2953\n",
      "Epoch 3/3\n",
      "347/347 [==============================] - 542s 2s/step - loss: 0.1792 - acc: 0.2919\n",
      "Epoch 3/3\n",
      "347/347 [==============================] - 463s 1s/step - loss: 0.9073 - acc: 0.0220\n",
      "347/347 [==============================] - 458s 1s/step - loss: 0.2196 - acc: 0.2953\n",
      "347/347 [==============================] - 458s 1s/step - loss: 0.1776 - acc: 0.2910\n",
      "347/347 [==============================] - 448s 1s/step - loss: 0.1782 - acc: 0.2920\n",
      "347/347 [==============================] - 92s 259ms/step - loss: 0.1764 - acc: 0.2941\n",
      "334/347 [===========================>..] - ETA: 3s - loss: 0.2205 - acc: 0.2884Epoch 1/3\n",
      "347/347 [==============================] - 93s 259ms/step - loss: 0.2205 - acc: 0.2897\n",
      "347/347 [==============================] - 94s 263ms/step - loss: 0.9073 - acc: 0.0189\n",
      "257/347 [=====================>........] - ETA: 22s - loss: 0.1768 - acc: 0.2851Epoch 1/3\n",
      "347/347 [==============================] - 85s 233ms/step - loss: 0.1763 - acc: 0.2897\n",
      "164/347 [=============>................] - ETA: 4:01 - loss: 0.1783 - acc: 0.2946Epoch 1/3\n",
      " 69/347 [====>.........................] - ETA: 2:11 - loss: 0.3707 - acc: 0.2061Epoch 1/3\n",
      "347/347 [==============================] - 485s 1s/step - loss: 0.2380 - acc: 0.2895\n",
      "347/347 [==============================] - 225s 574ms/step - loss: 0.2797 - acc: 0.2767\n",
      "Epoch 2/3\n",
      "347/347 [==============================] - 224s 574ms/step - loss: 0.2708 - acc: 0.2502\n",
      "Epoch 2/3\n",
      "347/347 [==============================] - 242s 600ms/step - loss: 0.1901 - acc: 0.2589\n",
      "Epoch 2/3\n",
      "347/347 [==============================] - 483s 1s/step - loss: 0.2778 - acc: 0.2953\n",
      "347/347 [==============================] - 489s 1s/step - loss: 0.1783 - acc: 0.2877\n",
      "347/347 [==============================] - 482s 1s/step - loss: 0.1777 - acc: 0.2952\n",
      "347/347 [==============================] - 114s 314ms/step - loss: 0.2089 - acc: 0.2953\n",
      "347/347 [==============================] - 248s 591ms/step - loss: 0.1884 - acc: 0.2666\n",
      "Epoch 2/3\n",
      "159/347 [============>.................] - ETA: 58s - loss: 0.2794 - acc: 0.2897Epoch 1/3\n",
      "347/347 [==============================] - 111s 313ms/step - loss: 0.2786 - acc: 0.2897\n",
      "347/347 [==============================] - 113s 316ms/step - loss: 0.1761 - acc: 0.2953\n",
      "347/347 [==============================] - 112s 314ms/step - loss: 0.1747 - acc: 0.3132\n",
      "322/347 [==========================>...] - ETA: 13s - loss: 0.2218 - acc: 0.2952Epoch 1/3\n",
      "347/347 [==============================] - 188s 542ms/step - loss: 0.2503 - acc: 0.2877\n",
      "Epoch 3/3\n",
      "347/347 [==============================] - 188s 542ms/step - loss: 0.2209 - acc: 0.2951\n",
      "Epoch 3/3\n",
      " 60/347 [====>.........................] - ETA: 2:01 - loss: 0.2511 - acc: 0.2964Epoch 1/3\n",
      "  2/347 [..............................] - ETA: 2:28 - loss: 0.7005 - acc: 0.0625   Epoch 1/3\n",
      "347/347 [==============================] - 177s 509ms/step - loss: 0.1793 - acc: 0.2824\n",
      "Epoch 3/3\n",
      "347/347 [==============================] - 175s 504ms/step - loss: 0.1792 - acc: 0.2906\n",
      "Epoch 3/3\n",
      "347/347 [==============================] - 251s 623ms/step - loss: 0.3989 - acc: 0.2748\n",
      "Epoch 2/3\n",
      "347/347 [==============================] - 208s 599ms/step - loss: 0.2478 - acc: 0.2896\n",
      "347/347 [==============================] - 211s 608ms/step - loss: 0.2156 - acc: 0.2953\n",
      "347/347 [==============================] - 212s 611ms/step - loss: 0.1781 - acc: 0.2931\n",
      "347/347 [==============================] - 262s 662ms/step - loss: 0.2617 - acc: 0.2577\n",
      "Epoch 2/3\n",
      "347/347 [==============================] - 38s 97ms/step - loss: 0.2451 - acc: 0.2953\n",
      "347/347 [==============================] - 38s 98ms/step - loss: 0.2074 - acc: 0.2897\n",
      "347/347 [==============================] - 211s 608ms/step - loss: 0.1776 - acc: 0.2973\n",
      "129/347 [==========>...................] - ETA: 18s - loss: 0.1763 - acc: 0.3021Epoch 1/3\n",
      " 71/347 [=====>........................] - ETA: 24s - loss: 0.1759 - acc: 0.3015Epoch 1/3\n",
      "347/347 [==============================] - 266s 657ms/step - loss: 0.1894 - acc: 0.2626\n",
      "103/347 [=======>......................] - ETA: 23s - loss: 0.1761 - acc: 0.2976Epoch 2/3\n",
      "347/347 [==============================] - 266s 651ms/step - loss: 0.1888 - acc: 0.2626\n",
      "Epoch 2/3\n",
      "347/347 [==============================] - 35s 88ms/step - loss: 0.1770 - acc: 0.3010\n",
      "263/347 [=====================>........] - ETA: 7s - loss: 0.1765 - acc: 0.2976Epoch 1/3\n",
      "347/347 [==============================] - 33s 88ms/step - loss: 0.1761 - acc: 0.3003\n",
      "141/347 [===========>..................] - ETA: 1:43 - loss: 0.2250 - acc: 0.2972Epoch 1/3\n",
      "347/347 [==============================] - 196s 565ms/step - loss: 0.3608 - acc: 0.2897\n",
      "Epoch 3/3\n",
      "347/347 [==============================] - 208s 598ms/step - loss: 0.2207 - acc: 0.2938\n",
      "Epoch 3/3\n",
      "347/347 [==============================] - 217s 625ms/step - loss: 0.1792 - acc: 0.2902\n",
      "Epoch 3/3\n",
      "347/347 [==============================] - 218s 628ms/step - loss: 0.1790 - acc: 0.2906\n",
      "Epoch 3/3\n",
      "347/347 [==============================] - 238s 685ms/step - loss: 0.3607 - acc: 0.2897\n",
      "347/347 [==============================] - 49s 129ms/step - loss: 0.3563 - acc: 0.2953\n",
      "347/347 [==============================] - 234s 675ms/step - loss: 0.2197 - acc: 0.2921\n",
      "273/347 [======================>.......] - ETA: 1:37 - loss: 0.1895 - acc: 0.2590Epoch 1/3\n",
      "347/347 [==============================] - 223s 642ms/step - loss: 0.1770 - acc: 0.2990\n",
      "347/347 [==============================] - 227s 655ms/step - loss: 0.1780 - acc: 0.2933\n",
      "347/347 [==============================] - 465s 1s/step - loss: 0.2952 - acc: 0.2810\n",
      "Epoch 2/3\n",
      "347/347 [==============================] - 43s 111ms/step - loss: 0.2124 - acc: 0.2897\n",
      "347/347 [==============================] - 468s 1s/step - loss: 0.3710 - acc: 0.2805\n",
      "Epoch 2/3\n",
      "307/347 [=========================>....] - ETA: 51s - loss: 0.1897 - acc: 0.2634Epoch 1/3\n",
      "347/347 [==============================] - 473s 1s/step - loss: 0.1879 - acc: 0.2608\n",
      "Epoch 2/3\n",
      "347/347 [==============================] - 43s 114ms/step - loss: 0.1790 - acc: 0.3007\n",
      "347/347 [==============================] - 42s 114ms/step - loss: 0.1737 - acc: 0.3104\n",
      " 42/347 [==>...........................] - ETA: 5:32 - loss: 0.2026 - acc: 0.2865Epoch 1/3\n",
      "  1/347 [..............................] - ETA: 3:41:43 - loss: 1.2256 - acc: 0.0000e+00Epoch 1/3\n",
      "347/347 [==============================] - 475s 1s/step - loss: 0.1889 - acc: 0.2649\n",
      " 21/347 [>.............................] - ETA: 5:47 - loss: 0.1816 - acc: 0.2961Epoch 2/3\n",
      "347/347 [==============================] - 444s 1s/step - loss: 0.2092 - acc: 0.2897\n",
      "Epoch 3/3\n",
      "347/347 [==============================] - 446s 1s/step - loss: 0.2195 - acc: 0.2953\n",
      "Epoch 3/3\n",
      "347/347 [==============================] - 448s 1s/step - loss: 0.1790 - acc: 0.2866\n",
      "Epoch 3/3\n",
      "347/347 [==============================] - 552s 1s/step - loss: 0.3564 - acc: 0.2811\n",
      "Epoch 2/3\n",
      "347/347 [==============================] - 457s 1s/step - loss: 0.1792 - acc: 0.2889\n",
      "Epoch 3/3\n",
      "347/347 [==============================] - 577s 2s/step - loss: 0.3241 - acc: 0.2719\n",
      "Epoch 2/3\n",
      "347/347 [==============================] - 576s 2s/step - loss: 0.1891 - acc: 0.2577\n",
      "Epoch 2/3\n",
      "347/347 [==============================] - 587s 2s/step - loss: 0.1874 - acc: 0.2688\n",
      "Epoch 2/3\n",
      "347/347 [==============================] - 445s 1s/step - loss: 0.2119 - acc: 0.2897\n",
      "347/347 [==============================] - 445s 1s/step - loss: 0.2151 - acc: 0.2953\n",
      "347/347 [==============================] - 444s 1s/step - loss: 0.1782 - acc: 0.2885\n",
      "347/347 [==============================] - 433s 1s/step - loss: 0.1780 - acc: 0.2942\n",
      "347/347 [==============================] - 83s 219ms/step - loss: 0.2086 - acc: 0.2953\n",
      "347/347 [==============================] - 491s 1s/step - loss: 0.3144 - acc: 0.2898\n",
      "Epoch 3/3\n",
      "347/347 [==============================] - 84s 221ms/step - loss: 0.2205 - acc: 0.2897\n",
      "347/347 [==============================] - 82s 218ms/step - loss: 0.1763 - acc: 0.2960\n",
      "347/347 [==============================] - 76s 204ms/step - loss: 0.1758 - acc: 0.2953\n",
      "347/347 [==============================] - 459s 1s/step - loss: 0.2128 - acc: 0.2953\n",
      "Epoch 3/3\n",
      "347/347 [==============================] - 442s 1s/step - loss: 0.1792 - acc: 0.2843\n",
      "Epoch 3/3\n",
      "347/347 [==============================] - 439s 1s/step - loss: 0.1789 - acc: 0.2881\n",
      "Epoch 3/3\n",
      "347/347 [==============================] - 269s 775ms/step - loss: 0.3090 - acc: 0.2897\n",
      "347/347 [==============================] - 252s 725ms/step - loss: 0.2066 - acc: 0.2953\n",
      "347/347 [==============================] - 248s 713ms/step - loss: 0.1778 - acc: 0.2867\n",
      "347/347 [==============================] - 247s 711ms/step - loss: 0.1776 - acc: 0.3003\n",
      "347/347 [==============================] - 53s 146ms/step - loss: 0.3144 - acc: 0.2958\n",
      "347/347 [==============================] - 50s 138ms/step - loss: 0.2071 - acc: 0.2897\n",
      "347/347 [==============================] - 45s 124ms/step - loss: 0.1752 - acc: 0.3004\n",
      "347/347 [==============================] - 37s 103ms/step - loss: 0.1751 - acc: 0.2971\n",
      "Epoch 1/3\n",
      "693/693 [==============================] - 78s 111ms/step - loss: 0.1855 - acc: 0.2764\n",
      "Epoch 2/3\n",
      "693/693 [==============================] - 65s 94ms/step - loss: 0.1773 - acc: 0.3045\n",
      "Epoch 3/3\n",
      "693/693 [==============================] - 65s 94ms/step - loss: 0.1741 - acc: 0.3273\n",
      "Best Accuracy for dataset ./data/song-data-labels-cleaned.csv is: 0.3171810060739517 using {'adapt_embedding': True, 'filters': 50, 'kernel': 5, 'mood_count': 17, 'multiplicator': 2, 'multiplicator2': 4, 'outputfunction': 'softmax'}\n",
      " mean=0.2925, std=0.002752 using {'adapt_embedding': True, 'filters': 50, 'kernel': 5, 'mood_count': 17, 'multiplicator': 2, 'multiplicator2': 3, 'outputfunction': 'relu'}\n",
      " mean=0.2987, std=0.002391 using {'adapt_embedding': True, 'filters': 50, 'kernel': 5, 'mood_count': 17, 'multiplicator': 2, 'multiplicator2': 3, 'outputfunction': 'softmax'}\n",
      " mean=0.2925, std=0.002752 using {'adapt_embedding': True, 'filters': 50, 'kernel': 5, 'mood_count': 17, 'multiplicator': 2, 'multiplicator2': 4, 'outputfunction': 'relu'}\n",
      " mean=0.3172, std=0.00776 using {'adapt_embedding': True, 'filters': 50, 'kernel': 5, 'mood_count': 17, 'multiplicator': 2, 'multiplicator2': 4, 'outputfunction': 'softmax'}\n",
      " mean=0.1543, std=0.1354 using {'adapt_embedding': True, 'filters': 100, 'kernel': 5, 'mood_count': 17, 'multiplicator': 2, 'multiplicator2': 3, 'outputfunction': 'relu'}\n",
      " mean=0.2919, std=0.002166 using {'adapt_embedding': True, 'filters': 100, 'kernel': 5, 'mood_count': 17, 'multiplicator': 2, 'multiplicator2': 3, 'outputfunction': 'softmax'}\n",
      " mean=0.2925, std=0.002752 using {'adapt_embedding': True, 'filters': 100, 'kernel': 5, 'mood_count': 17, 'multiplicator': 2, 'multiplicator2': 4, 'outputfunction': 'relu'}\n",
      " mean=0.3042, std=0.008979 using {'adapt_embedding': True, 'filters': 100, 'kernel': 5, 'mood_count': 17, 'multiplicator': 2, 'multiplicator2': 4, 'outputfunction': 'softmax'}\n",
      " mean=0.2925, std=0.002752 using {'adapt_embedding': False, 'filters': 50, 'kernel': 5, 'mood_count': 17, 'multiplicator': 2, 'multiplicator2': 3, 'outputfunction': 'relu'}\n",
      " mean=0.3007, std=0.000361 using {'adapt_embedding': False, 'filters': 50, 'kernel': 5, 'mood_count': 17, 'multiplicator': 2, 'multiplicator2': 3, 'outputfunction': 'softmax'}\n",
      " mean=0.2925, std=0.002752 using {'adapt_embedding': False, 'filters': 50, 'kernel': 5, 'mood_count': 17, 'multiplicator': 2, 'multiplicator2': 4, 'outputfunction': 'relu'}\n",
      " mean=0.3055, std=0.004873 using {'adapt_embedding': False, 'filters': 50, 'kernel': 5, 'mood_count': 17, 'multiplicator': 2, 'multiplicator2': 4, 'outputfunction': 'softmax'}\n",
      " mean=0.2925, std=0.002752 using {'adapt_embedding': False, 'filters': 100, 'kernel': 5, 'mood_count': 17, 'multiplicator': 2, 'multiplicator2': 3, 'outputfunction': 'relu'}\n",
      " mean=0.2956, std=0.0003609 using {'adapt_embedding': False, 'filters': 100, 'kernel': 5, 'mood_count': 17, 'multiplicator': 2, 'multiplicator2': 3, 'outputfunction': 'softmax'}\n",
      " mean=0.2928, std=0.003023 using {'adapt_embedding': False, 'filters': 100, 'kernel': 5, 'mood_count': 17, 'multiplicator': 2, 'multiplicator2': 4, 'outputfunction': 'relu'}\n",
      " mean=0.2988, std=0.001624 using {'adapt_embedding': False, 'filters': 100, 'kernel': 5, 'mood_count': 17, 'multiplicator': 2, 'multiplicator2': 4, 'outputfunction': 'softmax'}\n",
      "####################################################################################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing now dataset ./data/song-data-labels-cleaned-quadrant.csv with 4 moods\n",
      "['happy' 'sad' 'calm' 'anger']\n",
      "processsed: 500 rows out of 12670\n",
      "processsed: 1000 rows out of 12670\n",
      "processsed: 1500 rows out of 12670\n",
      "processsed: 2000 rows out of 12670\n",
      "processsed: 2500 rows out of 12670\n",
      "processsed: 3000 rows out of 12670\n",
      "processsed: 3500 rows out of 12670\n",
      "processsed: 4000 rows out of 12670\n",
      "processsed: 4500 rows out of 12670\n",
      "processsed: 5000 rows out of 12670\n",
      "processsed: 5500 rows out of 12670\n",
      "processsed: 6000 rows out of 12670\n",
      "processsed: 6500 rows out of 12670\n",
      "processsed: 7000 rows out of 12670\n",
      "processsed: 7500 rows out of 12670\n",
      "processsed: 8000 rows out of 12670\n",
      "processsed: 8500 rows out of 12670\n",
      "processsed: 9000 rows out of 12670\n",
      "processsed: 9500 rows out of 12670\n",
      "processsed: 10000 rows out of 12670\n",
      "processsed: 10500 rows out of 12670\n",
      "processsed: 11000 rows out of 12670\n",
      "processsed: 11500 rows out of 12670\n",
      "processsed: 12000 rows out of 12670\n",
      "processsed: 12500 rows out of 12670\n",
      "[['oh', 'lord', 'like', 'know', 'think', 'oh', 'oh', 'lord', 'wanna', 'love', 'life', 'heart', 'wonder', 'right', 'try', 'forget', 'near', 'darle', 'miss', 'darling', 'oh', 'bring', 'darle', 'sweet', 'kiss', 'darle', 'think', 'happy', 'think', 'mistake', 'think', 'darling', 'right', 'wrong', 'near', 'darle', 'miss', 'darling', 'oh', 'bring', 'darle', 'sweet', 'kiss', 'darle', 'oh', 'jah', 'oh', 'jah', 'like', 'near', 'oh', 'jah', 'oh', 'jah', 'miss', 'love'], ['bring', 'beat', 'honey', 'honey', 'star', 'way', 'glow', 'window', 'pane', 'feel', 'sun', 'near', 'everytime', 'touch', 'melt', 'away', 'everybody', 'ask', 'smile', 'ear', 'ear', 'know', 'perfect', 'worth', 'fight', 'tear', 'finally', 'baby', 'love', 'need', 'come', 'baby', 'give', 'need', 'stop', 'finally', 'love', 'huum', 'come', 'baby', 'love', 'love', 'huum', 'come', 'baby', 'love', 'love', 'baby', 'baby', 'hear', 'wind', 'whip', 'pass', 'face', 'dance', 'night', 'away', 'boy', 'lip', 'taste', 'like', 'night', 'champagne', 'kiss', 'everybody', 'ask', 'smile', 'ear', 'ear', 'love', 'hurt', 'know', 'go', 'to', 'real', 'work', 'perfect', 'worth', 'fight', 'tear', 'finally', '1st', 'baby', 'love', 'need', 'come', 'baby', 'give', 'need', 'stop', 'finally', 'love', 'huum', 'baby', 'love', 'love', 'huum', 'come', 'baby', 'love', 'love', 'baby', 'love', 'need', 'come', 'baby', 'give', 'need', 'stop', 'finally', 'love', 'baby', 'love', 'need', 'come', 'baby', 'give', 'need', 'stop', 'finally', 'love', 'baby', 'love', 'need', 'come', 'baby', 'give', 'need', 'stop', 'finally', 'love', 'baby', 'love', 'need', 'baby', 'baby', 'baby', 'give', 'cridto', 'leandro', 'cesar12']]\n",
      "[[  372 18244  1114   390   254   781    94    17    22  3765  1544   257\n",
      "   1036    61    48   461   191    10     3   142    61     3  1702   317\n",
      "      3  1702   317   473     9     3  1702   317   451     7     7     7\n",
      "      7     7     7     7     7     7     7     7   451     7     7     7\n",
      "      7     7     7     7     7     7     7     7    61     3  1702   317\n",
      "      3  1702   317   473     9     3  1702   317   451     7     7     7\n",
      "      7     7     7     7     7     7     7     7   451     7     7     7\n",
      "      7     7     7     7     7     7     7     7   483    27    32   213\n",
      "     36     2  3256   808    31   104    67  1185   895  2713   242   177\n",
      "      9     3   490     6   507   299   507   299   507   299   507   299\n",
      "     44   451     7     7     7     7     7     7     7     7     7     7\n",
      "      7   451     7     7     7     7     7     7     7     7     7     7\n",
      "      7   451     7     7     7    61     3  1702   317     3  1702   317\n",
      "    473     9     3  1702   317   451     7     7     7    61     3  1702\n",
      "    317     3  1702   317   473     9     3  1702   317   451     7     7]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     4    44    38    11    77     2\n",
      "   1397   106    26     2    76     8    13   256    11     2     3   175\n",
      "     11    15   457     8    42   123   164    32    42  1368    36     4\n",
      "     44    38    11    77     2  1397   106    26     2    76     8    13\n",
      "    256    11     2     3   175    11    13     4     9    29     2    13\n",
      "     11    29  2675     2   105    45  2959     2    11     2    59   888\n",
      "     13    11    71     4    11    48   124    82   379   256   196    21\n",
      "    100    11     2    50    90   262  3082    29     2    10    13    29\n",
      "     29    39    11    99     2     4    44    38    11    77     2  1397\n",
      "    106    26     2    76     8    13   256    11     2     3   175    11]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1834/658696831.py:44: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  model_CV = KerasClassifier(build_fn=create_model, epochs=epochs,\n",
      "2023-02-18 11:14:57.193402: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-18 11:14:57.252953: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-18 11:14:57.315648: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-18 11:14:57.347853: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-18 11:14:57.412829: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-02-18 11:14:57.413253: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-02-18 11:14:57.424557: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-18 11:14:57.468776: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-02-18 11:14:57.468906: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-02-18 11:14:57.469550: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-18 11:14:57.475317: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-18 11:14:57.534906: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-18 11:14:57.540245: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-02-18 11:14:57.540339: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-02-18 11:14:57.557401: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-02-18 11:14:57.557563: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-02-18 11:14:57.637990: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-02-18 11:14:57.640448: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-02-18 11:14:57.690377: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-02-18 11:14:57.690412: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-02-18 11:14:57.692604: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-02-18 11:14:57.692680: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-02-18 11:14:57.763007: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-02-18 11:14:57.763163: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-02-18 11:14:58.668635: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-18 11:14:58.669192: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-18 11:14:58.669275: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-02-18 11:14:58.708701: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-18 11:14:58.709102: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-18 11:14:58.709200: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-02-18 11:14:58.764929: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-18 11:14:58.765152: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-18 11:14:58.765223: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-02-18 11:14:58.827489: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-18 11:14:58.828083: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-18 11:14:58.828213: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-18 11:14:58.896382: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-18 11:14:58.896993: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-18 11:14:58.897106: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-02-18 11:14:58.950714: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-18 11:14:58.950822: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-18 11:14:58.950836: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-02-18 11:14:58.958402: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-18 11:14:58.958679: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-18 11:14:58.958743: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-02-18 11:14:58.984561: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-18 11:14:58.984705: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-18 11:14:58.984719: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-02-18 11:15:00.258206: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-02-18 11:15:00.258393: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-02-18 11:15:00.258422: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (LAPTOP-D6IC8GJA): /proc/driver/nvidia/version does not exist\n",
      "2023-02-18 11:15:00.258829: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-18 11:15:00.352574: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-02-18 11:15:00.352751: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-02-18 11:15:00.352839: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (LAPTOP-D6IC8GJA): /proc/driver/nvidia/version does not exist\n",
      "2023-02-18 11:15:00.353758: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-18 11:15:00.392739: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-02-18 11:15:00.392894: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-02-18 11:15:00.393462: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (LAPTOP-D6IC8GJA): /proc/driver/nvidia/version does not exist\n",
      "2023-02-18 11:15:00.393976: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-18 11:15:00.547266: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-02-18 11:15:00.547465: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-02-18 11:15:00.547495: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (LAPTOP-D6IC8GJA): /proc/driver/nvidia/version does not exist\n",
      "2023-02-18 11:15:00.547902: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-18 11:15:00.566773: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-02-18 11:15:00.566900: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-02-18 11:15:00.567028: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (LAPTOP-D6IC8GJA): /proc/driver/nvidia/version does not exist\n",
      "2023-02-18 11:15:00.567455: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-18 11:15:00.601718: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-02-18 11:15:00.601832: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-02-18 11:15:00.601931: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (LAPTOP-D6IC8GJA): /proc/driver/nvidia/version does not exist\n",
      "2023-02-18 11:15:00.602292: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-18 11:15:00.796043: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-02-18 11:15:00.796301: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-02-18 11:15:00.796333: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (LAPTOP-D6IC8GJA): /proc/driver/nvidia/version does not exist\n",
      "2023-02-18 11:15:00.797031: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-18 11:15:00.811190: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-02-18 11:15:00.811245: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-02-18 11:15:00.811266: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (LAPTOP-D6IC8GJA): /proc/driver/nvidia/version does not exist\n",
      "2023-02-18 11:15:00.811642: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "Epoch 1/3\n",
      "Epoch 1/3\n",
      "Epoch 1/3\n",
      "Epoch 1/3\n",
      "Epoch 1/3\n",
      "Epoch 1/3\n",
      "Epoch 1/3\n",
      "159/159 [==============================] - 118s 723ms/step - loss: 2.9467 - acc: 0.2725\n",
      "156/159 [============================>.] - ETA: 2s - loss: 0.5617 - acc: 0.2943Epoch 2/3\n",
      "159/159 [==============================] - 121s 735ms/step - loss: 0.5615 - acc: 0.2946\n",
      "158/159 [============================>.] - ETA: 0s - loss: 0.8608 - acc: 0.2708Epoch 2/3\n",
      "159/159 [==============================] - 121s 739ms/step - loss: 0.8600 - acc: 0.2711\n",
      "Epoch 2/3\n",
      "159/159 [==============================] - 123s 756ms/step - loss: 0.5632 - acc: 0.2855\n",
      "Epoch 2/3\n",
      "159/159 [==============================] - 130s 785ms/step - loss: 1.5256 - acc: 0.2737\n",
      "Epoch 2/3\n",
      "159/159 [==============================] - 132s 806ms/step - loss: 0.5643 - acc: 0.2786\n",
      "Epoch 2/3\n",
      "159/159 [==============================] - 135s 812ms/step - loss: 1.1543 - acc: 0.2863\n",
      "Epoch 2/3\n",
      "159/159 [==============================] - 136s 827ms/step - loss: 0.5625 - acc: 0.2802\n",
      "  1/159 [..............................] - ETA: 3:02 - loss: 0.9759 - acc: 0.2812Epoch 2/3\n",
      "159/159 [==============================] - 113s 709ms/step - loss: 0.7075 - acc: 0.2790\n",
      "Epoch 3/3\n",
      "159/159 [==============================] - 111s 696ms/step - loss: 0.5591 - acc: 0.2950\n",
      "Epoch 3/3\n",
      "159/159 [==============================] - 109s 687ms/step - loss: 0.5583 - acc: 0.2944\n",
      "Epoch 3/3\n",
      "159/159 [==============================] - 113s 711ms/step - loss: 0.5535 - acc: 0.3197\n",
      "Epoch 3/3\n",
      "159/159 [==============================] - 117s 734ms/step - loss: 0.5602 - acc: 0.2770\n",
      "Epoch 3/3\n",
      "159/159 [==============================] - 115s 721ms/step - loss: 0.5577 - acc: 0.2960\n",
      "Epoch 3/3\n",
      "159/159 [==============================] - 120s 753ms/step - loss: 0.5548 - acc: 0.3110\n",
      "Epoch 3/3\n",
      "159/159 [==============================] - 121s 759ms/step - loss: 0.5673 - acc: 0.2964\n",
      "Epoch 3/3\n",
      "159/159 [==============================] - 103s 647ms/step - loss: 0.5589 - acc: 0.2914\n",
      "159/159 [==============================] - 102s 639ms/step - loss: 0.5536 - acc: 0.3147\n",
      "159/159 [==============================] - 103s 649ms/step - loss: 0.5573 - acc: 0.3127\n",
      "159/159 [==============================] - 103s 645ms/step - loss: 0.5414 - acc: 0.3566\n",
      "159/159 [==============================] - 101s 635ms/step - loss: 0.5574 - acc: 0.3031\n",
      "159/159 [==============================] - 100s 627ms/step - loss: 0.5544 - acc: 0.3116\n",
      "159/159 [==============================] - 16s 79ms/step - loss: 0.5420 - acc: 0.3510\n",
      "159/159 [==============================] - 99s 622ms/step - loss: 0.5483 - acc: 0.3317\n",
      "159/159 [==============================] - 96s 601ms/step - loss: 0.5574 - acc: 0.3120\n",
      "159/159 [==============================] - 3s 17ms/step - loss: 0.5736 - acc: 0.3191\n",
      "Epoch 1/3\n",
      " 15/159 [=>............................] - ETA: 23s - loss: 3.7214 - acc: 0.1646Epoch 1/3\n",
      "159/159 [==============================] - 14s 82ms/step - loss: 0.5629 - acc: 0.2727\n",
      "159/159 [==============================] - 14s 82ms/step - loss: 0.5699 - acc: 0.2930\n",
      "159/159 [==============================] - 17s 95ms/step - loss: 0.5437 - acc: 0.3526\n",
      "159/159 [==============================] - 18s 99ms/step - loss: 0.5405 - acc: 0.3648\n",
      "159/159 [==============================] - 18s 99ms/step - loss: 0.5593 - acc: 0.2942\n",
      "159/159 [==============================] - 19s 106ms/step - loss: 0.5486 - acc: 0.3496\n",
      " 93/159 [================>.............] - ETA: 29s - loss: 4.4998 - acc: 0.2466Epoch 1/3\n",
      "149/159 [===========================>..] - ETA: 3s - loss: 3.8419 - acc: 0.1619Epoch 1/3\n",
      "102/159 [==================>...........] - ETA: 25s - loss: 4.3630 - acc: 0.2491Epoch 1/3\n",
      "103/159 [==================>...........] - ETA: 25s - loss: 4.3522 - acc: 0.2485Epoch 1/3\n",
      "104/159 [==================>...........] - ETA: 24s - loss: 4.3385 - acc: 0.2488Epoch 1/3\n",
      "106/159 [===================>..........] - ETA: 23s - loss: 4.3107 - acc: 0.2497Epoch 1/3\n",
      "159/159 [==============================] - 59s 360ms/step - loss: 3.8428 - acc: 0.1616\n",
      "Epoch 2/3\n",
      "159/159 [==============================] - 102s 611ms/step - loss: 3.5327 - acc: 0.2575\n",
      "Epoch 2/3\n",
      "159/159 [==============================] - 179s 1s/step - loss: 3.8562 - acc: 0.1596\n",
      "Epoch 3/3\n",
      "159/159 [==============================] - 200s 1s/step - loss: 0.5622 - acc: 0.2859\n",
      "Epoch 2/3\n",
      "159/159 [==============================] - 204s 1s/step - loss: 0.5625 - acc: 0.2768\n",
      "Epoch 2/3\n",
      "159/159 [==============================] - 236s 1s/step - loss: 3.8400 - acc: 0.1610\n",
      "Epoch 2/3\n",
      "159/159 [==============================] - 188s 1s/step - loss: 0.9168 - acc: 0.2906\n",
      "Epoch 3/3\n",
      "159/159 [==============================] - 242s 1s/step - loss: 0.5648 - acc: 0.3023\n",
      "Epoch 2/3\n",
      "159/159 [==============================] - 241s 1s/step - loss: 0.5623 - acc: 0.2855\n",
      "Epoch 2/3\n",
      "159/159 [==============================] - 243s 1s/step - loss: 5.5830 - acc: 0.2713\n",
      "Epoch 2/3\n",
      "159/159 [==============================] - 195s 1s/step - loss: 3.8562 - acc: 0.1596\n",
      "159/159 [==============================] - 192s 1s/step - loss: 0.5540 - acc: 0.3197\n",
      "Epoch 3/3\n",
      "159/159 [==============================] - 192s 1s/step - loss: 0.5578 - acc: 0.2788\n",
      "Epoch 3/3\n",
      "159/159 [==============================] - 195s 1s/step - loss: 0.5599 - acc: 0.2863\n",
      "159/159 [==============================] - 48s 273ms/step - loss: 3.8562 - acc: 0.1630\n",
      "159/159 [==============================] - 228s 1s/step - loss: 3.8562 - acc: 0.1596\n",
      "Epoch 3/3\n",
      "159/159 [==============================] - 225s 1s/step - loss: 0.5534 - acc: 0.3208\n",
      "Epoch 3/3\n",
      "159/159 [==============================] - 225s 1s/step - loss: 0.5576 - acc: 0.2828\n",
      "Epoch 3/3\n",
      "159/159 [==============================] - 228s 1s/step - loss: 5.5819 - acc: 0.2721\n",
      "Epoch 3/3\n",
      " 70/159 [============>.................] - ETA: 24s - loss: 0.5565 - acc: 0.2781Epoch 1/3\n",
      "159/159 [==============================] - 47s 268ms/step - loss: 0.5565 - acc: 0.2764\n",
      " 53/159 [=========>....................] - ETA: 56s - loss: 1.3352 - acc: 0.2842Epoch 1/3\n",
      "159/159 [==============================] - 182s 1s/step - loss: 0.5415 - acc: 0.3652\n",
      "159/159 [==============================] - 184s 1s/step - loss: 0.5547 - acc: 0.2979\n",
      "159/159 [==============================] - 118s 497ms/step - loss: 0.8263 - acc: 0.2901\n",
      "Epoch 2/3\n",
      "159/159 [==============================] - 43s 246ms/step - loss: 0.5462 - acc: 0.3502\n",
      "119/159 [=====================>........] - ETA: 51s - loss: 0.5419 - acc: 0.3603Epoch 1/3\n",
      "159/159 [==============================] - 203s 1s/step - loss: 3.8562 - acc: 0.1596\n",
      "159/159 [==============================] - 119s 510ms/step - loss: 0.7319 - acc: 0.2749\n",
      "Epoch 2/3\n",
      "159/159 [==============================] - 203s 1s/step - loss: 0.5546 - acc: 0.3031\n",
      "159/159 [==============================] - 204s 1s/step - loss: 0.5405 - acc: 0.3627\n",
      "159/159 [==============================] - 39s 222ms/step - loss: 0.5540 - acc: 0.2853\n",
      "159/159 [==============================] - 203s 1s/step - loss: 5.5819 - acc: 0.2721\n",
      "159/159 [==============================] - 77s 486ms/step - loss: 0.5576 - acc: 0.3137\n",
      "Epoch 3/3\n",
      " 44/159 [=======>......................] - ETA: 50s - loss: 0.5729 - acc: 0.2741Epoch 1/3\n",
      "159/159 [==============================] - 91s 344ms/step - loss: 0.5623 - acc: 0.2893\n",
      "Epoch 2/3\n",
      "159/159 [==============================] - 57s 358ms/step - loss: 0.5639 - acc: 0.2843\n",
      "Epoch 3/3\n",
      "159/159 [==============================] - 59s 372ms/step - loss: 0.5556 - acc: 0.3147\n",
      "159/159 [==============================] - 48s 288ms/step - loss: 3.8562 - acc: 0.1630\n",
      "159/159 [==============================] - 49s 285ms/step - loss: 5.4685 - acc: 0.2869\n",
      "159/159 [==============================] - 48s 289ms/step - loss: 0.5603 - acc: 0.2766\n",
      "159/159 [==============================] - 48s 285ms/step - loss: 0.5457 - acc: 0.3496\n",
      "159/159 [==============================] - 80s 412ms/step - loss: 0.5626 - acc: 0.2841\n",
      "Epoch 2/3\n",
      "159/159 [==============================] - 57s 360ms/step - loss: 0.5524 - acc: 0.3279\n",
      "Epoch 3/3\n",
      "159/159 [==============================] - 52s 322ms/step - loss: 0.5579 - acc: 0.3072\n",
      "159/159 [==============================] - 8s 42ms/step - loss: 0.5682 - acc: 0.3197\n",
      "121/159 [=====================>........] - ETA: 8s - loss: 0.5553 - acc: 0.3179Epoch 1/3\n",
      "114/159 [====================>.........] - ETA: 2s - loss: 0.5728 - acc: 0.3188Epoch 1/3\n",
      "141/159 [=========================>....] - ETA: 0s - loss: 0.5730 - acc: 0.3223Epoch 1/3\n",
      "150/159 [===========================>..] - ETA: 0s - loss: 0.5728 - acc: 0.3225Epoch 1/3\n",
      "159/159 [==============================] - 9s 47ms/step - loss: 0.5728 - acc: 0.3214\n",
      "150/159 [===========================>..] - ETA: 2s - loss: 0.5552 - acc: 0.3167Epoch 1/3\n",
      "159/159 [==============================] - 36s 225ms/step - loss: 0.5546 - acc: 0.3187\n",
      "Epoch 3/3\n",
      "115/159 [====================>.........] - ETA: 9s - loss: 0.5421 - acc: 0.3660Epoch 1/3\n",
      "159/159 [==============================] - 38s 239ms/step - loss: 0.5408 - acc: 0.3688\n",
      "159/159 [==============================] - 21s 104ms/step - loss: 0.5425 - acc: 0.3571\n",
      "  8/159 [>.............................] - ETA: 3:03 - loss: 5.6710 - acc: 0.2656Epoch 1/3\n",
      "159/159 [==============================] - 75s 471ms/step - loss: 0.5533 - acc: 0.3110\n",
      "159/159 [==============================] - 99s 554ms/step - loss: 3.9016 - acc: 0.1719\n",
      "Epoch 2/3\n",
      "159/159 [==============================] - 111s 599ms/step - loss: 0.5617 - acc: 0.2936\n",
      "Epoch 2/3\n",
      "159/159 [==============================] - 111s 611ms/step - loss: 0.5622 - acc: 0.2873\n",
      "Epoch 2/3\n",
      "159/159 [==============================] - 113s 607ms/step - loss: 0.7221 - acc: 0.2794\n",
      "Epoch 2/3\n",
      "159/159 [==============================] - 20s 96ms/step - loss: 0.5483 - acc: 0.3569\n",
      " 68/159 [===========>..................] - ETA: 55s - loss: 3.8528 - acc: 0.1760Epoch 1/3\n",
      "159/159 [==============================] - 97s 613ms/step - loss: 3.8553 - acc: 0.1606\n",
      "Epoch 3/3\n",
      "159/159 [==============================] - 97s 611ms/step - loss: 0.5574 - acc: 0.2991\n",
      "Epoch 3/3\n",
      "159/159 [==============================] - 98s 616ms/step - loss: 0.5525 - acc: 0.3256\n",
      "Epoch 3/3\n",
      "159/159 [==============================] - 208s 1s/step - loss: 5.5680 - acc: 0.2711\n",
      "  1/159 [..............................] - ETA: 1:51 - loss: 0.5840 - acc: 0.2188Epoch 2/3\n",
      "159/159 [==============================] - 100s 628ms/step - loss: 0.5595 - acc: 0.2865\n",
      "Epoch 3/3\n",
      "159/159 [==============================] - 223s 1s/step - loss: 5.4735 - acc: 0.2865\n",
      "Epoch 2/3\n",
      "159/159 [==============================] - 240s 1s/step - loss: 0.5613 - acc: 0.2820\n",
      "Epoch 2/3\n",
      "159/159 [==============================] - 106s 667ms/step - loss: 3.8565 - acc: 0.1596\n",
      "159/159 [==============================] - 106s 665ms/step - loss: 0.5560 - acc: 0.2877\n",
      "159/159 [==============================] - 105s 659ms/step - loss: 0.5567 - acc: 0.3007\n",
      "159/159 [==============================] - 107s 676ms/step - loss: 0.5357 - acc: 0.3735\n",
      "159/159 [==============================] - 14s 75ms/step - loss: 0.5431 - acc: 0.3494\n",
      " 46/159 [=======>......................] - ETA: 9s - loss: 3.8562 - acc: 0.1692Epoch 1/3\n",
      "159/159 [==============================] - 15s 84ms/step - loss: 3.8562 - acc: 0.1630\n",
      "102/159 [==================>...........] - ETA: 5s - loss: 0.5529 - acc: 0.3079Epoch 1/3\n",
      "159/159 [==============================] - 18s 97ms/step - loss: 0.5555 - acc: 0.2810\n",
      "159/159 [==============================] - 21s 96ms/step - loss: 0.5531 - acc: 0.3098\n",
      "159/159 [==============================] - 225s 1s/step - loss: 0.5643 - acc: 0.2786\n",
      "Epoch 2/3\n",
      "  4/159 [..............................] - ETA: 2:53 - loss: 3.3301 - acc: 0.2578Epoch 1/3\n",
      " 89/159 [===============>..............] - ETA: 1:04 - loss: 0.5567 - acc: 0.2855Epoch 1/3\n",
      "159/159 [==============================] - 171s 1s/step - loss: 5.5797 - acc: 0.2725\n",
      "Epoch 3/3\n",
      "159/159 [==============================] - 164s 1s/step - loss: 5.4700 - acc: 0.2867\n",
      "Epoch 3/3\n",
      "159/159 [==============================] - 165s 1s/step - loss: 0.5559 - acc: 0.2910\n",
      "Epoch 3/3\n",
      "159/159 [==============================] - 195s 1s/step - loss: 0.5569 - acc: 0.2983\n",
      "Epoch 3/3\n",
      "159/159 [==============================] - 251s 1s/step - loss: 5.5407 - acc: 0.2699\n",
      "Epoch 2/3\n",
      "159/159 [==============================] - 205s 1s/step - loss: 5.5790 - acc: 0.2725\n",
      "159/159 [==============================] - 209s 1s/step - loss: 5.0674 - acc: 0.2478\n",
      "159/159 [==============================] - 266s 1s/step - loss: 7.1508 - acc: 0.2871\n",
      "Epoch 2/3\n",
      "159/159 [==============================] - 265s 1s/step - loss: 0.5627 - acc: 0.2851\n",
      "Epoch 2/3\n",
      "159/159 [==============================] - 270s 1s/step - loss: 0.5622 - acc: 0.2908\n",
      "Epoch 2/3\n",
      "159/159 [==============================] - 192s 1s/step - loss: 0.5465 - acc: 0.3352\n",
      "159/159 [==============================] - 40s 230ms/step - loss: 5.4700 - acc: 0.2867\n",
      "159/159 [==============================] - 48s 261ms/step - loss: 0.5421 - acc: 0.3398\n",
      "159/159 [==============================] - 48s 256ms/step - loss: 3.8562 - acc: 0.1596\n",
      "159/159 [==============================] - 166s 1s/step - loss: 0.5569 - acc: 0.2916\n",
      "159/159 [==============================] - 35s 197ms/step - loss: 0.5534 - acc: 0.2810\n",
      "159/159 [==============================] - 176s 1s/step - loss: 5.5789 - acc: 0.2725\n",
      "Epoch 3/3\n",
      "159/159 [==============================] - 161s 1s/step - loss: 7.1957 - acc: 0.2857\n",
      "Epoch 3/3\n",
      "159/159 [==============================] - 158s 992ms/step - loss: 0.5553 - acc: 0.2887\n",
      "Epoch 3/3\n",
      "159/159 [==============================] - 153s 961ms/step - loss: 0.5539 - acc: 0.3062\n",
      "Epoch 3/3\n",
      "159/159 [==============================] - 114s 716ms/step - loss: 5.5789 - acc: 0.2725\n",
      "159/159 [==============================] - 111s 701ms/step - loss: 7.1957 - acc: 0.2871\n",
      "159/159 [==============================] - 108s 680ms/step - loss: 0.5400 - acc: 0.3441\n",
      "159/159 [==============================] - 106s 664ms/step - loss: 0.5448 - acc: 0.3386\n",
      "159/159 [==============================] - 7s 42ms/step - loss: 0.5584 - acc: 0.2928\n",
      "159/159 [==============================] - 9s 50ms/step - loss: 0.5353 - acc: 0.3518\n",
      "159/159 [==============================] - 5s 28ms/step - loss: 5.4700 - acc: 0.2867\n",
      "159/159 [==============================] - 5s 28ms/step - loss: 7.1911 - acc: 0.2725\n",
      "Epoch 1/3\n",
      "317/317 [==============================] - 34s 105ms/step - loss: 0.5587 - acc: 0.2955\n",
      "Epoch 2/3\n",
      "317/317 [==============================] - 33s 103ms/step - loss: 0.5501 - acc: 0.3272\n",
      "Epoch 3/3\n",
      "317/317 [==============================] - 31s 99ms/step - loss: 0.5269 - acc: 0.3850\n",
      "Best Accuracy for dataset ./data/song-data-labels-cleaned-quadrant.csv is: 0.35724152624607086 using {'adapt_embedding': True, 'filters': 50, 'kernel': 5, 'mood_count': 4, 'multiplicator': 2, 'multiplicator2': 4, 'outputfunction': 'softmax'}\n",
      " mean=0.2829, std=0.01016 using {'adapt_embedding': True, 'filters': 50, 'kernel': 5, 'mood_count': 4, 'multiplicator': 2, 'multiplicator2': 3, 'outputfunction': 'relu'}\n",
      " mean=0.3518, std=0.0007893 using {'adapt_embedding': True, 'filters': 50, 'kernel': 5, 'mood_count': 4, 'multiplicator': 2, 'multiplicator2': 3, 'outputfunction': 'softmax'}\n",
      " mean=0.3066, std=0.01243 using {'adapt_embedding': True, 'filters': 50, 'kernel': 5, 'mood_count': 4, 'multiplicator': 2, 'multiplicator2': 4, 'outputfunction': 'relu'}\n",
      " mean=0.3572, std=0.007597 using {'adapt_embedding': True, 'filters': 50, 'kernel': 5, 'mood_count': 4, 'multiplicator': 2, 'multiplicator2': 4, 'outputfunction': 'softmax'}\n",
      " mean=0.2197, std=0.05673 using {'adapt_embedding': True, 'filters': 100, 'kernel': 5, 'mood_count': 4, 'multiplicator': 2, 'multiplicator2': 3, 'outputfunction': 'relu'}\n",
      " mean=0.3178, std=0.03246 using {'adapt_embedding': True, 'filters': 100, 'kernel': 5, 'mood_count': 4, 'multiplicator': 2, 'multiplicator2': 3, 'outputfunction': 'softmax'}\n",
      " mean=0.2249, std=0.06196 using {'adapt_embedding': True, 'filters': 100, 'kernel': 5, 'mood_count': 4, 'multiplicator': 2, 'multiplicator2': 4, 'outputfunction': 'relu'}\n",
      " mean=0.3131, std=0.0365 using {'adapt_embedding': True, 'filters': 100, 'kernel': 5, 'mood_count': 4, 'multiplicator': 2, 'multiplicator2': 4, 'outputfunction': 'softmax'}\n",
      " mean=0.3205, std=0.0008879 using {'adapt_embedding': False, 'filters': 50, 'kernel': 5, 'mood_count': 4, 'multiplicator': 2, 'multiplicator2': 3, 'outputfunction': 'relu'}\n",
      " mean=0.357, std=9.866e-05 using {'adapt_embedding': False, 'filters': 50, 'kernel': 5, 'mood_count': 4, 'multiplicator': 2, 'multiplicator2': 3, 'outputfunction': 'softmax'}\n",
      " mean=0.2364, std=0.0734 using {'adapt_embedding': False, 'filters': 50, 'kernel': 5, 'mood_count': 4, 'multiplicator': 2, 'multiplicator2': 4, 'outputfunction': 'relu'}\n",
      " mean=0.3152, std=0.03423 using {'adapt_embedding': False, 'filters': 50, 'kernel': 5, 'mood_count': 4, 'multiplicator': 2, 'multiplicator2': 4, 'outputfunction': 'softmax'}\n",
      " mean=0.2232, std=0.06354 using {'adapt_embedding': False, 'filters': 100, 'kernel': 5, 'mood_count': 4, 'multiplicator': 2, 'multiplicator2': 3, 'outputfunction': 'relu'}\n",
      " mean=0.3104, std=0.0294 using {'adapt_embedding': False, 'filters': 100, 'kernel': 5, 'mood_count': 4, 'multiplicator': 2, 'multiplicator2': 3, 'outputfunction': 'softmax'}\n",
      " mean=0.2796, std=0.007103 using {'adapt_embedding': False, 'filters': 100, 'kernel': 5, 'mood_count': 4, 'multiplicator': 2, 'multiplicator2': 4, 'outputfunction': 'relu'}\n",
      " mean=0.3223, std=0.0295 using {'adapt_embedding': False, 'filters': 100, 'kernel': 5, 'mood_count': 4, 'multiplicator': 2, 'multiplicator2': 4, 'outputfunction': 'softmax'}\n",
      "####################################################################################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing now dataset ./data/song-data-labels-cleaned-seven-moods.csv with 7 moods\n",
      "['romantic' 'happy' 'grief' 'enthusiastic' 'calm' 'depressed' 'anger']\n",
      "processsed: 500 rows out of 19576\n",
      "processsed: 1000 rows out of 19576\n",
      "processsed: 1500 rows out of 19576\n",
      "processsed: 2000 rows out of 19576\n",
      "processsed: 2500 rows out of 19576\n",
      "processsed: 3000 rows out of 19576\n",
      "processsed: 3500 rows out of 19576\n",
      "processsed: 4000 rows out of 19576\n",
      "processsed: 4500 rows out of 19576\n",
      "processsed: 5000 rows out of 19576\n",
      "processsed: 5500 rows out of 19576\n",
      "processsed: 6000 rows out of 19576\n",
      "processsed: 6500 rows out of 19576\n",
      "processsed: 7000 rows out of 19576\n",
      "processsed: 7500 rows out of 19576\n",
      "processsed: 8000 rows out of 19576\n",
      "processsed: 8500 rows out of 19576\n",
      "processsed: 9000 rows out of 19576\n",
      "processsed: 9500 rows out of 19576\n",
      "processsed: 10000 rows out of 19576\n",
      "processsed: 10500 rows out of 19576\n",
      "processsed: 11000 rows out of 19576\n",
      "processsed: 11500 rows out of 19576\n",
      "processsed: 12000 rows out of 19576\n",
      "processsed: 12500 rows out of 19576\n",
      "processsed: 13000 rows out of 19576\n",
      "processsed: 13500 rows out of 19576\n",
      "processsed: 14000 rows out of 19576\n",
      "processsed: 14500 rows out of 19576\n",
      "processsed: 15000 rows out of 19576\n",
      "processsed: 15500 rows out of 19576\n",
      "processsed: 16000 rows out of 19576\n",
      "processsed: 16500 rows out of 19576\n",
      "processsed: 17000 rows out of 19576\n",
      "processsed: 17500 rows out of 19576\n",
      "processsed: 18000 rows out of 19576\n",
      "processsed: 18500 rows out of 19576\n",
      "processsed: 19000 rows out of 19576\n",
      "processsed: 19500 rows out of 19576\n",
      "[['lie', 'floor', 'feather', 'find', 'hand', 'wanna', 'believe', 'not', 'think', 's', 'god', 'heaven', 'hell', 'lose', 'lookin', 'sign', 'maybe', 'fool', 'swear', 'spine', 'get', 'shiver', 'like', 'kiss', 'time', 'tho', 'tear', 'stream', 'like', 'river', 'know', 'ask', 'sign', 'scare', 'tho', 'body', 'start', 'shakin', 'feel', 'dry', 'eye', 'tho', 'heart', 'break', 'know', 'love', 'sign', 'sign', 'findin', 'street', 'starin', 'reach', 'turn', 'lookin', 'stranger', 'eye', 's', 'god', 'heaven', 'hell', 'tt', 'feel', 'like', 'walk', 'cos', 'touch', 'like', 'swear', 'spine', 'get', 'shiver', 'like', 'kiss', 'time', 'tho', 'tear', 'stream', 'like', 'river', 'know', 'ask', 'sign', 'scare', 'tho', 'body', 'start', 'shakin', 'feel', 'dry', 'eye', 'tho', 'heart', 'break', 'know', 'love', 'sign', 'sign', 'lie', 'floor', 'feather', 'find', 'hand', 'wanna', 'believe', 'not', 'think', 'swear', 'spine', 'get', 'shiver', 'like', 'kiss', 'time', 'tho', 'tear', 'stream', 'like', 'river', 'know', 'ask', 'sign', 'scare', 'tho', 'body', 'start', 'shakin', 'feel', 'dry', 'eye', 'tho', 'heart', 'break', 'know', 'love', 'sign', 'sign'], ['left', 'left', 'left', 'left', 'box', 'left', 'closet', 'yes', 'stuff', 'yes', 'buy', 'touch', 'talk', 'mess', 's', 'fine', 'walk', 'talk', 'time', 'and-', 's', 'tag', 'bag', 'let', 'cab', 'standing', 'yard', 'tell', 'fool', 'talk', \"'bout\", 'find', 'man', 'like', 'get', 'twisted', 'know', 'know', 'minute', 'matter', 'fact', 'minute', 'know', 'know', 'tomorrow', 'second', 'thinkin', 'irreplaceable', 'ahead', 'grow', 'chick', 's', 'home', 'oops', 'bet', 'think', 'know', 'think', 'put', 'untrue', 'roll', 'car', 'buy', 'baby', 'drop', 'key', 'hurry', 'taxi', 'leave', 'stand', 'yard', 'tell', 'fool', 'talk', \"'bout\", 'find', 'man', 'like', 'get', 'twisted', 'know', 'know', 'minute', 'matter', 'fact', 'minute', 'know', 'know', 'tomorrow', 'second', 'thinkin', 'irreplaceable', 'baby', 'will', 'shed', 'tear', 'will', 'lose', 'wink', 'sleep', 'cause', 'truth', 'matter', 'replace', 'easy', 'left', 'left', 'left', 'left', 'mmmmmmmm', 'left', 'left', 'box', 'left', 'left', 'left', 'second', 'think', 'irreplaceable', 'know', 'know', 'minute', 'matter', 'fact', 'minute', 'know', 'know', 'tomorrow', 'second', 'thinkin', 'know', 'know', 'minute', 'matter', 'fact', 'minute', 'pack', 'things-', 'finish', 'cause', 'bed', 'lie', 'tomorrow', 'second', 'thinkin', 'irreplaceable']]\n",
      "[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0   16    3   16   39   60\n",
      "    19  111  133    1  438   54   21    8  152   61 1204    8  144   93\n",
      "    62  152  116    2   76   20   24  358    9  407   21   24  300  358\n",
      "    24   84    2    1   11   24    1  213   14    9  407   21   24   84\n",
      "    54 2264   34   54 2264   34   10   22  280  309    4   24    5 1101\n",
      "    10  811   21  139    8  152 1303   61  309   59   15    7  152  172\n",
      "    76   20   24  358    9  407   21   24  300  358   24   84    2    1\n",
      "    11   24    1  213   14    9  407   21   24   84   54 2264   34   54\n",
      "  2264   34   74   16  108   39   32   17   51    5   13  309  407   21\n",
      "   109  675  400  123   15   15   24   12    9  407   21   24  300   12\n",
      "    24   84    2    1   11   24    1  213   14    9  407   21   24   84\n",
      "    54 2264   34   54 2264   34   54 2264   34   54 2264   34]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0   16   27    3   68    8   22 1370   49  130\n",
      "     3  478   24   15  130  130  593    5   10   80    4   26   97  144\n",
      "   539   19   10   22  130    3  478   24   15   33  130  130  593    5\n",
      "    10  180  519  164  180    1  519  164  130    3  478   24   15   33\n",
      "   130  130  593    5   10  180  519  164  180    1  519  164]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1834/658696831.py:44: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  model_CV = KerasClassifier(build_fn=create_model, epochs=epochs,\n",
      "2023-02-18 12:00:18.922070: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-18 12:00:18.970399: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-18 12:00:19.046647: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-18 12:00:19.068479: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-18 12:00:19.129009: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-18 12:00:19.157473: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-18 12:00:19.163361: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-02-18 12:00:19.163537: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-02-18 12:00:19.189921: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-18 12:00:19.191397: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-02-18 12:00:19.191532: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-02-18 12:00:19.264118: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-18 12:00:19.267954: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-02-18 12:00:19.268080: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-02-18 12:00:19.288789: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-02-18 12:00:19.288824: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-02-18 12:00:19.353566: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-02-18 12:00:19.354044: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-02-18 12:00:19.370267: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-02-18 12:00:19.370404: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-02-18 12:00:19.413479: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-02-18 12:00:19.413607: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-02-18 12:00:19.496077: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-02-18 12:00:19.496199: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-02-18 12:00:20.385082: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-18 12:00:20.385715: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-18 12:00:20.385810: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-02-18 12:00:20.461559: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-18 12:00:20.461818: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-18 12:00:20.461941: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-02-18 12:00:20.493634: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-18 12:00:20.493855: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-18 12:00:20.493918: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-02-18 12:00:20.526440: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-18 12:00:20.526676: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-18 12:00:20.526736: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-02-18 12:00:20.578602: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-18 12:00:20.578858: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-18 12:00:20.578930: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-18 12:00:20.617520: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-18 12:00:20.618152: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-18 12:00:20.618231: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-02-18 12:00:20.647516: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-18 12:00:20.648642: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-18 12:00:20.648750: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-02-18 12:00:20.765208: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-18 12:00:20.765472: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-18 12:00:20.765543: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-02-18 12:00:21.918180: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-02-18 12:00:21.918393: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-02-18 12:00:21.918421: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (LAPTOP-D6IC8GJA): /proc/driver/nvidia/version does not exist\n",
      "2023-02-18 12:00:21.918809: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-18 12:00:22.085055: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-02-18 12:00:22.085187: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-02-18 12:00:22.085245: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (LAPTOP-D6IC8GJA): /proc/driver/nvidia/version does not exist\n",
      "2023-02-18 12:00:22.085679: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-18 12:00:22.133791: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-02-18 12:00:22.133978: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-02-18 12:00:22.134009: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (LAPTOP-D6IC8GJA): /proc/driver/nvidia/version does not exist\n",
      "2023-02-18 12:00:22.134405: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-18 12:00:22.201144: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-02-18 12:00:22.201325: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-02-18 12:00:22.201418: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (LAPTOP-D6IC8GJA): /proc/driver/nvidia/version does not exist\n",
      "2023-02-18 12:00:22.201793: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-18 12:00:22.202339: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-02-18 12:00:22.202494: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-02-18 12:00:22.202567: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (LAPTOP-D6IC8GJA): /proc/driver/nvidia/version does not exist\n",
      "2023-02-18 12:00:22.203000: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-18 12:00:22.260644: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-02-18 12:00:22.260766: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-02-18 12:00:22.260826: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (LAPTOP-D6IC8GJA): /proc/driver/nvidia/version does not exist\n",
      "2023-02-18 12:00:22.261117: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-18 12:00:22.402478: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-02-18 12:00:22.402524: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-02-18 12:00:22.402565: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (LAPTOP-D6IC8GJA): /proc/driver/nvidia/version does not exist\n",
      "2023-02-18 12:00:22.402876: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-18 12:00:22.527459: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-02-18 12:00:22.527588: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-02-18 12:00:22.527634: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (LAPTOP-D6IC8GJA): /proc/driver/nvidia/version does not exist\n",
      "2023-02-18 12:00:22.528009: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "Epoch 1/3\n",
      "Epoch 1/3\n",
      "Epoch 1/3\n",
      "Epoch 1/3\n",
      "Epoch 1/3\n",
      "Epoch 1/3\n",
      "Epoch 1/3\n",
      "245/245 [==============================] - 170s 681ms/step - loss: 0.7688 - acc: 0.1530\n",
      "Epoch 2/3\n",
      "245/245 [==============================] - 173s 690ms/step - loss: 0.4192 - acc: 0.1581\n",
      "245/245 [==============================] - 173s 692ms/step - loss: 0.7028 - acc: 0.1493\n",
      "Epoch 2/3\n",
      "Epoch 2/3\n",
      "245/245 [==============================] - 173s 695ms/step - loss: 0.4204 - acc: 0.1527\n",
      "Epoch 2/3\n",
      "245/245 [==============================] - 187s 745ms/step - loss: 0.4202 - acc: 0.1503\n",
      " 20/245 [=>............................] - ETA: 2:32 - loss: 0.4077 - acc: 0.1859Epoch 2/3\n",
      "245/245 [==============================] - 188s 748ms/step - loss: 0.9947 - acc: 0.1584\n",
      "Epoch 2/3\n",
      "245/245 [==============================] - 190s 760ms/step - loss: 0.8498 - acc: 0.1481\n",
      "Epoch 2/3\n",
      "245/245 [==============================] - 192s 767ms/step - loss: 0.4184 - acc: 0.1540\n",
      "Epoch 2/3\n",
      "245/245 [==============================] - 161s 658ms/step - loss: 0.4101 - acc: 0.1602\n",
      "Epoch 3/3\n",
      "245/245 [==============================] - 164s 667ms/step - loss: 0.4094 - acc: 0.1794\n",
      "Epoch 3/3\n",
      "245/245 [==============================] - 163s 666ms/step - loss: 0.4077 - acc: 0.1891\n",
      "Epoch 3/3\n",
      "245/245 [==============================] - 164s 669ms/step - loss: 0.6329 - acc: 0.1521\n",
      "Epoch 3/3\n",
      "245/245 [==============================] - 175s 714ms/step - loss: 0.6662 - acc: 0.1655\n",
      "Epoch 3/3\n",
      "245/245 [==============================] - 176s 720ms/step - loss: 0.4128 - acc: 0.1543\n",
      "234/245 [===========================>..] - ETA: 8s - loss: 0.4099 - acc: 0.1752Epoch 3/3\n",
      "245/245 [==============================] - 175s 714ms/step - loss: 0.6644 - acc: 0.1691\n",
      "Epoch 3/3\n",
      "245/245 [==============================] - 181s 737ms/step - loss: 0.4100 - acc: 0.1761\n",
      "Epoch 3/3\n",
      "245/245 [==============================] - 171s 696ms/step - loss: 0.4075 - acc: 0.1791\n",
      "245/245 [==============================] - 171s 699ms/step - loss: 0.4006 - acc: 0.2023\n",
      "245/245 [==============================] - 171s 700ms/step - loss: 0.3950 - acc: 0.2230\n",
      "245/245 [==============================] - 172s 704ms/step - loss: 0.6317 - acc: 0.1587\n",
      "245/245 [==============================] - 20s 68ms/step - loss: 0.4061 - acc: 0.1649\n",
      "245/245 [==============================] - 167s 679ms/step - loss: 0.6649 - acc: 0.1633\n",
      "245/245 [==============================] - 166s 676ms/step - loss: 0.4068 - acc: 0.1802\n",
      "245/245 [==============================] - 165s 674ms/step - loss: 0.4103 - acc: 0.1520\n",
      "245/245 [==============================] - 158s 644ms/step - loss: 0.4009 - acc: 0.2019\n",
      " 44/245 [====>.........................] - ETA: 4s - loss: 0.4063 - acc: 0.2124Epoch 1/3\n",
      "245/245 [==============================] - 8s 28ms/step - loss: 0.4068 - acc: 0.2049\n",
      "245/245 [==============================] - 8s 29ms/step - loss: 0.3968 - acc: 0.2148\n",
      " 14/245 [>.............................] - ETA: 52s - loss: 2.1458 - acc: 0.1652Epoch 1/3\n",
      " 25/245 [==>...........................] - ETA: 45s - loss: 1.9180 - acc: 0.1500Epoch 1/3\n",
      "245/245 [==============================] - 23s 86ms/step - loss: 0.3950 - acc: 0.2271\n",
      "245/245 [==============================] - 24s 90ms/step - loss: 0.3986 - acc: 0.2114\n",
      "245/245 [==============================] - 25s 91ms/step - loss: 0.6268 - acc: 0.1605\n",
      "245/245 [==============================] - 27s 99ms/step - loss: 0.4085 - acc: 0.1605\n",
      "245/245 [==============================] - 27s 98ms/step - loss: 0.6690 - acc: 0.1794\n",
      "119/245 [=============>................] - ETA: 1:02 - loss: 1.1872 - acc: 0.1526Epoch 1/3\n",
      "109/245 [============>.................] - ETA: 1:23 - loss: 2.1956 - acc: 0.1075Epoch 1/3\n",
      "154/245 [=================>............] - ETA: 45s - loss: 1.1357 - acc: 0.1520Epoch 1/3\n",
      "110/245 [============>.................] - ETA: 1:22 - loss: 2.1957 - acc: 0.1080Epoch 1/3\n",
      "111/245 [============>.................] - ETA: 1:22 - loss: 2.1957 - acc: 0.1081Epoch 1/3\n",
      "245/245 [==============================] - 177s 703ms/step - loss: 1.0447 - acc: 0.1550\n",
      "Epoch 2/3\n",
      "245/245 [==============================] - 226s 899ms/step - loss: 2.2000 - acc: 0.1087\n",
      "Epoch 2/3\n",
      "245/245 [==============================] - 241s 951ms/step - loss: 0.4199 - acc: 0.1503\n",
      "Epoch 2/3\n",
      "245/245 [==============================] - 297s 1s/step - loss: 0.4166 - acc: 0.1576\n",
      "Epoch 2/3\n",
      "245/245 [==============================] - 376s 1s/step - loss: 0.8818 - acc: 0.1543\n",
      "Epoch 2/3\n",
      "245/245 [==============================] - 376s 1s/step - loss: 0.4196 - acc: 0.1564\n",
      "Epoch 2/3\n",
      "245/245 [==============================] - 379s 1s/step - loss: 2.1989 - acc: 0.1041\n",
      "  2/245 [..............................] - ETA: 5:42 - loss: 0.5386 - acc: 0.1875Epoch 2/3\n",
      "245/245 [==============================] - 378s 1s/step - loss: 0.4172 - acc: 0.1610\n",
      "Epoch 2/3\n",
      "245/245 [==============================] - 310s 1s/step - loss: 0.9163 - acc: 0.1642\n",
      "Epoch 3/3\n",
      "245/245 [==============================] - 306s 1s/step - loss: 2.2036 - acc: 0.1033\n",
      "Epoch 3/3\n",
      "245/245 [==============================] - 307s 1s/step - loss: 0.4126 - acc: 0.1531\n",
      "Epoch 3/3\n",
      "245/245 [==============================] - 305s 1s/step - loss: 0.4096 - acc: 0.1752\n",
      "Epoch 3/3\n",
      "245/245 [==============================] - 307s 1s/step - loss: 0.8678 - acc: 0.1714\n",
      "245/245 [==============================] - 348s 1s/step - loss: 2.2036 - acc: 0.1032\n",
      "Epoch 3/3\n",
      "245/245 [==============================] - 352s 1s/step - loss: 0.5958 - acc: 0.1489\n",
      "Epoch 3/3\n",
      "245/245 [==============================] - 352s 1s/step - loss: 0.4117 - acc: 0.1573\n",
      "118/245 [=============>................] - ETA: 2:35 - loss: 0.4027 - acc: 0.1949Epoch 3/3\n",
      "245/245 [==============================] - 351s 1s/step - loss: 0.4095 - acc: 0.1750\n",
      "Epoch 3/3\n",
      "245/245 [==============================] - 297s 1s/step - loss: 2.2036 - acc: 0.1033\n",
      "245/245 [==============================] - 297s 1s/step - loss: 0.4063 - acc: 0.1843\n",
      "245/245 [==============================] - 59s 225ms/step - loss: 0.6249 - acc: 0.1618\n",
      "245/245 [==============================] - 58s 218ms/step - loss: 2.2036 - acc: 0.1055\n",
      " 50/245 [=====>........................] - ETA: 42s - loss: 0.4037 - acc: 0.1912Epoch 1/3\n",
      " 62/245 [======>.......................] - ETA: 40s - loss: 0.4033 - acc: 0.1941Epoch 1/3\n",
      "245/245 [==============================] - 273s 1s/step - loss: 0.4000 - acc: 0.2068\n",
      "245/245 [==============================] - 58s 222ms/step - loss: 0.4031 - acc: 0.1969\n",
      " 91/245 [==========>...................] - ETA: 1:15 - loss: 0.9256 - acc: 0.1463Epoch 1/3\n",
      "245/245 [==============================] - 60s 231ms/step - loss: 0.3959 - acc: 0.2240\n",
      " 36/245 [===>..........................] - ETA: 1:55 - loss: 0.4493 - acc: 0.1389Epoch 1/3\n",
      "245/245 [==============================] - 153s 500ms/step - loss: 0.7644 - acc: 0.1474\n",
      "Epoch 2/3\n",
      "245/245 [==============================] - 158s 511ms/step - loss: 0.9128 - acc: 0.1582\n",
      "Epoch 2/3\n",
      "245/245 [==============================] - 309s 1s/step - loss: 2.2036 - acc: 0.1033\n",
      "245/245 [==============================] - 309s 1s/step - loss: 0.5939 - acc: 0.1682\n",
      "245/245 [==============================] - 309s 1s/step - loss: 0.4025 - acc: 0.1934\n",
      "245/245 [==============================] - 309s 1s/step - loss: 0.3993 - acc: 0.2133\n",
      "245/245 [==============================] - 166s 519ms/step - loss: 0.4194 - acc: 0.1610\n",
      "Epoch 2/3\n",
      "245/245 [==============================] - 119s 487ms/step - loss: 0.6743 - acc: 0.1579\n",
      "Epoch 3/3\n",
      "245/245 [==============================] - 121s 495ms/step - loss: 0.8059 - acc: 0.1664\n",
      " 98/245 [===========>..................] - ETA: 43s - loss: 0.3960 - acc: 0.2165Epoch 3/3\n",
      "245/245 [==============================] - 162s 482ms/step - loss: 0.4182 - acc: 0.1607\n",
      "Epoch 2/3\n",
      "245/245 [==============================] - 71s 277ms/step - loss: 0.3991 - acc: 0.2011\n",
      "245/245 [==============================] - 71s 279ms/step - loss: 2.2036 - acc: 0.1055\n",
      "245/245 [==============================] - 71s 278ms/step - loss: 0.3963 - acc: 0.2038\n",
      "245/245 [==============================] - 73s 286ms/step - loss: 0.5862 - acc: 0.1799\n",
      "141/245 [================>.............] - ETA: 48s - loss: 0.6783 - acc: 0.1625Epoch 1/3\n",
      " 96/245 [==========>...................] - ETA: 1:04 - loss: 0.4133 - acc: 0.1592Epoch 1/3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142/245 [================>.............] - ETA: 47s - loss: 0.6780 - acc: 0.1624Epoch 1/3\n",
      "143/245 [================>.............] - ETA: 46s - loss: 0.6781 - acc: 0.1626Epoch 1/3\n",
      "245/245 [==============================] - 108s 440ms/step - loss: 0.4085 - acc: 0.1889\n",
      "Epoch 3/3\n",
      "245/245 [==============================] - 112s 456ms/step - loss: 0.6736 - acc: 0.1641\n",
      "245/245 [==============================] - 114s 464ms/step - loss: 0.8048 - acc: 0.1741\n",
      "245/245 [==============================] - 113s 460ms/step - loss: 0.4118 - acc: 0.1642\n",
      "Epoch 3/3\n",
      "245/245 [==============================] - 29s 101ms/step - loss: 0.6837 - acc: 0.1604\n",
      "245/245 [==============================] - 29s 101ms/step - loss: 0.8167 - acc: 0.1581\n",
      "134/245 [===============>..............] - ETA: 1:09 - loss: 0.4211 - acc: 0.1474Epoch 1/3\n",
      " 74/245 [========>.....................] - ETA: 1:38 - loss: 0.4073 - acc: 0.1799Epoch 1/3\n",
      "245/245 [==============================] - 131s 534ms/step - loss: 0.3976 - acc: 0.2160\n",
      "245/245 [==============================] - 27s 97ms/step - loss: 0.3962 - acc: 0.2294\n",
      "245/245 [==============================] - 171s 596ms/step - loss: 0.4177 - acc: 0.1517\n",
      "Epoch 2/3\n",
      "245/245 [==============================] - 172s 598ms/step - loss: 0.4188 - acc: 0.1469\n",
      "Epoch 2/3\n",
      "245/245 [==============================] - 174s 607ms/step - loss: 1.1632 - acc: 0.1616\n",
      "Epoch 2/3\n",
      "245/245 [==============================] - 176s 611ms/step - loss: 0.7862 - acc: 0.1582\n",
      "Epoch 2/3\n",
      " 31/245 [==>...........................] - ETA: 2:09 - loss: 1.0553 - acc: 0.1562Epoch 1/3\n",
      "245/245 [==============================] - 135s 552ms/step - loss: 0.4052 - acc: 0.1918\n",
      "245/245 [==============================] - 27s 97ms/step - loss: 0.4050 - acc: 0.1747\n",
      " 17/245 [=>............................] - ETA: 4:46 - loss: 0.4598 - acc: 0.1691Epoch 1/3\n",
      "245/245 [==============================] - 146s 595ms/step - loss: 0.4126 - acc: 0.1552\n",
      "Epoch 3/3\n",
      "245/245 [==============================] - 147s 601ms/step - loss: 0.4112 - acc: 0.1635\n",
      "Epoch 3/3\n",
      "245/245 [==============================] - 147s 599ms/step - loss: 0.9381 - acc: 0.1631\n",
      "Epoch 3/3\n",
      "245/245 [==============================] - 147s 601ms/step - loss: 0.5900 - acc: 0.1564\n",
      "Epoch 3/3\n",
      "245/245 [==============================] - 343s 1s/step - loss: 3.7257 - acc: 0.1462\n",
      "Epoch 2/3\n",
      "245/245 [==============================] - 340s 1s/step - loss: 0.9728 - acc: 0.1531\n",
      "Epoch 2/3\n",
      "245/245 [==============================] - 166s 678ms/step - loss: 0.4089 - acc: 0.1731\n",
      "245/245 [==============================] - 166s 675ms/step - loss: 0.8051 - acc: 0.1682\n",
      "245/245 [==============================] - 169s 689ms/step - loss: 0.4027 - acc: 0.2047\n",
      "245/245 [==============================] - 166s 678ms/step - loss: 0.4101 - acc: 0.1594\n",
      "245/245 [==============================] - 23s 79ms/step - loss: 0.4019 - acc: 0.1866\n",
      " 57/245 [=====>........................] - ETA: 3:06 - loss: 0.4115 - acc: 0.1667Epoch 1/3\n",
      "245/245 [==============================] - 327s 1s/step - loss: 0.4187 - acc: 0.1676\n",
      "Epoch 2/3\n",
      "245/245 [==============================] - 28s 105ms/step - loss: 0.3955 - acc: 0.2229\n",
      "245/245 [==============================] - 29s 110ms/step - loss: 0.8155 - acc: 0.1596\n",
      "245/245 [==============================] - 29s 108ms/step - loss: 0.4096 - acc: 0.1745\n",
      "111/245 [============>.................] - ETA: 2:05 - loss: 0.4107 - acc: 0.1610Epoch 1/3\n",
      "112/245 [============>.................] - ETA: 2:04 - loss: 0.4106 - acc: 0.1613Epoch 1/3\n",
      " 62/245 [======>.......................] - ETA: 2:42 - loss: 0.4142 - acc: 0.1557Epoch 1/3\n",
      "245/245 [==============================] - 320s 1s/step - loss: 0.4195 - acc: 0.1576\n",
      "Epoch 2/3\n",
      "245/245 [==============================] - 251s 1s/step - loss: 3.7398 - acc: 0.1466\n",
      "Epoch 3/3\n",
      "245/245 [==============================] - 252s 1s/step - loss: 0.4105 - acc: 0.1651\n",
      "Epoch 3/3\n",
      "245/245 [==============================] - 274s 1s/step - loss: 0.4102 - acc: 0.1727\n",
      "Epoch 3/3\n",
      "245/245 [==============================] - 356s 1s/step - loss: 0.9553 - acc: 0.1534\n",
      "Epoch 2/3\n",
      "245/245 [==============================] - 298s 1s/step - loss: 0.4107 - acc: 0.1702\n",
      "Epoch 3/3\n",
      "245/245 [==============================] - 381s 1s/step - loss: 2.2141 - acc: 0.1049\n",
      "Epoch 2/3\n",
      "245/245 [==============================] - 382s 1s/step - loss: 0.4181 - acc: 0.1571\n",
      "Epoch 2/3\n",
      "245/245 [==============================] - 395s 1s/step - loss: 0.4197 - acc: 0.1516\n",
      "Epoch 2/3\n",
      "245/245 [==============================] - 307s 1s/step - loss: 3.7396 - acc: 0.1466\n",
      "245/245 [==============================] - 306s 1s/step - loss: 0.4126 - acc: 0.1585\n",
      "245/245 [==============================] - 67s 244ms/step - loss: 3.7244 - acc: 0.1501\n",
      "245/245 [==============================] - 66s 241ms/step - loss: 0.4086 - acc: 0.1591\n",
      "245/245 [==============================] - 300s 1s/step - loss: 0.3981 - acc: 0.2180\n",
      "245/245 [==============================] - 271s 1s/step - loss: 0.4010 - acc: 0.1953\n",
      "245/245 [==============================] - 54s 195ms/step - loss: 0.3977 - acc: 0.2143\n",
      "245/245 [==============================] - 304s 1s/step - loss: 0.8583 - acc: 0.1603\n",
      "Epoch 3/3\n",
      "245/245 [==============================] - 44s 159ms/step - loss: 0.4026 - acc: 0.1879\n",
      "245/245 [==============================] - 271s 1s/step - loss: 2.2036 - acc: 0.1033\n",
      "Epoch 3/3\n",
      "245/245 [==============================] - 272s 1s/step - loss: 0.4109 - acc: 0.1623\n",
      "  1/245 [..............................] - ETA: 3:25 - loss: 2.2036 - acc: 0.0625Epoch 3/3\n",
      "245/245 [==============================] - 257s 1s/step - loss: 0.4095 - acc: 0.1722\n",
      "Epoch 3/3\n",
      "245/245 [==============================] - 181s 737ms/step - loss: 0.8571 - acc: 0.1719\n",
      "245/245 [==============================] - 165s 671ms/step - loss: 0.3994 - acc: 0.2029\n",
      "245/245 [==============================] - 166s 678ms/step - loss: 2.2039 - acc: 0.1033\n",
      "245/245 [==============================] - 157s 640ms/step - loss: 0.3993 - acc: 0.2074\n",
      "245/245 [==============================] - 17s 63ms/step - loss: 2.2036 - acc: 0.1055\n",
      "245/245 [==============================] - 18s 72ms/step - loss: 0.4011 - acc: 0.2089\n",
      "245/245 [==============================] - 16s 61ms/step - loss: 0.8551 - acc: 0.1889\n",
      "245/245 [==============================] - 8s 30ms/step - loss: 0.3923 - acc: 0.2296\n",
      "Epoch 1/3\n",
      "490/490 [==============================] - 93s 188ms/step - loss: 0.4125 - acc: 0.1678\n",
      "Epoch 2/3\n",
      "490/490 [==============================] - 90s 184ms/step - loss: 0.3956 - acc: 0.2131\n",
      "Epoch 3/3\n",
      "490/490 [==============================] - 89s 182ms/step - loss: 0.3829 - acc: 0.2459\n",
      "Best Accuracy for dataset ./data/song-data-labels-cleaned-seven-moods.csv is: 0.2192848026752472 using {'adapt_embedding': False, 'filters': 100, 'kernel': 5, 'mood_count': 7, 'multiplicator': 2, 'multiplicator2': 4, 'outputfunction': 'softmax'}\n",
      " mean=0.1627, std=0.002171 using {'adapt_embedding': True, 'filters': 50, 'kernel': 5, 'mood_count': 7, 'multiplicator': 2, 'multiplicator2': 3, 'outputfunction': 'relu'}\n",
      " mean=0.2192, std=0.007854 using {'adapt_embedding': True, 'filters': 50, 'kernel': 5, 'mood_count': 7, 'multiplicator': 2, 'multiplicator2': 3, 'outputfunction': 'softmax'}\n",
      " mean=0.17, std=0.009451 using {'adapt_embedding': True, 'filters': 50, 'kernel': 5, 'mood_count': 7, 'multiplicator': 2, 'multiplicator2': 4, 'outputfunction': 'relu'}\n",
      " mean=0.2098, std=0.004981 using {'adapt_embedding': True, 'filters': 50, 'kernel': 5, 'mood_count': 7, 'multiplicator': 2, 'multiplicator2': 4, 'outputfunction': 'softmax'}\n",
      " mean=0.1337, std=0.02816 using {'adapt_embedding': True, 'filters': 100, 'kernel': 5, 'mood_count': 7, 'multiplicator': 2, 'multiplicator2': 3, 'outputfunction': 'relu'}\n",
      " mean=0.2105, std=0.01354 using {'adapt_embedding': True, 'filters': 100, 'kernel': 5, 'mood_count': 7, 'multiplicator': 2, 'multiplicator2': 3, 'outputfunction': 'softmax'}\n",
      " mean=0.1427, std=0.03723 using {'adapt_embedding': True, 'filters': 100, 'kernel': 5, 'mood_count': 7, 'multiplicator': 2, 'multiplicator2': 4, 'outputfunction': 'relu'}\n",
      " mean=0.2025, std=0.001341 using {'adapt_embedding': True, 'filters': 100, 'kernel': 5, 'mood_count': 7, 'multiplicator': 2, 'multiplicator2': 4, 'outputfunction': 'softmax'}\n",
      " mean=0.1593, std=0.001149 using {'adapt_embedding': False, 'filters': 50, 'kernel': 5, 'mood_count': 7, 'multiplicator': 2, 'multiplicator2': 3, 'outputfunction': 'relu'}\n",
      " mean=0.202, std=0.02733 using {'adapt_embedding': False, 'filters': 50, 'kernel': 5, 'mood_count': 7, 'multiplicator': 2, 'multiplicator2': 3, 'outputfunction': 'softmax'}\n",
      " mean=0.167, std=0.007407 using {'adapt_embedding': False, 'filters': 50, 'kernel': 5, 'mood_count': 7, 'multiplicator': 2, 'multiplicator2': 4, 'outputfunction': 'relu'}\n",
      " mean=0.2047, std=0.01814 using {'adapt_embedding': False, 'filters': 50, 'kernel': 5, 'mood_count': 7, 'multiplicator': 2, 'multiplicator2': 4, 'outputfunction': 'softmax'}\n",
      " mean=0.1546, std=0.004534 using {'adapt_embedding': False, 'filters': 100, 'kernel': 5, 'mood_count': 7, 'multiplicator': 2, 'multiplicator2': 3, 'outputfunction': 'relu'}\n",
      " mean=0.2011, std=0.01322 using {'adapt_embedding': False, 'filters': 100, 'kernel': 5, 'mood_count': 7, 'multiplicator': 2, 'multiplicator2': 3, 'outputfunction': 'softmax'}\n",
      " mean=0.1472, std=0.0417 using {'adapt_embedding': False, 'filters': 100, 'kernel': 5, 'mood_count': 7, 'multiplicator': 2, 'multiplicator2': 4, 'outputfunction': 'relu'}\n",
      " mean=0.2193, std=0.01034 using {'adapt_embedding': False, 'filters': 100, 'kernel': 5, 'mood_count': 7, 'multiplicator': 2, 'multiplicator2': 4, 'outputfunction': 'softmax'}\n",
      "####################################################################################################\n"
     ]
    }
   ],
   "source": [
    "input_data = [\n",
    "    [\"./data/song-data-labels-cleaned.csv\",17],\n",
    "    [\"./data/song-data-labels-cleaned-quadrant.csv\",4],\n",
    "    [\"./data/song-data-labels-cleaned-seven-moods.csv\",7]\n",
    "]\n",
    "for i in input_data:\n",
    "    test_dataset(i[0], i[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fcaafdd",
   "metadata": {},
   "source": [
    "# Sample prediction\n",
    "Code might be broken due to heavy restructuring of the notebook, but it should only assist as a \"documentation\" for us developers anyway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f391ee6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "def tokenization(text: list[spacy.tokens.token.Token]) -> list[spacy.tokens.token.Token]:\n",
    "    \"\"\"Use this function to tokenize text.\n",
    "\n",
    "    :param text: Text as list\n",
    "    :type text: list[spacy.tokens.token.Token]\n",
    "    :return: Tokenized text as list\n",
    "    :rtype: list[spacy.tokens.token.Token]\n",
    "    \"\"\"\n",
    "\n",
    "    token_list = []\n",
    "    for doc in text: \n",
    "        # iterate over tokens in docs\n",
    "        for token in doc:\n",
    "            token_list.append(token)\n",
    "\n",
    "    return token_list\n",
    "\n",
    "\n",
    "def stop_word_removal(text: list[spacy.tokens.token.Token]) -> list[spacy.tokens.token.Token]: \n",
    "    \"\"\"Use this function to remove stop words. \n",
    "\n",
    "    :param text: Tokens to remove stop words from \n",
    "    :type text: list[spacy.tokens.token.Token]\n",
    "    :return: Tokens without stop words\n",
    "    :rtype: list[spacy.tokens.token.Token]\n",
    "    \"\"\"\n",
    "\n",
    "    token_list_without_stop = []\n",
    "    # Don't add token to list if stop word\n",
    "    for token in text:\n",
    "        if token.is_stop == False: \n",
    "            token_list_without_stop.append(token)\n",
    "\n",
    "    return token_list_without_stop\n",
    "\n",
    "\n",
    "def punctutation_removal(text: list[spacy.tokens.token.Token]) -> list[spacy.tokens.token.Token]: \n",
    "    \"\"\"Use this function to remove punctuation.\n",
    "\n",
    "    :param text: Tokens to remove punctuation from\n",
    "    :type text: list[spacy.tokens.token.Token]\n",
    "    :return: Tokens without punctuation\n",
    "    :rtype: list[spacy.tokens.token.Token]\n",
    "    \"\"\"\n",
    "\n",
    "    token_list_no_stop_no_punct = []\n",
    "    # Don't add token to list if punctuation\n",
    "    for token in text:\n",
    "        if token.is_punct == False:\n",
    "            token_list_no_stop_no_punct.append(token)\n",
    "\n",
    "    return token_list_no_stop_no_punct\n",
    "\n",
    "\n",
    "def lemmatization(text: list[spacy.tokens.token.Token]) -> list[str]: \n",
    "    \"\"\"Use this function to lemmatize a given text.\n",
    "\n",
    "    :param text: Tokens to lemmatize\n",
    "    :type text: list[spacy.tokens.token.Token]\n",
    "    :return: lemmatized tokens\n",
    "    :rtype: list[str]\n",
    "    \"\"\"\n",
    "\n",
    "    token_list_no_stop_no_punct_lemmatized = []\n",
    "    for token in text: \n",
    "        if \"\\n\" not in token.lemma_:\n",
    "            token_list_no_stop_no_punct_lemmatized.append(token.lemma_)\n",
    "    return token_list_no_stop_no_punct_lemmatized\n",
    "\n",
    "\n",
    "def processing_pipeline(song_data: dict) -> dict:\n",
    "    \"\"\"Use this function to execute the entire processing pipeline on given song data.\n",
    "    Preprocessing steps:\n",
    "    - Tokenization\n",
    "    - Stop word removal\n",
    "    - Punctuation removal\n",
    "    - Lemmatization\n",
    "    - ...\n",
    "\n",
    "    :param song_data: song data saved in a json file containing song name, artist name and lyrics\n",
    "    :type song_data: dict\n",
    "    :return: preprocessed song data\n",
    "    :rtype: dict\n",
    "    \"\"\"\n",
    "\n",
    "    nlp = spacy.load(\"en_core_web_sm\", disable = ['ner'])\n",
    "    text_nlp_pipe = list(nlp.pipe([song_data[\"Lyrics\"]]))\n",
    "    \n",
    "    # Tokenization\n",
    "    song_data[\"Lyrics\"] = tokenization(text_nlp_pipe)\n",
    "    # Stop word removal\n",
    "    song_data[\"Lyrics\"] = stop_word_removal(song_data[\"Lyrics\"])\n",
    "    # Punctuation removal\n",
    "    song_data[\"Lyrics\"] = punctutation_removal(song_data[\"Lyrics\"])\n",
    "    # Lemmatization\n",
    "    song_data[\"Lyrics\"] = lemmatization(song_data[\"Lyrics\"])\n",
    "\n",
    "    return song_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "578cbdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test model:\n",
    "test_song = {\n",
    "        \"Song\": \"Mockingbird\",\n",
    "        \"Artist\": \"Eminem\",\n",
    "        \"Lyrics\": (\n",
    "            \"\"\"Yeah\n",
    "            I know sometimes things may not always make sense to you right now\n",
    "            But hey, what daddy always tell you?\n",
    "            Straighten up little soldier\n",
    "            Stiffen up that upper lip\n",
    "            What you crying about?\n",
    "            You got me\n",
    "            Hailie, I know you miss your mom, and I know you miss your dad\n",
    "            When I'm gone, but I'm trying to give you the life that I never had\n",
    "            I can see you're sad, even when you smile, even when you laugh\n",
    "            I can see it in your eyes, deep inside you want to cry\n",
    "            'Cause you're scared, I ain't there, daddy's with you in your prayers\n",
    "            No more crying, wipe them tears, daddy's here, no more nightmares\n",
    "            We gon' pull together through it, we gon' do it\n",
    "            Laney uncle's crazy, ain't he? Yeah, but he loves you girl and you better know it\n",
    "            We're all we got in this world, when it spins, when it swirls\n",
    "            When it whirls, when it twirls, two little beautiful girls\n",
    "            Lookin' puzzled, in a daze, I know it's confusing you\n",
    "            Daddy's always on the move, mamma's always on the news\n",
    "            I try to keep you sheltered from it, but somehow it seems\n",
    "            The harder that I try to do that, the more it backfires on me\n",
    "            All the things growing up, his daddy, daddy had to see\n",
    "            Daddy don't want you to see, but you see just as much as he did\n",
    "            We did not plan it to be this way, your mother and me\n",
    "            But things have gotten so bad between us, I don't see us ever being together\n",
    "            Ever again like we used to be when we was teenagers\n",
    "            But then of course everything always happens for a reason\n",
    "            I guess it was never meant to be\n",
    "            But it's just something we have no control, over and that's what destiny is\n",
    "            But no more worries, rest your head and go to sleep\n",
    "            Maybe one day we'll wake up, and this will all just be a dream\n",
    "            Now hush little baby, don't you cry\n",
    "            Everything's gonna be alright\n",
    "            Stiffen that upper-lip up, little lady, I told ya\n",
    "            Daddy's here to hold ya through the night\n",
    "            I know mommy's not here right now, and we don't know why\n",
    "            We fear how we feel inside\n",
    "            It may seem a little crazy, pretty baby\n",
    "            But I promise momma's gon' be alright\n",
    "            Huh, it's funny\n",
    "            I remember back one year when daddy had no money\n",
    "            Mommy wrapped the Christmas presents up and stuck 'em under the tree\n",
    "            And said, \"Some of 'em were from me, 'cause Daddy couldn't buy 'em\"\n",
    "            I'll never forget that Christmas, I sat up the whole night crying\n",
    "            'Cause daddy felt like a bum\n",
    "            See daddy had a job\n",
    "            But his job was to keep the food on the table for you and mom\n",
    "            And at the time every house that we lived in\n",
    "            Either kept getting broke into and robbed\n",
    "            Or shot up on the block\n",
    "            And your Mom was saving money for you in a jar\n",
    "            Tryna start a piggy bank for you, so you could go to college\n",
    "            Almost had a thousand dollars 'til someone broke in and stole it\n",
    "            And I know it hurt so bad, it broke your momma's heart\n",
    "            And it seemed like everything was just startin' to fall apart\n",
    "            Mom and dad was arguin' a lot, so momma moved back\n",
    "            On the Chalmers in the flat one-bedroom apartment\n",
    "            And dad moved back to the other side of 8 Mile on Novara\n",
    "            And that's when daddy went to California with his C.D\n",
    "            And met Dr. Dre, and flew you and momma out to see me\n",
    "            But daddy had to work, you and momma had to leave me\n",
    "            Then you started seeing daddy on the T.V\n",
    "            And momma didn't like it, and you and Laney were to young to understand it\n",
    "            Papa was a rollin' stone, momma developed a habit\n",
    "            And it all happened too fast for either one of us to grab it\n",
    "            I'm just sorry you were there and had to witness it first hand\n",
    "            'Cause all I ever wanted to do was just make you proud\n",
    "            Now I'm sittin' in this empty house\n",
    "            Just reminiscing, lookin' at your baby pictures\n",
    "            It just trips me out\n",
    "            To see how much you both have grown\n",
    "            It's almost like you're sisters now\n",
    "            Wow, guess you pretty much are, and daddy's still here\n",
    "            Laney, I'm talkin' to you too, daddy's still here\n",
    "            I like the sound of that, yeah, It's got a ring to it, don't it?\n",
    "            Shh, momma's only gone for the moment\n",
    "            Now hush little baby, don't you cry\n",
    "            Everything's gonna be alright\n",
    "            Stiffen that upper-lip up, little lady, I told ya\n",
    "            Daddy's here to hold ya through the night\n",
    "            I know mommy's not here right now, and we don't know why\n",
    "            We fear how we feel inside\n",
    "            It may seem a little crazy, pretty baby\n",
    "            But I promise, momma's gon' be alright\n",
    "            And if you ask me too\n",
    "            Daddy's gonna buy you a Mockingbird\n",
    "            I'ma give you the world\n",
    "            I'ma buy a diamond ring for you, I'ma sing for you\n",
    "            I'll do anything for you to see you smile\n",
    "            And if that Mockingbird don't sing, and that ring don't shine\n",
    "            I'ma break that birdies neck\n",
    "            I'd go back to the jeweler who sold it to ya\n",
    "            And make him eat every karat, don't fuck with dad (haha)\"\"\"\n",
    "        ),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "8781897a",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_song = processing_pipeline(test_song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "b0c51077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['yeah', 'know', 'thing', 'sense', 'right', 'hey', 'daddy', 'tell', 'straighten', 'little', 'soldier', 'Stiffen', 'upper', 'lip', 'cry', 'get', 'Hailie', 'know', 'miss', 'mom', 'know', 'miss', 'dad', 'go', 'try', 'life', 'sad', 'smile', 'laugh', 'eye', 'deep', 'inside', 'want', 'cry', \"'cause\", 'scared', 'be', 'daddy', 'prayer', 'crying', 'wipe', 'tear', 'daddy', 'nightmare', 'gon', 'pull', 'gon', 'Laney', 'uncle', 'crazy', 'be', 'yeah', 'love', 'girl', 'well', 'know', 'get', 'world', 'spin', 'swirl', 'whirl', 'twirl', 'little', 'beautiful', 'girl', 'Lookin', 'puzzle', 'daze', 'know', 'confuse', 'Daddy', 'mamma', 'news', 'try', 'shelter', 'hard', 'try', 'backfire', 'thing', 'grow', 'daddy', 'daddy', 'daddy', 'want', 'plan', 'way', 'mother', 'thing', 'get', 'bad', 'like', 'teenager', 'course', 'happen', 'reason', 'guess', 'mean', 'control', 'destiny', 'worry', 'rest', 'head', 'sleep', 'maybe', 'day', 'wake', 'dream', 'hush', 'little', 'baby', 'cry', 'go', 'to', 'alright', 'Stiffen', 'upper', 'lip', 'little', 'lady', 'tell', 'ya', 'Daddy', 'hold', 'ya', 'night', 'know', 'mommy', 'right', 'know', 'fear', 'feel', 'inside', 'little', 'crazy', 'pretty', 'baby', 'promise', 'momma', 'gon', 'alright', 'Huh', 'funny', 'remember', 'year', 'daddy', 'money', 'Mommy', 'wrap', 'Christmas', 'present', 'stick', 'them', 'tree', 'say', 'them', \"'cause\", 'daddy', 'buy', 'them', 'forget', 'Christmas', 'sit', 'night', 'cry', \"'cause\", 'daddy', 'feel', 'like', 'bum', 'daddy', 'job', 'job', 'food', 'table', 'mom', 'time', 'house', 'live', 'keep', 'getting', 'break', 'rob', 'shoot', 'block', 'Mom', 'save', 'money', 'jar', 'Tryna', 'start', 'piggy', 'bank', 'college', 'thousand', 'dollar', 'til', 'break', 'steal', 'know', 'hurt', 'bad', 'break', 'momma', 'heart', 'like', 'startin', 'fall', 'apart', 'Mom', 'dad', 'arguin', 'lot', 'momma', 'move', 'Chalmers', 'flat', 'bedroom', 'apartment', 'dad', 'move', '8', 'Mile', 'Novara', 'daddy', 'go', 'California', 'C.D', 'meet', 'Dr.', 'Dre', 'fly', 'momma', 'daddy', 'work', 'momma', 'leave', 'start', 'see', 'daddy', 'T.V', 'momma', 'like', 'Laney', 'young', 'understand', 'Papa', 'rollin', 'stone', 'momma', 'develop', 'habit', 'happen', 'fast', 'grab', 'sorry', 'witness', 'hand', \"'cause\", 'want', 'proud', 'sittin', 'house', 'reminiscing', 'lookin', 'baby', 'picture', 'trip', 'grow', 'like', 'sister', 'wow', 'guess', 'pretty', 'daddy', 'Laney', 'talkin', 'daddy', 'like', 'sound', 'yeah', 'get', 'ring', 'Shh', 'momma', 'go', 'moment', 'hush', 'little', 'baby', 'cry', 'go', 'to', 'alright', 'Stiffen', 'upper', 'lip', 'little', 'lady', 'tell', 'ya', 'Daddy', 'hold', 'ya', 'night', 'know', 'mommy', 'right', 'know', 'fear', 'feel', 'inside', 'little', 'crazy', 'pretty', 'baby', 'promise', 'momma', 'gon', 'alright', 'ask', 'Daddy', 'go', 'to', 'buy', 'Mockingbird', 'world', 'buy', 'diamond', 'ring', 'sing', 'smile', 'Mockingbird', 'sing', 'ring', 'shine', 'break', 'birdie', 'neck', 'jeweler', 'sell', 'ya', 'eat', 'karat', 'fuck', 'dad', 'haha']\n"
     ]
    }
   ],
   "source": [
    "print(processed_song[\"Lyrics\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "e72b5d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the tokenizer\n",
    "with open('tokenizer.pickle', 'rb') as handle:\n",
    "    token = pickle.load(handle)\n",
    "# tokenize\n",
    "text = token.texts_to_sequences([processed_song[\"Lyrics\"]])\n",
    "text = pad_sequences(text, 180)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "1f801eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  179    26    60    62   447    10     3  1320   447   775   775  1071\n",
      "    855  1017     8   265    41   218  1169    53  1722   413   869  1017\n",
      "    156   211  2485  1636    82  6451  1412  3248   459   900   285    53\n",
      "    371     2   136    98    53  1139    21     3  2863    37   258  1017\n",
      "    906 17626   304  1139   255  1561  1258  2518   906   255  1689   392\n",
      "    447     4   929   147  2364   126  1139   447   150  1139    24    82\n",
      "     73   447  1139     3 19532   189   162  1033  1047   314  1139  6769\n",
      "   1766   232   242   702   291  1315    49    62     9   757  1193   265\n",
      "  14576   528    14   365   664   180     3   601  1212   208   267   447\n",
      "  19532   603   447     3   167    18     6   317  6076  1139     4   249\n",
      "   1099    43    14    60     4    13   166  9315  3856   334    43   313\n",
      "     20   110   447    40   110    26     2  2008    27     2   159    10\n",
      "     64    43   182   267    14   262  1139   571   166   140   447     4\n",
      "     13   349 12588    33   349   638   317   121   124 12588   121   317\n",
      "    144    53  7780   797  8104   445   110   424 12472   153   906  3373]]\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "70e71d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 21ms/step\n"
     ]
    }
   ],
   "source": [
    "pred = keras_model.predict(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "1a8601a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.03035141 0.07305858 0.01072862 0.00733498 0.10687384 0.01872347\n",
      "  0.00504669 0.09882194 0.00760916 0.00933376 0.00872805 0.02308791\n",
      "  0.19617064 0.00064153 0.08658496 0.19915532 0.11774925]]\n"
     ]
    }
   ],
   "source": [
    "print(pred)\n",
    "# propability distr."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "de5d65b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_mood=np.argmax(pred,axis=1)\n",
    "# get the mood that is predicted the most "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "0a0dde91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15]\n"
     ]
    }
   ],
   "source": [
    "print(pred_mood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "1dc196a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sad']\n"
     ]
    }
   ],
   "source": [
    "# load the label encoder\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "encoder.classes_ = numpy.load('label_encoder.npy', allow_pickle=True)\n",
    "\n",
    "print(encoder.inverse_transform(pred_mood))\n",
    "# reverse transform the mood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ac45eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ta_3.10",
   "language": "python",
   "name": "ta_3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
