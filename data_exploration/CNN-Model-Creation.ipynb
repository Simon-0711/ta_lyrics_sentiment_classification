{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "43598701",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.keras.layers import Dense, Embedding,GlobalMaxPooling1D\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Embedding\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Dropout, Conv1D, MaxPool1D, GlobalMaxPool1D, Embedding, Activation\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import PorterStemmer\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Preprocessing and model is used from \n",
    "# https://www.kaggle.com/code/jagannathrk/word2vec-cnn-text-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "291f2af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "def tokenization(text):\n",
    "    \"\"\"Use this function to tokenize text.\n",
    "\n",
    "    :param text: Text as list\n",
    "    :type text: list[spacy.tokens.token.Token]\n",
    "    :return: Tokenized text as list\n",
    "    :rtype: list[spacy.tokens.token.Token]\n",
    "    \"\"\"\n",
    "\n",
    "    token_list = []\n",
    "    for doc in text: \n",
    "        # iterate over tokens in docs\n",
    "        for token in doc:\n",
    "            token_list.append(token)\n",
    "\n",
    "    return token_list\n",
    "\n",
    "\n",
    "def stop_word_removal(text): \n",
    "    \"\"\"Use this function to remove stop words. \n",
    "\n",
    "    :param text: Tokens to remove stop words from \n",
    "    :type text: list[spacy.tokens.token.Token]\n",
    "    :return: Tokens without stop words\n",
    "    :rtype: list[spacy.tokens.token.Token]\n",
    "    \"\"\"\n",
    "\n",
    "    token_list_without_stop = []\n",
    "    # Don't add token to list if stop word\n",
    "    for token in text:\n",
    "        if token.is_stop == False: \n",
    "            token_list_without_stop.append(token)\n",
    "\n",
    "    return token_list_without_stop\n",
    "\n",
    "\n",
    "def punctutation_removal(text): \n",
    "    \"\"\"Use this function to remove punctuation.\n",
    "\n",
    "    :param text: Tokens to remove punctuation from\n",
    "    :type text: list[spacy.tokens.token.Token]\n",
    "    :return: Tokens without punctuation\n",
    "    :rtype: list[spacy.tokens.token.Token]\n",
    "    \"\"\"\n",
    "\n",
    "    token_list_no_stop_no_punct = []\n",
    "    # Don't add token to list if punctuation\n",
    "    for token in text:\n",
    "        if token.is_punct == False:\n",
    "            token_list_no_stop_no_punct.append(token)\n",
    "\n",
    "    return token_list_no_stop_no_punct\n",
    "\n",
    "\n",
    "def lemmatization(text): \n",
    "    \"\"\"Use this function to lemmatize a given text.\n",
    "\n",
    "    :param text: Tokens to lemmatize\n",
    "    :type text: list[spacy.tokens.token.Token]\n",
    "    :return: lemmatized tokens\n",
    "    :rtype: list[str]\n",
    "    \"\"\"\n",
    "\n",
    "    token_list_no_stop_no_punct_lemmatized = []\n",
    "    for token in text: \n",
    "        if \"\\n\" not in token.lemma_:\n",
    "            token_list_no_stop_no_punct_lemmatized.append(token.lemma_)\n",
    "    return token_list_no_stop_no_punct_lemmatized\n",
    "\n",
    "\n",
    "def processing_pipeline(song_data):\n",
    "    \"\"\"Use this function to execute the entire processing pipeline on given song data.\n",
    "    Preprocessing steps:\n",
    "    - Tokenization\n",
    "    - Stop word removal\n",
    "    - Punctuation removal\n",
    "    - Lemmatization\n",
    "    - ...\n",
    "\n",
    "    :param song_data: song data saved in a json file containing song name, artist name and lyrics\n",
    "    :type song_data: dict\n",
    "    :return: preprocessed song data\n",
    "    :rtype: dict\n",
    "    \"\"\"\n",
    "\n",
    "    nlp = spacy.load(\"en_core_web_sm\", disable = ['ner'])\n",
    "    \n",
    "    for row in range(len(song_data)):\n",
    "        text_nlp_pipe = list(nlp.pipe([song_data.iloc[row][\"Lyric\"]]))\n",
    "    \n",
    "        # Tokenization\n",
    "        song_data.at[row,\"Lyric\"] = tokenization(text_nlp_pipe)\n",
    "        # Stop word removal\n",
    "        song_data.at[row,\"Lyric\"] = stop_word_removal(song_data.iloc[row][\"Lyric\"])\n",
    "        # Punctuation removal\n",
    "        song_data.at[row,\"Lyric\"] = punctutation_removal(song_data.iloc[row][\"Lyric\"])\n",
    "        # Lemmatization\n",
    "        song_data.at[row,\"Lyric\"] = lemmatization(song_data.iloc[row][\"Lyric\"])\n",
    "        song_data.at[row,\"Lyric\"] = \" \".join(song_data.iloc[row][\"Lyric\"])\n",
    "        song_data.at[row,\"Lyric\"] = song_data.iloc[row][\"Lyric\"].lower()\n",
    "        if row%500 == 0 and row >= 500:\n",
    "            print(f\"processsed: {row} rows out of {len(song_data)}\")\n",
    "    return song_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "af1a15a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/song-data-labels-cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "521cac23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processsed: 500 rows out of 27705\n",
      "processsed: 1000 rows out of 27705\n",
      "processsed: 1500 rows out of 27705\n",
      "processsed: 2000 rows out of 27705\n",
      "processsed: 2500 rows out of 27705\n",
      "processsed: 3000 rows out of 27705\n",
      "processsed: 3500 rows out of 27705\n",
      "processsed: 4000 rows out of 27705\n",
      "processsed: 4500 rows out of 27705\n",
      "processsed: 5000 rows out of 27705\n",
      "processsed: 5500 rows out of 27705\n",
      "processsed: 6000 rows out of 27705\n",
      "processsed: 6500 rows out of 27705\n",
      "processsed: 7000 rows out of 27705\n",
      "processsed: 7500 rows out of 27705\n",
      "processsed: 8000 rows out of 27705\n",
      "processsed: 8500 rows out of 27705\n",
      "processsed: 9000 rows out of 27705\n",
      "processsed: 9500 rows out of 27705\n",
      "processsed: 10000 rows out of 27705\n",
      "processsed: 10500 rows out of 27705\n",
      "processsed: 11000 rows out of 27705\n",
      "processsed: 11500 rows out of 27705\n",
      "processsed: 12000 rows out of 27705\n",
      "processsed: 12500 rows out of 27705\n",
      "processsed: 13000 rows out of 27705\n",
      "processsed: 13500 rows out of 27705\n",
      "processsed: 14000 rows out of 27705\n",
      "processsed: 14500 rows out of 27705\n",
      "processsed: 15000 rows out of 27705\n",
      "processsed: 15500 rows out of 27705\n",
      "processsed: 16000 rows out of 27705\n",
      "processsed: 16500 rows out of 27705\n",
      "processsed: 17000 rows out of 27705\n",
      "processsed: 17500 rows out of 27705\n",
      "processsed: 18000 rows out of 27705\n",
      "processsed: 18500 rows out of 27705\n",
      "processsed: 19000 rows out of 27705\n",
      "processsed: 19500 rows out of 27705\n",
      "processsed: 20000 rows out of 27705\n",
      "processsed: 20500 rows out of 27705\n",
      "processsed: 21000 rows out of 27705\n",
      "processsed: 21500 rows out of 27705\n",
      "processsed: 22000 rows out of 27705\n",
      "processsed: 22500 rows out of 27705\n",
      "processsed: 23000 rows out of 27705\n",
      "processsed: 23500 rows out of 27705\n",
      "processsed: 24000 rows out of 27705\n",
      "processsed: 24500 rows out of 27705\n",
      "processsed: 25000 rows out of 27705\n",
      "processsed: 25500 rows out of 27705\n",
      "processsed: 26000 rows out of 27705\n",
      "processsed: 26500 rows out of 27705\n",
      "processsed: 27000 rows out of 27705\n",
      "processsed: 27500 rows out of 27705\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['lie floor feather find hand wanna believe not think s god heaven hell lose lookin sign maybe fool swear spine get shiver like kiss time tho tear stream like river know ask sign scare tho body start shakin feel dry eye tho heart break know love sign sign findin street starin reach turn lookin stranger eye s god heaven hell tt feel like walk cos touch like swear spine get shiver like kiss time tho tear stream like river know ask sign scare tho body start shakin feel dry eye tho heart break know love sign sign lie floor feather find hand wanna believe not think swear spine get shiver like kiss time tho tear stream like river know ask sign scare tho body start shakin feel dry eye tho heart break know love sign sign',\n",
       " 'oh lord like know think oh oh lord wanna love life heart wonder right try forget near darle miss darling oh bring darle sweet kiss darle think happy think mistake think darling right wrong near darle miss darling oh bring darle sweet kiss darle oh jah oh jah like near oh jah oh jah miss love']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)\n",
    "df = processing_pipeline(df)\n",
    "list(df[\"Lyric\"].head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2ea2bcaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['lie', 'floor', 'feather', 'find', 'hand', 'wanna', 'believe', 'not', 'think', 's', 'god', 'heaven', 'hell', 'lose', 'lookin', 'sign', 'maybe', 'fool', 'swear', 'spine', 'get', 'shiver', 'like', 'kiss', 'time', 'tho', 'tear', 'stream', 'like', 'river', 'know', 'ask', 'sign', 'scare', 'tho', 'body', 'start', 'shakin', 'feel', 'dry', 'eye', 'tho', 'heart', 'break', 'know', 'love', 'sign', 'sign', 'findin', 'street', 'starin', 'reach', 'turn', 'lookin', 'stranger', 'eye', 's', 'god', 'heaven', 'hell', 'tt', 'feel', 'like', 'walk', 'cos', 'touch', 'like', 'swear', 'spine', 'get', 'shiver', 'like', 'kiss', 'time', 'tho', 'tear', 'stream', 'like', 'river', 'know', 'ask', 'sign', 'scare', 'tho', 'body', 'start', 'shakin', 'feel', 'dry', 'eye', 'tho', 'heart', 'break', 'know', 'love', 'sign', 'sign', 'lie', 'floor', 'feather', 'find', 'hand', 'wanna', 'believe', 'not', 'think', 'swear', 'spine', 'get', 'shiver', 'like', 'kiss', 'time', 'tho', 'tear', 'stream', 'like', 'river', 'know', 'ask', 'sign', 'scare', 'tho', 'body', 'start', 'shakin', 'feel', 'dry', 'eye', 'tho', 'heart', 'break', 'know', 'love', 'sign', 'sign'], ['oh', 'lord', 'like', 'know', 'think', 'oh', 'oh', 'lord', 'wanna', 'love', 'life', 'heart', 'wonder', 'right', 'try', 'forget', 'near', 'darle', 'miss', 'darling', 'oh', 'bring', 'darle', 'sweet', 'kiss', 'darle', 'think', 'happy', 'think', 'mistake', 'think', 'darling', 'right', 'wrong', 'near', 'darle', 'miss', 'darling', 'oh', 'bring', 'darle', 'sweet', 'kiss', 'darle', 'oh', 'jah', 'oh', 'jah', 'like', 'near', 'oh', 'jah', 'oh', 'jah', 'miss', 'love']]\n"
     ]
    }
   ],
   "source": [
    "# merge lyrics together\n",
    "lyrics = []\n",
    "for i in df['Lyric']:\n",
    "    lyrics.append(i.split())\n",
    "print(lyrics[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a9ef229f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec<vocab=27165, vector_size=150, alpha=0.025>\n"
     ]
    }
   ],
   "source": [
    "# train the word2vec model\n",
    "# vector size according to https://moj-analytical-services.github.io/NLP-guidance/NNmodels.html#:~:text=The%20standard%20Word2Vec%20pre%2Dtrained,fewer%20dimensions%20to%20represent%20them.\n",
    "# micount = 2 to prevent misspellings\n",
    "word2vec_model = Word2Vec(lyrics, vector_size=150, window=5, min_count=2, workers=16)\n",
    "print(word2vec_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "58b953eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0    76    19   223   739   251   176   236     9   269  1216   406\n",
      "    544    34   415     9  3789    52   351    76    16   162    10     1\n",
      "     34   393    68    36   177    52     2   136    44     9    52    67\n",
      "   1407   148   778    76    46   422    20    53    16   104   487    52\n",
      "      2  2007    47     5    55     5    55    76    16   162    10     1\n",
      "     34   393    68    36   177    52     2   136    44     9    52    67\n",
      "   1407   148   778    43   206     5   452    16   438     3    16    47\n",
      "     16    99    76   162    18   162    10     1    34   439   107    68\n",
      "     36   177    96   136    44     9    52    67  1407   148   778    76]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0   108\n",
      "    160   282   282   120    12   763   322  2709    10    81   376   711\n",
      "    132   802    15   131   140   124   587   587     2     4    13   130\n",
      "    150   345   420   138    87   432    14     1    22     5    14   109\n",
      "     22    71   432     1 15006     5    14     1     1 15006     5    14\n",
      "      1     1    14    14    45   188  1330   220    65    86    26    15\n",
      "     76   334   333     3    26  1995   112   131   140   124   587   587\n",
      "      1   136     2     4    13   130   150   345   420   138    87   432\n",
      "   4244    14     1    22     5    14   109    22    71   432     1 15006\n",
      "     14     1     1 15006     5    14     1     1    14     1    22     5\n",
      "     14   109    22    71   432     1    14     1    22     5    14   109\n",
      "     22    71   432     1    14     1    22     5    14   109    22    71\n",
      "    432     1    14     1    22    14    14    14   109 26598 17244 26599]]\n"
     ]
    }
   ],
   "source": [
    "# use the keras tokenizer and apply it to the lyrics\n",
    "# number in first row is vocab size\n",
    "token = Tokenizer(27165)\n",
    "token.fit_on_texts(df['Lyric'])\n",
    "text = token.texts_to_sequences(df['Lyric'])\n",
    "text = pad_sequences(text, 180)\n",
    "print(text[3:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2b322a9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "        0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "        0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "        0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "        0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "        0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "        0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "        0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        0.]], dtype=float32)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encode the labels\n",
    "le = preprocessing.LabelEncoder()\n",
    "y = le.fit_transform(df['Mood'])\n",
    "y = to_categorical(y)\n",
    "y[:10]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "152ea206",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(np.array(text), y, test_size=0.2, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4d1db6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to use the word2vec as an layer in keras \n",
    "# taken from the gensim wikipage: https://github.com/RaRe-Technologies/gensim/wiki/Using-Gensim-Embeddings-with-Keras-and-Tensorflow\n",
    "\n",
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "def gensim_to_keras_embedding(model, train_embeddings=False):\n",
    "    \"\"\"Get a Keras 'Embedding' layer with weights set from Word2Vec model's learned word embeddings.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_embeddings : bool\n",
    "        If False, the returned weights are frozen and stopped from being updated.\n",
    "        If True, the weights can / will be further updated in Keras.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    `keras.layers.Embedding`\n",
    "        Embedding layer, to be used as input to deeper network layers.\n",
    "\n",
    "    \"\"\"\n",
    "    keyed_vectors = model.wv  # structure holding the result of training\n",
    "    weights = keyed_vectors.vectors  # vectors themselves, a 2D numpy array    \n",
    "    index_to_key = keyed_vectors.index_to_key  # which row in `weights` corresponds to which word?\n",
    "\n",
    "    layer = Embedding(\n",
    "        input_dim=weights.shape[0],\n",
    "        output_dim=weights.shape[1],\n",
    "        weights=[weights],\n",
    "        trainable=train_embeddings,\n",
    "    )\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a660e0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_model = Sequential()\n",
    "keras_model.add(gensim_to_keras_embedding(word2vec_model, True))\n",
    "keras_model.add(Dropout(0.2))\n",
    "keras_model.add(Conv1D(50, 5, activation='relu', padding='same', strides=1))\n",
    "keras_model.add(Conv1D(50, 5, activation='relu', padding='same', strides=1))\n",
    "keras_model.add(MaxPool1D())\n",
    "keras_model.add(Dropout(0.2))\n",
    "keras_model.add(Conv1D(100, 5, activation='relu', padding='same', strides=1))\n",
    "keras_model.add(Conv1D(100, 5, activation='relu', padding='same', strides=1))\n",
    "keras_model.add(MaxPool1D())\n",
    "keras_model.add(Dropout(0.2))\n",
    "keras_model.add(Conv1D(150, 5, activation='relu', padding='same', strides=1))\n",
    "keras_model.add(Conv1D(150, 5, activation='relu', padding='same', strides=1))\n",
    "keras_model.add(GlobalMaxPool1D())\n",
    "keras_model.add(Dropout(0.2))\n",
    "keras_model.add(Dense(200))\n",
    "keras_model.add(Activation('relu'))\n",
    "keras_model.add(Dropout(0.2))\n",
    "# Number of moods to be classified to\n",
    "keras_model.add(Dense(17))\n",
    "keras_model.add(Activation('softmax'))\n",
    "keras_model.compile(loss='binary_crossentropy', metrics=['acc'], optimizer='adam')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1170775b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "693/693 [==============================] - 72s 100ms/step - loss: 0.1866 - acc: 0.2750 - val_loss: 0.1916 - val_acc: 0.3043\n",
      "Epoch 2/3\n",
      "693/693 [==============================] - 76s 110ms/step - loss: 0.1767 - acc: 0.3139 - val_loss: 0.1757 - val_acc: 0.3337\n",
      "Epoch 3/3\n",
      "324/693 [=============>................] - ETA: 37s - loss: 0.1732 - acc: 0.3382"
     ]
    }
   ],
   "source": [
    "keras_model.fit(x_train, y_train, batch_size=32, epochs=3, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3da8cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./cnn_model_v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13efec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "\n",
    "# let's create a function that creates the model (required for KerasClassifier) \n",
    "# while accepting the hyperparameters we want to tune \n",
    "# we also pass some default values such as optimizer='rmsprop'\n",
    "def create_model(filters,multiplicator, multiplicator2, kernel, adapt_embedding):\n",
    "    # define model\n",
    "    \n",
    "    keras_model = Sequential()\n",
    "    keras_model.add(gensim_to_keras_embedding(word2vec_model, True))\n",
    "    keras_model.add(Dropout(0.2))\n",
    "    keras_model.add(Conv1D(filters, kernel, activation='relu', padding='same', strides=1))\n",
    "    keras_model.add(Conv1D(filters, kernel, activation='relu', padding='same', strides=1))\n",
    "    keras_model.add(MaxPool1D())\n",
    "    keras_model.add(Dropout(0.2))\n",
    "    keras_model.add(Conv1D(filters*multiplicator, kernel, activation='relu', padding='same', strides=1))\n",
    "    keras_model.add(Conv1D(filters*multiplicator, kernel, activation='relu', padding='same', strides=1))\n",
    "    keras_model.add(MaxPool1D())\n",
    "    keras_model.add(Dropout(0.2))\n",
    "    keras_model.add(Conv1D(filters*multiplicator2, kernel, activation='relu', padding='same', strides=1))\n",
    "    keras_model.add(Conv1D(filters*multiplicator2, kernel, activation='relu', padding='same', strides=1))\n",
    "    keras_model.add(GlobalMaxPool1D())\n",
    "    keras_model.add(Dropout(0.2))\n",
    "    keras_model.add(Dense(200))\n",
    "    keras_model.add(Activation('relu'))\n",
    "    keras_model.add(Dropout(0.2))\n",
    "    # Number of moods to be classified to\n",
    "    keras_model.add(Dense(17))\n",
    "    keras_model.add(Activation('softmax'))\n",
    "    \n",
    "    keras_model.compile(loss='binary_crossentropy', metrics=['acc'], optimizer='adam')\n",
    "    return keras_model\n",
    "\n",
    "\n",
    "\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "batch_size = 32\n",
    "epochs = 3\n",
    "\n",
    "model_CV = KerasClassifier(build_fn=create_model, epochs=epochs, \n",
    "                           batch_size=batch_size, verbose=1)\n",
    "# define the grid search parameters\n",
    "filters = [50]\n",
    "multiplicator = [2]\n",
    "multiplicator2 = [4]\n",
    "kernel = [5]\n",
    "adapt_embedding = [True, False]\n",
    "\n",
    "param_grid = dict(filters=filters, multiplicator=multiplicator, multiplicator2=multiplicator2, kernel=kernel, adapt_embedding=adapt_embedding)\n",
    "grid = GridSearchCV(estimator=model_CV, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "grid_result = grid.fit(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1b1a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print results\n",
    "print(f'Best Accuracy for {grid_result.best_score_} using {grid_result.best_params_}')\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f' mean={mean:.4}, std={stdev:.4} using {param}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d97bb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "\n",
    "# let's create a function that creates the model (required for KerasClassifier) \n",
    "# while accepting the hyperparameters we want to tune \n",
    "# we also pass some default values such as optimizer='rmsprop'\n",
    "def create_model(filters,multiplicator, multiplicator2, kernel, adapt_embedding):\n",
    "    # define model\n",
    "    \n",
    "    keras_model = Sequential()\n",
    "    keras_model.add(gensim_to_keras_embedding(word2vec_model, True))\n",
    "    keras_model.add(Dropout(0.2))\n",
    "    keras_model.add(Conv1D(filters, kernel, activation='relu', padding='same', strides=1))\n",
    "    keras_model.add(MaxPool1D())\n",
    "    keras_model.add(Dropout(0.2))\n",
    "    keras_model.add(Conv1D(filters*multiplicator, kernel, activation='relu', padding='same', strides=1))\n",
    "    keras_model.add(MaxPool1D())\n",
    "    keras_model.add(Dropout(0.2))\n",
    "    keras_model.add(Conv1D(filters*multiplicator2, kernel, activation='relu', padding='same', strides=1))\n",
    "    keras_model.add(GlobalMaxPool1D())\n",
    "    keras_model.add(Dropout(0.2))\n",
    "    keras_model.add(Dense(200))\n",
    "    keras_model.add(Activation('relu'))\n",
    "    keras_model.add(Dropout(0.2))\n",
    "    # Number of moods to be classified to\n",
    "    keras_model.add(Dense(17))\n",
    "    keras_model.add(Activation('softmax'))\n",
    "    \n",
    "    keras_model.compile(loss='binary_crossentropy', metrics=['acc'], optimizer='adam')\n",
    "    return keras_model\n",
    "\n",
    "\n",
    "\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "batch_size = 32\n",
    "epochs = 3\n",
    "\n",
    "model_CV = KerasClassifier(build_fn=create_model, epochs=epochs, \n",
    "                           batch_size=batch_size, verbose=1)\n",
    "# define the grid search parameters\n",
    "filters = [50]\n",
    "multiplicator = [2]\n",
    "multiplicator2 = [4]\n",
    "kernel = [5]\n",
    "adapt_embedding = [True, False]\n",
    "\n",
    "param_grid = dict(filters=filters, multiplicator=multiplicator, multiplicator2=multiplicator2, kernel=kernel, adapt_embedding=adapt_embedding)\n",
    "grid = GridSearchCV(estimator=model_CV, param_grid=param_grid, n_jobs=-1, cv=2)\n",
    "grid_result = grid.fit(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5f0148b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Accuracy for 0.3266558349132538 using {'adapt_embedding': True, 'filters': 50, 'kernel': 5, 'multiplicator': 2, 'multiplicator2': 4}\n",
      " mean=0.3267, std=0.001534 using {'adapt_embedding': True, 'filters': 50, 'kernel': 5, 'multiplicator': 2, 'multiplicator2': 4}\n",
      " mean=0.3196, std=0.005053 using {'adapt_embedding': False, 'filters': 50, 'kernel': 5, 'multiplicator': 2, 'multiplicator2': 4}\n"
     ]
    }
   ],
   "source": [
    "# print results\n",
    "print(f'Best Accuracy for {grid_result.best_score_} using {grid_result.best_params_}')\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f' mean={mean:.4}, std={stdev:.4} using {param}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ta_3.10",
   "language": "python",
   "name": "ta_3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
