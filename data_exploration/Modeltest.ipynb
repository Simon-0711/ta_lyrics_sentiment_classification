{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e35493ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-20 08:27:14.847890: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-20 08:27:15.373398: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-01-20 08:27:15.373439: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-01-20 08:27:16.836341: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-20 08:27:16.836446: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-20 08:27:16.836453: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.keras.layers import Dense, Embedding,GlobalMaxPooling1D\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e1b4fa13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "def tokenization(text):\n",
    "    \"\"\"Use this function to tokenize text.\n",
    "\n",
    "    :param text: Text as list\n",
    "    :type text: list[spacy.tokens.token.Token]\n",
    "    :return: Tokenized text as list\n",
    "    :rtype: list[spacy.tokens.token.Token]\n",
    "    \"\"\"\n",
    "\n",
    "    token_list = []\n",
    "    for doc in text: \n",
    "        # iterate over tokens in docs\n",
    "        for token in doc:\n",
    "            token_list.append(token)\n",
    "\n",
    "    return token_list\n",
    "\n",
    "\n",
    "def stop_word_removal(text): \n",
    "    \"\"\"Use this function to remove stop words. \n",
    "\n",
    "    :param text: Tokens to remove stop words from \n",
    "    :type text: list[spacy.tokens.token.Token]\n",
    "    :return: Tokens without stop words\n",
    "    :rtype: list[spacy.tokens.token.Token]\n",
    "    \"\"\"\n",
    "\n",
    "    token_list_without_stop = []\n",
    "    # Don't add token to list if stop word\n",
    "    for token in text:\n",
    "        if token.is_stop == False: \n",
    "            token_list_without_stop.append(token)\n",
    "\n",
    "    return token_list_without_stop\n",
    "\n",
    "\n",
    "def punctutation_removal(text): \n",
    "    \"\"\"Use this function to remove punctuation.\n",
    "\n",
    "    :param text: Tokens to remove punctuation from\n",
    "    :type text: list[spacy.tokens.token.Token]\n",
    "    :return: Tokens without punctuation\n",
    "    :rtype: list[spacy.tokens.token.Token]\n",
    "    \"\"\"\n",
    "\n",
    "    token_list_no_stop_no_punct = []\n",
    "    # Don't add token to list if punctuation\n",
    "    for token in text:\n",
    "        if token.is_punct == False:\n",
    "            token_list_no_stop_no_punct.append(token)\n",
    "\n",
    "    return token_list_no_stop_no_punct\n",
    "\n",
    "\n",
    "def lemmatization(text): \n",
    "    \"\"\"Use this function to lemmatize a given text.\n",
    "\n",
    "    :param text: Tokens to lemmatize\n",
    "    :type text: list[spacy.tokens.token.Token]\n",
    "    :return: lemmatized tokens\n",
    "    :rtype: list[str]\n",
    "    \"\"\"\n",
    "\n",
    "    token_list_no_stop_no_punct_lemmatized = []\n",
    "    for token in text: \n",
    "        if \"\\n\" not in token.lemma_:\n",
    "            token_list_no_stop_no_punct_lemmatized.append(token.lemma_)\n",
    "    return token_list_no_stop_no_punct_lemmatized\n",
    "\n",
    "def padding(text):\n",
    "    \"\"\"Use this function to trim the lyrics to a certain size.\n",
    "\n",
    "    :param text: Tokens to lemmatize\n",
    "    :type text: list[spacy.tokens.token.Token]\n",
    "    :return: lemmatized tokens\n",
    "    :rtype: list[str]\n",
    "    \"\"\"\n",
    "\n",
    "    token_list_no_stop_no_punct_lemmatized = []\n",
    "    for token in text: \n",
    "        if \"\\n\" not in token.lemma_:\n",
    "            token_list_no_stop_no_punct_lemmatized.append(token.lemma_)\n",
    "    return token_list_no_stop_no_punct_lemmatized    \n",
    "\n",
    "\n",
    "def processing_pipeline(song_data):\n",
    "    \"\"\"Use this function to execute the entire processing pipeline on given song data.\n",
    "    Preprocessing steps:\n",
    "    - Tokenization\n",
    "    - Stop word removal\n",
    "    - Punctuation removal\n",
    "    - Lemmatization\n",
    "    - ...\n",
    "\n",
    "    :param song_data: song data saved in a json file containing song name, artist name and lyrics\n",
    "    :type song_data: dict\n",
    "    :return: preprocessed song data\n",
    "    :rtype: dict\n",
    "    \"\"\"\n",
    "\n",
    "    nlp = spacy.load(\"en_core_web_sm\", disable = ['ner'])\n",
    "    \n",
    "    for row in range(len(song_data)):\n",
    "        text_nlp_pipe = list(nlp.pipe([song_data.iloc[row][\"Lyrics\"]]))\n",
    "    \n",
    "        # Tokenization\n",
    "        song_data.at[row,\"Lyrics\"] = tokenization(text_nlp_pipe)\n",
    "        # Stop word removal\n",
    "        song_data.at[row,\"Lyrics\"] = stop_word_removal(song_data.iloc[row][\"Lyrics\"])\n",
    "        # Punctuation removal\n",
    "        song_data.at[row,\"Lyrics\"] = punctutation_removal(song_data.iloc[row][\"Lyrics\"])\n",
    "        # Lemmatization\n",
    "        song_data.at[row,\"Lyrics\"] = lemmatization(song_data.iloc[row][\"Lyrics\"])\n",
    "        # Padding Funktion\n",
    "        \n",
    "        \n",
    "    return song_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c71825c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_df = pd.read_csv('./data/lyrics-data.csv')\n",
    "artists_df = pd.read_csv('./data/artists-data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5450cd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_songs = lyrics_df[\"language\"] == \"en\"\n",
    "english_songs_df = lyrics_df[english_songs]\n",
    "english_songs_df.head(5)\n",
    "#drop SLink Column since it has no use\n",
    "english_songs_df=english_songs_df.drop('SLink', axis=1)\n",
    "# rename columns to more expressive names\n",
    "english_songs_df= english_songs_df.rename(columns={'ALink': 'Link', 'SName': 'Song_Name', 'Lyric': 'Lyrics', 'language': 'Language'})\n",
    "# Get artist names from dataset 2\n",
    "english_songs_df = pd.merge(english_songs_df, artists_df[['Link','Artist']], on='Link', how='left')\n",
    "english_songs_df = english_songs_df.drop([\"Link\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f9a469b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"I feel so unsure\\nAs I take your hand and lead you to the dance floor\\nAs the music dies, something in your eyes\\nCalls to mind a silver screen\\nAnd all those sad goodbyes\\n\\nI'm never gonna dance again\\nGuilty feet have got no rhythm\\nThough it's easy to pretend\\nI know you're not a fool\\n\\nShould've known better than to cheat a friend\\nAnd waste the chance that I've been given\\nSo I'm never gonna dance again\\nThe way I danced with you\\n\\nTime can never mend\\nThe careless whispers of a good friend\\nTo the heart and mind\\nIgnorance is kind\\nThere's no comfort in the truth\\nPain is all you'll find\\n\\nI'm never gonna dance again\\nGuilty feet have got no rhythm\\nThough it's easy to pretend\\nI know you're not a fool\\n\\nShould've known better than to cheat a friend\\nAnd waste this chance that I've been given\\nSo I'm never gonna dance again\\nThe way I danced with you\\n\\nNever without your love\\n\\nTonight the music seems so loud\\nI wish that we could lose this crowd\\nMaybe it's better this way\\nWe'd hurt each other with the things we'd want to say\\n\\nWe could have been so good together\\nWe could have lived this dance forever\\nBut now who's gonna dance with me?\\nPlease stay\\n\\nAnd I'm never gonna dance again\\nGuilty feet have got no rhythm\\nThough it's easy to pretend\\nI know you're not a fool\\n\\nShould've known better than to cheat a friend\\nAnd waste the chance that I've been given\\nSo I'm never gonna dance again\\nThe way I danced with you\\n\\n(Now that you're gone) Now that you're gone\\n(Now that you're gone) What I did's so wrong, so wrong\\nThat you had to leave me alone?\",\n",
       " \"Don't let them fool, ya\\nOr even try to school, ya! Oh, no!\\nWe've got a mind of our own\\nSo go to hell if what you? re thinking is not right!\\nLove would never leave us alone\\nA-yin the darkness there must come out to light\\n\\nCould you be loved and be loved?\\nCould you be loved and be loved?\\n\\nDon't let them change ya, oh!\\nOr even rearrange ya! Oh, no!\\nWe've got a life to live\\nThey say: only, only\\nOnly the fittest of the fittest shall survive\\nStay alive! Oh!\\n\\nCould you be loved and be loved?\\nCould you be loved, wo now! And be loved?\\n\\nCould you be\\nCould you be\\nCould you be loved?\\n\\nSay something!\\n\\nSe ligue na ternura\\nSe ligue no amor\\nSe ligue na ternura\\nSe ligue na cor\\nSe ligue na alegria\\nSe ligue no prazer\\nSe ligue, fique atento, se ligue, fique astral\\n\\nCould you be loved and be loved?\"]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = english_songs_df[:20]\n",
    "test.head(2)\n",
    "list(test[\"Lyrics\"].head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f4268baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = processing_pipeline(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "bb739cfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Song_Name</th>\n",
       "      <th>Lyrics</th>\n",
       "      <th>Language</th>\n",
       "      <th>Artist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Careless Whisper</td>\n",
       "      <td>[feel, unsure, hand, lead, dance, floor, music...</td>\n",
       "      <td>en</td>\n",
       "      <td>Ivete Sangalo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Could You Be Loved / Citação Musical do Rap: S...</td>\n",
       "      <td>[let, fool, ya, try, school, ya, oh, get, mind...</td>\n",
       "      <td>en</td>\n",
       "      <td>Ivete Sangalo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cruisin' (Part. Saulo)</td>\n",
       "      <td>[baby, let, cruise, away, confuse, way, clear,...</td>\n",
       "      <td>en</td>\n",
       "      <td>Ivete Sangalo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Easy</td>\n",
       "      <td>[know, sound, funny, stand, pain, Girl, leave,...</td>\n",
       "      <td>en</td>\n",
       "      <td>Ivete Sangalo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>For Your Babies (The Voice cover)</td>\n",
       "      <td>[get, look, hope, lad, face, beam, smile, get,...</td>\n",
       "      <td>en</td>\n",
       "      <td>Ivete Sangalo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Song_Name  \\\n",
       "0                                   Careless Whisper   \n",
       "1  Could You Be Loved / Citação Musical do Rap: S...   \n",
       "2                             Cruisin' (Part. Saulo)   \n",
       "3                                               Easy   \n",
       "4                  For Your Babies (The Voice cover)   \n",
       "\n",
       "                                              Lyrics Language         Artist  \n",
       "0  [feel, unsure, hand, lead, dance, floor, music...       en  Ivete Sangalo  \n",
       "1  [let, fool, ya, try, school, ya, oh, get, mind...       en  Ivete Sangalo  \n",
       "2  [baby, let, cruise, away, confuse, way, clear,...       en  Ivete Sangalo  \n",
       "3  [know, sound, funny, stand, pain, Girl, leave,...       en  Ivete Sangalo  \n",
       "4  [get, look, hope, lad, face, beam, smile, get,...       en  Ivete Sangalo  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ff8d688",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vocab_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m Sequential([\n\u001b[0;32m----> 2\u001b[0m     Embedding(\u001b[43mvocab_size\u001b[49m, \u001b[38;5;241m8\u001b[39m, input_length\u001b[38;5;241m=\u001b[39mmax_length),\n\u001b[1;32m      3\u001b[0m    Conv1D(\u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m5\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m      4\u001b[0m     GlobalMaxPooling1D(),\n\u001b[1;32m      5\u001b[0m   Dense(\u001b[38;5;241m10\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m      6\u001b[0m   Dense(\u001b[38;5;241m1\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigmoid\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m ])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vocab_size' is not defined"
     ]
    }
   ],
   "source": [
    "# todo: vocab size bestimmen \n",
    "# todo: padding funktion einbauen \n",
    "# todo: testen mit labeled data\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(vocab_size, 8, input_length=max_length),\n",
    "   Conv1D(128, 5, activation='relu'),\n",
    "    GlobalMaxPooling1D(),\n",
    "  Dense(10, activation='relu'),\n",
    "  Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "history = model.fit(x_train, y_train, epochs=20, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192f925c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ta_3.10",
   "language": "python",
   "name": "ta_3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
